

\section{Introduction}
\label{sec:intro}

% Importance and explanation of the Problem we address: Fine-tuning pre-trained diffusion policies while maintaining multimodality.
 Robotic manipulation tasks are inherently multimodal, admitting diverse yet valid strategies: a cup can be grasped from either side, a block can be rotated clockwise or counterclockwise, and redundant kinematics allow the same goal to be reached via distinct motions. These scenarios naturally give rise to multimodal action distributions, whose preservation is key for policies that are robust, versatile, and adaptable to perturbations and unforeseen situations. %Policies that collapse to a single dominant strategy not only lose flexibility but also fail to generalize when the environment demands an alternative solution. 
Recent advances in generative policy learning have shown that expressive architectures such as diffusion~\citep{chi2023diffusion,kang2023efficient,psenka2023learning} and flow-based models~\citep{lipman2022flow,park2025flow} can capture such multimodality from demonstrations. However, %because they are typically trained with supervised imitation objectives, 
their behavior is bounded by the coverage of the demonstration dataset. \Ac{rl} provides a natural mechanism to adapt and improve these pre-trained policies beyond demonstrations. Yet, RL fine-tuning often biases the policy toward \reb{few} reward-maximizing behaviors at the expense of diversity, %a phenomenon akin to reward hacking~\cite{liu2025flow}, 
an issue that is further exacerbated when the fine-tuning reward is misaligned with the implicit objectives expressed in demonstrations~\citep{zhou2024rethinking,brown2019extrapolating}. The central problem we address in this work is therefore: \textit{how can we fine-tune pre-trained generative policies with RL while preserving the multimodality acquired by supervised pre-training?}


% What the community currently does: most overlook the multimodal aspect, destroying multimodality, some try to be clearer on what multimodality is, but they assum knowledge of the number of modalities of the policy
Despite the communityâ€™s growing interest in policies showcasing multimodal behaviors, little work systematically examines how \ac{rl} adaptation affects multimodality. Existing research splits broadly into two directions. A first line of work focuses on fine-tuning expressive policies such as diffusion or flow models with \ac{rl} to improve robustness and returns~\citep{park2025flow,ren2024diffusion,chen2024diffusion}. These approaches, however, do not account for multimodality in the action distribution, and often collapse the diverse behaviors captured during demonstration into a single dominant strategy. A second line of work begins to address multimodality more explicitly, for instance by proposing metrics to characterize it~\citep{jia2024towards} or by leveraging language conditioning and instruction diversity~\citep{black2410pi0,kim2024openvla}. Yet, these efforts either rely on assumptions that the number of modes is known in advance or that multimodality can be fully captured through language and labels. In practice, the modalities contained in the demonstration are usually unknown, and language provides only a coarse handle on behavior, which prevents precise encoding of low-level motor attributes such as magnitudes, scales, and endpoints~\citep{lee2025molmoact}. %, leaving much of the multimodal structure unaddressed during RL adaptation. 


%What we propose instead: unsupervised mode discovery, and intrinsic reward to maintain diversity
In this work, we propose MD\textendash MAD (\emph{\textbf{M}ode \textbf{D}iscovery for \textbf{M}ultimodal \textbf{A}ction \textbf{D}istributions}), a method to fine-tune expressive generative policies while explicitly preserving multimodality. We begin by introducing a \reb{measure} of multimodality for this class of noise-conditioned generative models, such as diffusion and flow-based policies. Inspired by prior work on unsupervised skill discovery~\citep{gregor2016variational, eysenbach2018diversity}, we then design a mode discovery procedure that uncovers latent behavioral modes in pre-trained policies without assuming prior knowledge or relying on external annotations. This discovery process serves a dual purpose: it uncovers and makes controllable the latent modalities of the policy, and it enables the estimation of the policy's multimodality via a mutual information objective. This objective is subsequently employed as an intrinsic reward during reinforcement learning fine-tuning, regularizing the policy to retain diverse behaviors, as shown in Figure~\ref{fig:fig1}. We evaluated the proposed regularization method on multiple robotic manipulation tasks exhibiting multimodal behaviors. Across all tasks, our approach achieves comparable task success to standard fine-tuning while retaining action multimodality, demonstrating the effectiveness of our regularization objective.


In summary, our contributions are: \textbf{1)} A \reb{proxy to} measure multimodality \reb{of generative policies} %action distributions 
that does not rely on mode labels or language supervision. \textbf{2)} An unsupervised mode-discovery framework enabling the identification of latent behavioral modes in pre-trained generative policies. \textbf{3)} A mode-preserving RL fine-tuning objective, where intrinsic rewards derived from discovered modes prevent collapse while improving task performance. \textbf{4)} An empirical evaluation on robotic manipulation tasks showing that our method preserves multimodality while enhancing task success.