


\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\columnwidth]{figures/01_intro/all_skills.pdf}
    \caption{\textbf{Lorem ipsum dolor sit amet.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. TO TRY: Anymal and Avoid on top, or on second page. Try }
    \label{fig:fig1}
  \end{figure}
  

\section{Introduction}
\label{sec:intro}

% Importance and explanation of the Problem we address: Fine-tuning pre-trained diffusion policies while maintaining multimodality.
 Robotic tasks are inherently multimodal, admitting diverse yet valid strategies: a cup can be grasped from either side, a block can be rotated clockwise or counterclockwise, and kinematic redundancy enables diverse motions to reach the same goal. Preserving this diversity is crucial for policies that are robust, versatile, and adaptable to perturbations and unforeseen situations. %Policies that collapse to a single dominant strategy not only lose flexibility but also fail to generalize when the environment demands an alternative solution. 
Recent advances in generative policy learning show that expressive architectures such as diffusion~\citep{chi2023diffusion,kang2023efficient,psenka2023learning} and flow-based models~\citep{lipman2022flow,park2025flow} can capture multimodal behavior from demonstrations. However, their behavior remains limited by the support of the demonstration dataset. \Ac{rl} provides a natural mechanism to adapt and improve these pre-trained policies beyond demonstrations, yet \ac{rlft} often concentrates probability mass on a small set of reward-maximizing behaviors, degrading behavioral diversity~\citep{zhou2024rethinking,brown2019extrapolating}. 
The central problem we address is therefore: \textit{how can we fine-tune pre-trained generative policies with \ac{rl} while preserving the multimodality acquired during supervised pre-training?}


% What the community currently does: most overlook the multimodal aspect, destroying multimodality, some try to be clearer on what multimodality is, but they assum knowledge of the number of modalities of the policy
% Despite the communityâ€™s growing interest in policies showcasing multimodal behaviors, little work systematically examines how \ac{rl} adaptation affects multimodality. Existing research splits broadly into two directions. A first line of work focuses on \ac{rlft} of expressive policies such as diffusion or flow models to improve robustness and returns~\citep{park2025flow,ren2024diffusion,chen2024diffusion}. These approaches, however, do not account for multimodality in the action distribution, and often collapse the diverse behaviors captured during demonstration into a single dominant strategy. A second line of work begins to address multimodality more explicitly, for instance by proposing metrics to characterize it~\citep{jia2024towards} or by leveraging language conditioning and instruction diversity~\citep{black2410pi0,kim2024openvla}. Yet, these efforts either rely on assumptions that the number of modes is known in advance or that multimodality can be fully captured through language and labels. In practice, the modalities contained in the demonstration are usually unknown, and language provides only a coarse handle on behavior, which prevents precise encoding of low-level motor attributes such as magnitudes, scales, and endpoints~\citep{lee2025molmoact}. %, leaving much of the multimodal structure unaddressed during RL adaptation. 
Despite growing interest in multimodal policies, the effect of \ac{rl} adaptation on behavioral diversity remains underexplored. Prior work falls broadly into two directions. A first line studies \ac{rlft} of expressive generative policies (e.g., diffusion or flow models) to improve robustness and return~\citep{park2025flow,ren2024diffusion,chen2024diffusion}. However, these methods typically optimize task reward without explicitly accounting for multimodality, and can induce mode collapse, eliminating behaviors learned from demonstration. A second line addresses multimodality more directly, for instance, by proposing diversity metrics~\citep{jia2024towards} or leveraging language conditioning and instruction diversity~\citep{black2410pi0,kim2024openvla}. Yet, these approaches often assume that the number of modes is known \emph{a priori} or that language and labels provide a sufficient description of multimodal structure. In practice, the modes present in demonstrations are rarely known, and language offers only coarse semantic control, making it difficult to preserve fine-grained motor attributes such as magnitudes, temporal scales, and endpoints~\citep{lee2025molmoact}. % , leaving much of the multimodal structure unaddressed during \ac{rl} adaptation. 



%What we propose instead: unsupervised mode discovery, and intrinsic reward to maintain diversity
% In this work, we propose \methodname{} (\emph{\textbf{M}ode \textbf{D}iscovery for \textbf{M}ultimodal \textbf{A}ction \textbf{D}istributions}), a method to fine-tune expressive generative policies while explicitly preserving multimodality.
% In this work, we propose \methodname{} (\emph{\textbf{B}ehavioral \textbf{M}ode \textbf{D}iscovery}), a framework for \ac{rlft} that preserves multimodal behavior by uncovering and leveraging latent behavioral modes in pre-trained generative policies. 
% % We begin by introducing a \reb{measure} of multimodality for this class of noise-conditioned generative models, such as diffusion and flow-based policies. 
% Inspired by prior work on unsupervised skill discovery~\citep{gregor2016variational, eysenbach2018diversity}, we then design a mode discovery procedure that uncovers latent behavioral modes in pre-trained policies without assuming prior knowledge or relying on external annotations. This discovery process serves a dual purpose: it uncovers and makes controllable the latent modalities of the policy, and it enables the estimation of the policy's multimodality via a mutual information objective. This objective is subsequently employed as an intrinsic reward during \ac{rlft}, regularizing the policy to retain diverse behaviors, as shown in Figure~\ref{fig:fig1}. We evaluated the proposed regularization method on multiple robotic manipulation tasks exhibiting multimodal behaviors. Across all tasks, our approach achieves comparable task success to standard fine-tuning while retaining action multimodality, demonstrating the effectiveness of our regularization objective.

In this work, we propose \methodname{} (\emph{\textbf{B}ehavioral \textbf{M}ode \textbf{D}iscovery}), a framework for \ac{rlft} that preserves multimodal behavior by uncovering and leveraging latent behavioral modes in pre-trained generative policies. 
We begin by introducing a tractable proxy for multimodality in noise-conditioned generative policies %: under the assumption that the pre-trained policy is multimodal---in the sense that different latent noise realizations induce distinct behaviors---
based on the mutual information between the latent noise and the induced behavior. Inspired by unsupervised skill discovery~\citep{gregor2016variational, eysenbach2018diversity}, we then design a mode-discovery procedure that exposes this structure without annotations or prior knowledge of the number of modes, yielding a controllable latent representation and a practical mutual-information estimate. Finally, we use this estimate as an intrinsic reward during \ac{rlft}, regularizing fine-tuning to prevent mode collapse while maintaining task performance (Figure~\ref{fig:fig1}). Our experiments show that across diverse multimodal robotic manipulation tasks, our approach matches or improves task success relative to standard fine-tuning while better preserving multimodality.


In summary, our contributions are: \textbf{1)} A mutual-information--based proxy for quantifying multimodality in noise-conditioned generative policies, without mode labels or language supervision. \textbf{2)} An unsupervised mode-discovery procedure that identifies and makes controllable latent behavioral modes in pre-trained generative policies. \textbf{3)} A mode-preserving \ac{rlft} objective that uses a mutual-information intrinsic reward to regularize fine-tuning and prevent mode collapse while maintaining task performance. \textbf{4)} An empirical evaluation on multimodal robotic manipulation tasks demonstrating improved task success and stronger multimodality preservation compared to standard fine-tuning.