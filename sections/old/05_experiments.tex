\section{Experiments}

We evaluated our method in two distinct settings: 1) an illustrative setting generating a 2D Gaussian mixture reward landscape, 2) diverse ManiSkill~\citep{tao2024maniskill3} and D3IL~\citep{jia2024towards} \textit{multimodal} tasks. 
Our experimental evaluation investigates the mode-collapse effect of \ac{rl} fine-tuning under different techniques, and assesses the effectiveness of our approach in uncovering modes of pre-trained policies and preserving multimodal behaviors after fine-tuning. We benchmark task performance relative to competitive baselines, and analyze how our intrinsic reward balances success rates with diversity preservation. In addition, we conduct ablation studies on the most critical design choices to isolate their impact.


\paragraph{Baselines}
Following the characterization introduced in Section~\ref{sec:fine-tuning_tech}, we compare our method against representative approaches for on-policy fine-tuning of diffusion policies, direct fine-tuning, redisual and steering methods, none of which explicitly aim to preserve multimodality. As a direct fine-tuning approach, we include \texttt{DPPO}~\citep{ren2024diffusion}, which optimizes the diffusion policy weights with PPO. We consider the DDIM parameterization of the generative process to ensure non-memoryless noise schedules, while keeping a balance between $\eta>0$ and the number of reverse diffusion steps to allow weight fine-tuning.  To examine the effect of decreasing the number of reverse diffusion steps, we also consider the original hyperparameters of the \texttt{DPPO} baseline that uses the full denoising chain for action sampling with DDPM parameterization, and fine-tunes the last $10$ steps, denoted \texttt{DPPO[10]}, which makes the generation process non-memoryless. As a residual fine-tuning approach (\texttt{RES}), we evaluate Policy Decorator~\citep{yuan2024policy}, where a lightweight residual network is trained on top of the frozen pre-trained diffusion model. This allows task adaptation while limiting catastrophic interference with the base model. 
Finally, we consider~\citep{wagenmaker2025steering} a steering-based baseline, \texttt{SP}, which adapts the latent noise distribution $w$ to bias the pre-trained policy toward high-reward behaviors. This category operates entirely in the latent space and, like the others, does not include any explicit mechanism for mode discovery or diversity preservation.  
Importantly, our approach is orthogonal to these categories: the proposed multimodality-preserving regularizer can be combined with either residual or steering-based fine-tuning under non-memoryless noise schedules. Accordingly, we report results both for the standalone baselines and for their variants augmented with our multimodality regularizer, denoted as \texttt{X[MD]}, where \texttt{X} indicates the corresponding baseline.


\al{Mention further implementation deatils in the appendix}

\paragraph{Evaluation Metrics}
We evaluate fine-tuned policies along two axes: \emph{task success} and \emph{behavioral diversity}. For task success, we report overall success rate $\mathrm{SR}$, and two mode-aggregated measures of the success rate to integrate behavioral diversity: the success rate weighted for each mode
$
\mathrm{SR}_{\text{M}}=\tfrac{1}{K}\sum_{i=1}^K \mathrm{SR}_i,
$
which guards against degenerate solutions (e.g., \(100\%\) success on a single mode but failure on others), and
mode coverage ${\mathrm{mc}@\tau=\tfrac{1}{K}\sum_{i=1}^K \mathbf{1}\{\mathrm{SR}_i \ge \tau\}}$,
the fraction of modes solved above threshold \(\tau\). 

To further measure multimodality, we follow the D3IL benchmark and compute the entropy of the empirical distribution over modes among all rollouts: $H(\pi)=-\sum_{i=1}^K p_i\log p_i$, where $p_i$ is the fraction of episodes in mode $i$. A higher entropy reflects more balanced usage of the available modes, whereas a reduction after fine-tuning is indicative of mode collapse. All metrics are computed from $N=1024$ evaluation episodes with fixed seeds for fair comparison, and we report both the mean and standard deviation over three independent runs with different random seeds.



\subsection{Implementation Details} 
\al{This can be shrunk and the detailed version moved to the appendix. We can also have tables summarizing all the hyperparameters in the appendix or just metion that we will release the codebase.}

We discuss the implementation and training of the pre-trained policy, steering policy, and the discriminator, respectively. We further discuss how to integrate our method with general fine-tuning techniques.

\textbf{Pre-trained policy and DPPO fine-tuning} \quad
The diffusion policy is trained with the standard behavioral cloning objective for diffusion models, where the network predicts the injected noise conditioned on the noisy actions. We follow the implementation and hyperparameter setup of DPPO~\cite{ren2024diffusion}, using a cosine noise schedule during training. The action horizon coincides with the execution horizon and consists of $4$ action steps per chunk. Pre-training is performed with $20$ denoising steps, while inference uses DDIM~\citep{song2020denoising} sampling with $2$ steps. For frozen policies, we set $\eta=0$, whereas for fine-tuning, we set $\eta=1$, which is equivalent to applying DDPM~\citep{ho2020denoising}. This choice ensures steerability of the policy and avoids memoryless noise schedules. The policy head is implemented as a multi-layer perceptron (MLP) with hidden dimensions $\{512, 512, 512\}$, and a time-embedding dimension of $16$, which we found to improve training stability compared to UNet backbones, similar to~\cite{ren2024diffusion}.

\textbf{Residual Policy}
\al{Here we want to mention that we tune the residual component as suggested in the paper to try constrain the contribution of the residual to not completely override the original action sampled by the diffusion policy while imporving success rate. Probably this should be aslo mentioned on the baseline description to make it clear that it is there.}

\textbf{Steering policy} \quad
The steering policy $\pi^{\mathcal{W}}(w \mid s, z)$ is implemented as a Gaussian policy parameterized by an MLP with hidden layers of size $\{256, 256, 256\}$. To constrain its support within that of the original diffusion prior, we apply a KL regularization during training of the form
\[
\mathcal{L}_{\mathrm{KL}} = \mathbb{E}_{s,z}\Big[ 
D_{\mathrm{KL}}\!\left(\pi^{\mathcal{W}}(w \mid s,z)\,\big\|\,\mathcal{N}(0,I)\right)
\Big],
\]
where $\mathcal{N}(0,I)$ denotes the isotropic Gaussian prior used in the diffusion model. The latent variable $z \in {0,1,\dots,K{-}1}$ is sampled from a uniform categorical prior $p(z)$, as we empirically found discrete latents easier to learn and more stable than continuous ones. The dimensionality of the latent space is a hyperparameter, in the experiments we consider $K=\{ 4, 8, 16\}$. Training proceeds in two stages: for the first 200 epochs, the steering policy is optimized only with the intrinsic reward $\log q_\phi(z \mid s, a) - \log p(z)$, serving as a mode-discovery phase; in the remaining epochs, the environment reward is added to steer behaviors toward high-return regions while retaining multimodality. PPO is used for optimization with clipping parameter $\epsilon=0.2$, GAE $\lambda=0.95$, discount $\gamma=0.99$, and Adam with learning rate $3\times 10^{-4}$. %\al{Missing the critic?}


\textbf{Inference model} \quad The inference model $q_\phi(z \mid s,a)$ is implemented as a categorical classifier over the latent codes ${z \in \{0,\dots,K-1\}}$. It consists of a multilayer perceptron with hidden layers of dimension $\{256,256,256\}$, Mish activations~\citep{misra2019mish}, and a final softmax output producing the class probabilities $q_\phi(z \mid s,a)$. To prevent overfitting to small variations in continuous states, Gaussian noise with standard deviation $\{1.0, 0.01, 0.001\}$ (depending on the task) is injected into the inputs during training only. 
The model is trained by minimizing the negative log-likelihood $
\mathcal{L}_{\mathrm{NLL}}(\phi) = - \mathbb{E}_{(s,a,z)}\big[\log q_\phi(z \mid s,a)\big],
$
where the expectation is taken over state-action pairs generated by the steering policy and latent codes sampled from the prior $p(z)$. During training of the steering policy, the log-posterior $\log q_\phi(z \mid s,a)$ serves as an intrinsic reward, combined with the prior correction term $-\log p(z)$, thereby providing the intrinsic objective for mode discovery and diversity-preserving fine-tuning.


\textbf{Integrating with other fine-tuning techniques.} \quad
The steering policy with mode discovery uncovers and controls the behavioral modes of the pre-trained diffusion mode, steering them toward regions of high reward. However, because this mechanism does not update the diffusion weights directly, its performance remains bounded by the expressiveness of the pre-trained policy. From this perspective, the steering policy can be viewed as an \emph{exploration agent} that guides state visitation in a structured way, and can therefore be seamlessly combined with existing fine-tuning methods discussed in Section~\ref{sec:rw}. A key distinction is that our framework provides access to a discriminator that evaluates whether the fine-tuned behaviors remain consistent with the discovered modes, supplying an intrinsic reward that discourages collapse into a single strategy. While the steering policy itself can continue to adapt jointly with the diffusion model, we found it beneficial to update the discriminator with a very low learning rate: this allows it to accommodate novel states encountered during fine-tuning while preserving the previously identified mode structure, thereby stabilizing multimodality retention.


\subsection{2D Gaussian Mixture}
\label{sec:2D_Gaussians}
% \al{We want show the following things:
% Q1: is the mutual information a relatively good metric for multimodality? 
% Q2: Does the latent space captures relevant modes in the pre-trained policy?
% Q3: In a state where there is multimodality, what is the latent space that the steering policy covers?
% Q4: What is the role of the dimensionality of Z?
% Q5 Baseline Comparison
% Q6: Does it do that even if the reward is not symmetric?
% Q7: What if the policy is stochastic and non-memoryless?
% }


As a controlled setting to study the proposed method, we design a simple two-dimensional navigation task where the reward landscape is defined by a mixture of Gaussian peaks. The agent’s state is its 2D position $(x,y)$, and actions are modeled as small displacements $\Delta x, \Delta y$. The reward at position $\text{pos}=(x,y)$ is computed as a sum of unnormalized Gaussians centered at a fixed set of goal locations $\{(c_x,c_y)\}$:
\begin{equation}
    r(x,y) = \sum_{(c_x,c_y) \in \mathcal{C}} \exp\!\left(-\tfrac{(x-c_x)^2+(y-c_y)^2}{2\sigma^2}\right),
\end{equation}
where $\sigma$ controls the spread of each mode.  
The environment is multimodal by construction: high reward can be obtained by reaching any of the Gaussian peaks. An episode is considered successful if the agent reaches within a fixed distance threshold of one of the goal centers. 

To gain a deeper understanding of our method, we go beyond overall performance and analyze its internal mechanisms and robustness. In particular, we study the validity of mutual information as a measure of multimodality, the structure and coverage of the learned latent space, and the influence of its dimensionality. We further examine whether the proposed intrinsic reward effectively preserves multimodal behaviors under different reward landscapes and policy settings. 


% \begin{figure*}[t]
%   \centering
%   % ---------- LEFT: 2 rows × 3 cols ----------
%   \begin{minipage}[t]{0.64\textwidth}
%     \centering
%     % Row 1
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/expert_trajectories_1.png}
%       \caption{Dataset (1 mode)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/expert_trajectories_2.png}
%       \caption{Dataset (2 modes)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/expert_trajectories_4.png}
%       \caption{Dataset (4 modes)}
%     \end{subfigure}

%     \vspace{0.6em}

%     % Row 2
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/final_frame_p1.png}
%       \caption{Policy (1 mode)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/final_frame_p2.png}
%       \caption{Policy (2 modes)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%     \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/final_frame_p4.png}
%       \caption{Policy (4 modes)}
%     \end{subfigure}

%     \captionof{figure}{Policies with different numbers of modes in the Gaussian-mixture landscape. \al{Substitute the dataset with the monte-carlo estimate of the action distribution.}}
%     \label{fig:q1-env-trajectories}
%   \end{minipage}
%   \hfill
%   % ---------- RIGHT: 2 rows × 2 cols (modes by z) ----------
%   \begin{minipage}[t]{0.33\textwidth}
%     \centering

%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z0.png}
%       \caption{$z=0$}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z1.png}
%       \caption{$z=1$}
%     \end{subfigure}

%     \vspace{0.6em}

%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z2.png}
%       \caption{$z=2$}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z3.png}
%       \caption{$z=3$}
%     \end{subfigure}


\begin{figure*}[t]
  \centering
  % ---------- LEFT: 2 rows × 3 cols ----------
  \begin{minipage}[t]{0.64\textwidth}
    \centering
    % Row 1
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{iclr2026/figures/2D_Mixture/final_frame_p1.png}
      \caption{Policy (1 mode)}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=0.9\linewidth]{iclr2026/figures/2D_Mixture/final_frame_p2.png}
      \caption{Policy (2 modes)}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
    \includegraphics[width=0.9\linewidth]{iclr2026/figures/2D_Mixture/final_frame_p4.png}
      \caption{Policy (4 modes)}
    \end{subfigure}


    \vspace{0.6em}

    % Row 2
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/action_dist_t0_s0_model_1.pdf}
      \caption{1 peak.}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/action_dist_t0_s0_model_2.pdf}
      \caption{2 peaks.}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.32\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/action_dist_t0_s0_model_3.pdf}
      \caption{4 peaks.}
    \end{subfigure}
    \captionof{figure}{Policies with different numbers of modes in the Gaussian-mixture landscape. \al{Substitute the dataset with the monte-carlo estimate of the action distribution.}}
    \label{fig:q1-env-trajectories}
  \end{minipage}
  \hfill
  % ---------- RIGHT: 2 rows × 2 cols (modes by z) ----------
  \begin{minipage}[t]{0.33\textwidth}
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z0.png}
      \caption{$z=0$}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z1.png}
      \caption{$z=1$}
    \end{subfigure}

    \vspace{0.6em}

    \begin{subfigure}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z2.png}
      \caption{$z=2$}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=\linewidth]{iclr2026/figures/2D_Mixture/mode_z3.png}
      \caption{$z=3$}
    \end{subfigure}
    \captionof{figure}{Rollouts generated by sampling latent codes \(z\in\{0,1,2,3\}\) from the learned steering policy. \al{What about here, instead of repeating the datasets, we visualize the monte carlo estimate of hte action distribution?}}
    \label{fig:q1-modes-by-z}
  \end{minipage}
\end{figure*}

\paragraph{Mutual Information as a Proxy for Multimodality and Mode Discovery.}  
We first evaluate whether mutual information provides a reliable measure of multimodality. To this end, we construct three expert datasets that exhibit one, two, or four goal modes in the Gaussian-mixture environment and train separate policies on each dataset. If $\mathcal{M}$ is a valid multimodality metric, we expect its value to increase systematically with the number of available modes. We compute $\mathcal{M}$ using the variational estimate in Equation~\ref{eq:variational_MI}, which is also used to train a discrete latent space $Z \equiv \{0,1,2,3\}$ and the inference model $q_\phi$ over latent codes. In this setting, the steering policy is trained solely with the mutual-information objective (without task reward), allowing us to directly test whether maximizing $\mathcal{M}$ corresponds to capturing and differentiating the demonstrated modes.  Figure~\ref{fig:q1-env-trajectories} shows the expert datasets (top row) and the corresponding learned policies (bottom row). As expected, the policies reproduce the number of modes present in the demonstrations by consistently reaching the corresponding Gaussian peaks.


\begin{wraptable}[9]{r}{0.48\textwidth} % [9] = reserve ~9 text lines
\vspace{-0.6\baselineskip}              % tweak if needed
\centering
\caption{\textbf{Mutual information and discriminator loss.} }
\begin{tabular}{lcc}
\toprule
\textbf{Policy} & \textbf{$\mathcal{M}$ ($\uparrow$)} & \textbf{Disc. Loss ($\downarrow$)}\\
\midrule
1 mode  & $1.06 \pm 0.00$  & $0.33 \pm 0.02$ \\
2 modes & $0.58 \pm 0.02$  & $0.82 \pm 0.02$ \\
4 modes & $0.00 \pm 0.00$  & $1.38 \pm 0.00$ \\
\bottomrule
\end{tabular}
\label{tab:mi_disc}
\end{wraptable}




 Table~\ref{tab:mi_disc} reports the final mutual-information estimate and discriminator loss after jointly training the steering policy and the discriminator with the objective defined in Equation~\ref{eq:variational_MI}. Mutual information increases monotonically with the number of modes: the four-mode policy achieves the highest value, the two-mode policy an intermediate value, while the unimodal policy remains at zero.  Conversely, the discriminator loss is lowest when four modes are present, since the discriminator can reliably distinguish them, and highest in the unimodal case, where no meaningful structure can be exploited. These results confirm that the proposed mutual-information estimate correlates with the degree of multimodality and can be used as a training signal to preserve diverse behaviors. Figure~\ref{fig:q1-modes-by-z} illustrates trajectories sampled by selecting one specific latent code $z \in Z$, confirming that the latent space capture relevant modes in the pre-trained policy. 

It is important to note, however, that this estimate of the mutual information is not an absolute metric: its value depends on the learned latent representation and the capacity of the discriminator, and is therefore not directly comparable across different models or training runs. Instead, it should be regarded as a relative proxy that reflects the preservation of multimodality within a given training setup. In the following experiments, we empirically demonstrate its effectiveness as a fine-tuning signal.



\begin{wrapfigure}{r}{0.4\textwidth}  % r = right, l = left
\vspace{-0.2\baselineskip}             % adjust vertical spacing if needed
\centering
\includegraphics[width=0.85\linewidth]{iclr2026/figures/2D_Mixture/latent_structure_z=4.pdf}

\caption{Latent noise samples $w$ for $z\in\{0,1,2,3\}$. }
\label{fig:latent_structure}
\vspace{-0.9cm} 
\end{wrapfigure}

\paragraph{What is the structure learned by the steering policy in the policy latent space?}
We probe what the steering policy actually learns by inspecting the input-noise latents it predicts, rather than the trajectories executed by the full policy. 
Concretely, for the initial state $s_0$ and each skill label $z \in \{0,1,2,3\}$, we draw $1024$ samples $w \sim \pi^{\mathcal{W}}(w\mid s_0,z)$ and visualize them in Figure~\ref{fig:latent_structure}  together with kernel-density contours and the per-skill mean. 
The figure reveals a clear four-cluster organization where each skill forms a compact, well-separated mode in the latent space, with only limited cross-skill overlap. This analysis shows that the steering head has learned a discrete, multimodal latent structure aligned with the modes present in the original demonstration dataset.


% % \vspace{0.8cm} 
% \paragraph{What if the policy is stochastic and non-memoryless?}
% \al{This experiment, if completed, will likely go in the appendix as it is not fundamental. Probably this could be part of the implementation details.} To learn a steering policy, there must be statistical dependence between the input noise of the diffusion model and the generated actions. This dependence is highly influenced by the noise scheduler of the diffusion model~\citep{domingo2024adjoint}. While DDIM sampling allows making the generation process deterministic, determinism prevents fine-tuning the diffusion model, so we need to retain a level of stochasticity. We can achieve that in two different ways, which si either optimizing the $\eta$ parameter, or by fixing it and reducing the number of denoising steps for generation. We empirically found the second option to be better as higher stochasticity, even if with lower generation steps, allows for better exploration and thus fine-tuning of the model weights. 



\paragraph{Baseline Comparison}

We compare baseline fine-tuning methods trained solely with the task objective against their counterparts augmented with our proposed multimodality-preserving regularizer, denoted by the \texttt{[MD]} tag. In these variants, a steering policy and a mutual-information–based intrinsic reward are used to retain diverse behaviors during fine-tuning. We evaluate performance under two reward landscapes, created by rotating the Gaussian peaks by ${\tfrac{\pi}{8}, \tfrac{\pi}{4}}$, and additionally test robustness in asymmetric settings by randomly scaling the strength of individual Gaussians. Finally, we analyze the effect of varying the dimensionality of the latent space $\mathcal{Z}$. Table~\ref{tab:toy_results} reports results for both goals across baseline methods, their [MD]-regularized variants, and varying dimensionalities of $\mathcal{Z}$. We first discuss baseline performance, then the role of mode discovery and latent dimensionality, and finally the unbalanced reward setting.


% \begin{tabular}{l|cccc||cccc}
% \toprule
% \rowcolor{lightgray}  & \multicolumn{4}{c||}{\textbf{Goal [1]}} & \multicolumn{4}{c}{\textbf{Goal [2]}} \\
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ \\
% \midrule
% \textbf{PD} & 0.95 & 0.96 & 1.00 & 1.00 & 0.75 & 0.51 & 0.50 & 0.77 \\
% \textbf{SP} & 1.00 & 0.25 & 0.25 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
% \textbf{DPPO} & 1.00 & 0.50 & 0.50 & 0.45 & 1.00 & 0.25 & 0.25 & 0.00 \\
% \textbf{DDIM} & 1.00 & 0.25 & 0.25 & 0.00 & 0.61 & 0.18 & 0.00 & 0.39 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=4$}} \\
% \textbf{PD[+MD]} & 1.00 & 1.00 & 1.00 & 0.99 & 1.00 & 0.75 & 0.75 & 0.74 \\
% \textbf{SP[+MD]} & 0.80 & 0.75 & 0.75 & 0.99 & 0.00 & 0.00 & 0.00 & 0.74 \\
% \textbf{DDIM[+MD]} & 1.00 & 1.00 & 1.00 & 0.99 & 1.00 & 0.75 & 0.75 & 0.74 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=8$}} \\
% \textbf{PD[+MD]} & 0.87 & 0.87 & 0.75 & 0.99 & 1.00 & 1.00 & 1.00 & 0.93 \\
% \textbf{SP[+MD]} & 0.43 & 0.54 & 0.25 & 0.96 & 0.00 & 0.00 & 0.00 & 0.91 \\
% \textbf{DDIM[+MD]} & 0.70 & 0.79 & 0.50 & 0.93 & 1.00 & 1.00 & 1.00 & 0.99 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=16$}} \\
% \textbf{PD[+MD]} & 1.00 & 1.00 & 1.00 & 0.94 & 1.00 & 1.00 & 1.00 & 0.94 \\
% \textbf{SP[+MD]} & 0.05 & 0.07 & 0.00 & 0.98 & 0.00 & 0.00 & 0.00 & 0.97 \\
% \textbf{DDIM[+MD]} & 0.40 & 0.40 & 0.00 & 1.00 & 0.79 & 0.82 & 0.50 & 0.94 \\
% \midrule
% \bottomrule
% \end{tabular}

\begin{table}[!t] % float environment, allows caption + top placement
\centering
\caption{\al{Separate this evaluation from the one of the z! And add stds.} Evaluation on the Gaussian-mixture environment under two rotated goals. 
We report multimodal success rate ($\mathrm{SR}_{\mathrm{M}}$), mode coverage at $80\%$ ($\mathrm{mc}@0.80$), entropy of the discovered modes ($\mathcal{H}$), and unbalanced multimodal success rate ($\mathrm{SR}_{\mathrm{M}}$ Unb.) across baselines and their variants augmented with our mode discovery regularizer (\texttt{[MD]}). 
Results are shown for different latent dimensionalities $z \in \{4,8,16\}$.}
\label{tab:toy_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccc!{\color{gray}\vrule width 0.6pt}c||ccc!{\color{gray}\vrule width 0.6pt}c}
% \caption{Your caption goes here.}
\toprule
\rowcolor{lightgray}  & \multicolumn{4}{c||}{\textbf{Goal [1]}} & \multicolumn{4}{c}{\textbf{Goal [2]}} \\
\rowcolor{lightgray} \textbf{Method} & $\mathrm{SR}_{\mathrm{M}}$ & $\mathrm{mc}@0.80$ & $\mathcal{H}$ & $\mathrm{SR}_{\mathrm{M}}$ (Unb.) & $\mathrm{SR}_{\mathrm{M}}$ & $\mathrm{mc}@0.80$  & $\mathcal{H}$ & $\mathrm{SR}_{\mathrm{M}}$ (Unb.) \\
\midrule
\rowcolor{lightgray} \multicolumn{9}{l}{\textbf{Baselines}} \\
\texttt{RP} & 0.96 & 1.00 & 1.00 & 0.68 & 0.51 & 0.50 & 0.77 & 0.43 \\
\texttt{SP} & 0.25 & 0.25 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
\texttt{DPPO} & 0.50 & 0.50 & 0.45 & 0.25 & 0.25 & 0.25 & 0.00 & 0.25 \\
\texttt{DDIM} & 0.25 & 0.25 & 0.00 & 0.25 & 0.18 & 0.00 & 0.39 & 0.25 \\
\midrule
\rowcolor{lightgray} \multicolumn{9}{l}{\textbf{dim $z=4$}} \\
\texttt{RP\;[MD]} & \textbf{1.00} & \textbf{1.00 }& \textbf{0.99} & \textbf{1.00} & 0.75 & 0.75 & 0.74 & 0.75 \\
\texttt{SP\;[MD]} & \textbf{0.75} & \textbf{0.75} & \textbf{0.99} & 0.00 & 0.00 & 0.00 & 0.74 & 0.00 \\
\texttt{DDIM\;[MD]} & \textbf{1.00} & \textbf{1.00} & \textbf{0.99 }& \textbf{1.00} & 0.75 & 0.75 & 0.74 & 0.75 \\
\midrule
\rowcolor{lightgray} \multicolumn{9}{l}{\textbf{dim $z=8$}} \\
\texttt{RP\;[MD]} & 0.87 & 0.75 & 0.99 & 0.87 & 1.00 & 1.00& 0.93 & 1.00 \\
\texttt{SP\;[MD]} & 0.54 & 0.25 & 0.96 & 0.15 & 0.00 & 0.00 & 0.91 & 0.00 \\
\texttt{DDIM\;[MD]} & 0.79 & 0.50 & 0.93 & 0.79 & \textbf{1.00} & \textbf{1.00}& \textbf{0.99} & \textbf{1.00} \\
\midrule
\rowcolor{lightgray} \multicolumn{9}{l}{\textbf{dim $z=16$}} \\
\texttt{RP\;[MD]} & 1.00 & 1.00 & 0.94 & 1.00 &\textbf{ 1.00} & \textbf{1.00} &\textbf{ 0.94} & \textbf{1.00} \\
\texttt{SP\;[MD]} & 0.07 & 0.00 & 0.98 & 0.09 & 0.00 & 0.00 & 0.97 & 0.00 \\
\texttt{DDIM\;[MD]} & 0.40 & 0.00 & 1.00 & 0.40 & 0.82 & 0.50 & 0.94 & 0.82 \\
\midrule
\bottomrule
\end{tabular}
}
\end{table}




% \begin{tabular}{l|ccccc||ccccc}
% \toprule
% \rowcolor{lightgray}  & \multicolumn{5}{c||}{\textbf{Goal [1]}} & \multicolumn{5}{c}{\textbf{Goal [2]}} \\
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & SR (Unb.) & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & SR (Unb.) \\
% \midrule
% \rowcolor{lightgray} \multicolumn{11}{c}{\textbf{Baselines}} \\
% \textbf{PD} & 0.95 & 0.96 & 1.00 & 1.00 & 0.77 & 0.75 & 0.51 & 0.50 & 0.77 & 0.84 \\
% \textbf{SP} & 1.00 & 0.25 & 0.25 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
% \textbf{DPPO} & 1.00 & 0.50 & 0.50 & 0.45 & 1.00 & 1.00 & 0.25 & 0.25 & 0.00 & 1.00 \\
% \textbf{DDIM} & 1.00 & 0.25 & 0.25 & 0.00 & 1.00 & 0.61 & 0.18 & 0.00 & 0.39 & 1.00 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{11}{c}{\textbf{dim $z=4$}} \\
% \textbf{PD[+MD]} & 1.00 & 1.00 & 1.00 & 0.99 & 1.00 & 1.00 & 0.75 & 0.75 & 0.74 & 1.00 \\
% \textbf{SP[+MD]} & 0.80 & 0.75 & 0.75 & 0.99 & 0.00 & 0.00 & 0.00 & 0.00 & 0.74 & 0.00 \\
% \textbf{DDIM[+MD]} & 1.00 & 1.00 & 1.00 & 0.99 & 1.00 & 1.00 & 0.75 & 0.75 & 0.74 & 1.00 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{11}{c}{\textbf{dim $z=8$}} \\
% \textbf{PD[+MD]} & 0.87 & 0.87 & 0.75 & 0.99 & 0.87 & 1.00 & 1.00 & 1.00 & 0.93 & 1.00 \\
% \textbf{SP[+MD]} & 0.43 & 0.54 & 0.25 & 0.96 & 0.12 & 0.00 & 0.00 & 0.00 & 0.91 & 0.00 \\
% \textbf{DDIM[+MD]} & 0.70 & 0.79 & 0.50 & 0.93 & 0.70 & 1.00 & 1.00 & 1.00 & 0.99 & 1.00 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{11}{c}{\textbf{dim $z=16$}} \\
% \textbf{PD[+MD]} & 1.00 & 1.00 & 1.00 & 0.94 & 1.00 & 1.00 & 1.00 & 1.00 & 0.94 & 1.00 \\
% \textbf{SP[+MD]} & 0.05 & 0.07 & 0.00 & 0.98 & 0.07 & 0.00 & 0.00 & 0.00 & 0.97 & 0.00 \\
% \textbf{DDIM[+MD]} & 0.40 & 0.40 & 0.00 & 1.00 & 0.40 & 0.79 & 0.82 & 0.50 & 0.94 & 0.79 \\
% \midrule
% \bottomrule
% \end{tabular}

Among baselines, the residual method \texttt{RP} shows the strongest performance, successfully solving Goal1 and retaining two modes in Goal2. This is expected, as residual fine-tuning preserves the original policy while applying constrained corrections, though some multimodality is still lost. The steering baseline \texttt{SP} exhibits limited success on Goal1 and collapses completely on Goal2, reflecting the limited expressivity of steering alone. Both \texttt{DPPO} and \texttt{DDIM} improve task success rates, with \texttt{DPPO} benefitting from its greater expressivity at the cost of multimodality: it retains two modes in Goal1 but collapses to a single behavior in Goal2, where the reward deviates more strongly from the demonstrations.

Introducing mode discovery consistently improves both task success and multimodality. The gains are most pronounced for \texttt{DDIM}, which recovers full mode coverage in Goal1 and substantially improves performance in Goal2. Residual fine-tuning (\texttt{RP[MD]}) also benefits, especially on Goal2, where $z=8$ or $z=16$ enable recovery of all modes. For \texttt{SP}, mode discovery partially alleviates collapse in Goal1 but remains insufficient in Goal~2, underscoring the need to combine our regularizer with more expressive fine-tuning strategies.

The dimensionality of $\mathcal{Z}$ has a clear impact. For Goal1, which is closer to the demonstrations, low-dimensional $z$ is sufficient to preserve multimodality. For Goal2, higher dimensions ($z=8,16$) improve coverage by encouraging exploration of diverse trajectories. However, overly large latent spaces can introduce inefficiencies: for example, \texttt{DDIM[MD]} deteriorates in Goal~1 at $z=16$, likely due to a trade-off between task optimization and diversity pressure. This suggests that latent dimensionality should be matched to the complexity of the multimodal structure, and that diversity metrics beyond state coverage may further improve mode discovery. %Given our design of the discriminator, we can observe how for example a dimensionality of $4$ leads to the loss of some modalities as the modes discovered by the model are diverse enough to be recognised by the discriminator but to not cover well the full state space. Increasing the dimensionality helps exploration and thus the retention of all modalities, but the higher the number of modes, the harder the problem becomes, making more samples inefficient for the training of DDIM, for example. 

Finally, the unbalanced reward landscape highlights the robustness of [MD]. Baselines suffer substantial drops in balanced success rates when rewards favor a subset of goals, collapsing to dominant peaks. In contrast, [MD]-regularized variants consistently preserve multimodality, solving all modes in both Goal1 and Goal2 for \texttt{RP} and \texttt{DDIM}. This demonstrates that our method not only stabilizes fine-tuning under symmetric tasks but also counteracts reward asymmetries that would otherwise bias the policy toward fewer behaviors.




% It might be interesting here to include also a disucssion of the continus latent space, but given the time probably the best thing to do is to mention that empirically we found that the best solution for learning was the discrete state space as it becomes a classification problem for the discriminator instead of a regression porblem and we can cite some work that explore continuous state space and mention that it's a potential avenue for future work. 



% \begin{tabular}{l|cccc||cccc}
% \toprule
% \rowcolor{lightgray}  & \multicolumn{4}{c||}{\textbf{Goal [1]}} & \multicolumn{4}{c}{\textbf{Goal [2]}} \\
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ \\
% \midrule
% \textbf{PD} & 0.77 & 0.68 & 0.50 & 0.51 & 0.82 & 0.42 & 0.25 & 0.50 \\
% \textbf{SP} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
% \textbf{DPPO} & 1.00 & 0.25 & 0.25 & 0.00 & 1.00 & 0.25 & 0.25 & 0.00 \\
% \textbf{DDIM} & 1.00 & 0.25 & 0.25 & 0.00 & 1.00 & 0.25 & 0.25 & 0.00 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=4$}} \\
% \textbf{PD[+MD]} & 0.58 & 0.29 & 0.00 & 0.50 & 1.00 & 0.75 & 0.75 & 0.76 \\
% \textbf{DDIM[+MD]} & 0.77 & 0.64 & 0.50 & 0.74 & 0.70 & 0.60 & 0.50 & 0.75 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=8$}} \\
% \textbf{PD[+MD]} & 0.46 & 0.49 & 0.25 & 0.94 & 0.61 & 0.49 & 0.25 & 0.76 \\
% \textbf{DDIM[+MD]} & 0.66 & 0.77 & 0.50 & 0.94 & 0.59 & 0.45 & 0.00 & 0.75 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=16$}} \\
% \textbf{PD[+MD]} & 0.34 & 0.28 & 0.00 & 0.96 & 0.52 & 0.58 & 0.25 & 0.90 \\
% \textbf{DDIM[+MD]} & 0.29 & 0.29 & 0.00 & 0.99 & 0.48 & 0.50 & 0.25 & 0.94 \\
% \midrule
% \bottomrule
% \end{tabular}

% \begin{tabular}{l|cccc||cccc}
% \toprule
% \rowcolor{lightgray}  & \multicolumn{4}{c||}{\textbf{Goal [1]}} & \multicolumn{4}{c}{\textbf{Goal [2]}} \\
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ \\
% \midrule
% \textbf{PD} & 0.77 & 0.68 & 0.50 & 0.51 & 0.84 & 0.43 & 0.25 & 0.50 \\
% \textbf{SP} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
% \textbf{DPPO} & 1.00 & 0.25 & 0.25 & 0.00 & 1.00 & 0.25 & 0.25 & 0.00 \\
% \textbf{DDIM} & 1.00 & 0.25 & 0.25 & 0.00 & 1.00 & 0.25 & 0.25 & 0.00 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=4$}} \\
% \textbf{PD[+MD]} & 0.77 & 0.42 & 0.25 & 0.44 & 0.80 & 0.43 & 0.25 & 0.44 \\
% \textbf{DDIM[+MD]} & 0.72 & 0.63 & 0.50 & 0.70 & 0.70 & 0.60 & 0.50 & 0.75 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=8$}} \\
% \textbf{PD[+MD]} & 0.73 & 0.86 & 0.75 & 0.87 & 0.65 & 0.67 & 0.50 & 0.91 \\
% \textbf{DDIM[+MD]} & 0.48 & 0.47 & 0.25 & 0.89 & 0.59 & 0.56 & 0.25 & 0.95 \\
% \midrule
% \rowcolor{lightgray} \multicolumn{9}{c}{\textbf{dim $z=16$}} \\
% \textbf{PD[+MD]} & 0.45 & 0.37 & 0.00 & 0.93 & 0.52 & 0.54 & 0.25 & 0.88 \\
% \textbf{DDIM[+MD]} & 0.44 & 0.38 & 0.00 & 0.98 & 0.51 & 0.51 & 0.00 & 0.94 \\
% \midrule
% \bottomrule
% \end{tabular}




\paragraph{Removing the steering policy after fine-tuning}
The steering policy can be seen as an auxiliary mechanism to guide exploration in the latent noise space during fine-tuning, enabling the discovery and control of distinct behavioral modes without directly conditioning the policy itself. An important question, however, is what happens once fine-tuning is complete and the steering head is removed, i.e., when actions are again sampled from the original latent noise distribution.

\begin{table}[!t] 
\centering
\caption{Evaluation after removing the steering policy post–fine-tuning. 
We report multimodal success rate ($\mathrm{SR}_{\mathrm{M}}$), coverage at $80\%$, and entropy $\mathcal{H}$ for baselines augmented with mode discovery (\texttt{[MD]}). 
Results highlight that residual fine-tuning (\texttt{PD[+MD]}) retains multimodality even without explicit steering, whereas \texttt{DDIM[+MD]} and \texttt{SP[+MD]} remain strongly dependent on the steering mechanism.}

\label{tab:toy_results_NOSTEER}
\begin{tabular}{l|ccc||ccc}
\toprule
\rowcolor{lightgray}  & \multicolumn{3}{c||}{\textbf{Goal [1]}} & \multicolumn{3}{c}{\textbf{Goal [2]}} \\
\rowcolor{lightgray} \textbf{Method}& $\mathrm{SR}_{\mathrm{M}}$ &  $\mathrm{mc}@0.80$  & $\mathcal{H}$  & $\mathrm{SR}_{\mathrm{M}}$ &  $\mathrm{mc}@0.80$  & $\mathcal{H}$ \\
\midrule
\rowcolor{lightgray} \multicolumn{7}{l}{\textbf{dim $z=4$}} \\
\texttt{RP\;[MD]} & 0.64 & 0.50 & 0.96  & 0.51 & 0.50 & 0.76 \\
\texttt{SP\;[MD]}  & 0.00 & 0.00 & 1.00  & 0.00 & 0.00 & 0.90 \\
\texttt{DDIM\;[MD]} & 0.17 & 0.00 & 1.00 & 0.14 & 0.00 & 0.74 \\
\midrule
\rowcolor{lightgray} \multicolumn{7}{l}{\textbf{dim $z=8$}} \\
\texttt{RP\;[MD]}  & 0.99 & 1.00 & 1.00  & 0.62 & 0.25 & 0.81 \\
\texttt{SP\;[MD]} & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.90 \\
\texttt{DDIM\;[MD]}  & 0.11 & 0.00 & 0.99  & 0.19 & 0.00 & 0.98 \\
\midrule
\rowcolor{lightgray} \multicolumn{7}{l}{\textbf{dim $z=16$}} \\
\texttt{RP\;[MD]} & 0.95 & 1.00 & 0.88 & 0.98 & 1.00 & 0.96 \\
\texttt{SP\;[MD]}  & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.90 \\
\texttt{DDIM\;[MD]}  & 0.08 & 0.00 & 1.00  & 0.15 & 0.00 & 0.88 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:toy_results_NOSTEER} shows that the residual baseline (\texttt{PD[+MD]}) exhibits only a minor drop in performance once the steering policy is removed. In fact, performance tends to improve with higher-dimensional latent spaces ($z=8,16$), suggesting that exploration guided by the steering policy leads to durable improvements in the underlying policy. In other words, even without explicit steering, the residual update mechanism internalizes multimodal behaviors during fine-tuning and can reproduce them directly at test time. In contrast, \texttt{DDIM[+MD]} shows a sharp dependence on the steering policy: once steering is removed, success rates collapse despite maintaining high entropy values. This reflects the limited stochasticity of DDIM sampling, where steering primarily acts as a means to reach regions of high reward but does not leave lasting changes in the base policy. Similarly, \texttt{SP[+MD]} fails entirely without steering, further underscoring that steering alone is insufficient to ensure stable retention of multimodality.

Overall, these results highlight that residual fine-tuning is a particularly strong baseline: it not only preserves multimodal behaviors but also integrates them into the policy itself, making it less dependent on the steering mechanism at test time.
% This suggests an interesting future direction: using the steering policy as a scaffolding tool to discover and collect diverse behaviors, followed by supervised or reinforcement learning updates to distill these behaviors into the diffusion weights. Such an approach could combine the stability of residual fine-tuning with the expressivity of full diffusion-policy optimization.


% \al{To integrate: 
% \begin{itemize}
%     \item comparison with KL regularization, 
%     \item comparison without pre-training the steering policy.
%     \item (If time) Flow-based policies.
% \end{itemize}}




\subsection{Robotic Manipulation}

We investigate the behavior of different fine-tuning techniques in fine-tuning pre-trained diffusion policies, and assess whether our proposed method is able to retain multimodal behaviors, regularizing learning. 

\paragraph{Tasks} We evaluate our approach on three robotic manipulation tasks in ManiSkill~\citep{tao2024maniskill3}, \emph{Reach}, \emph{Lift}, and \emph{Avoid} \al{potentially four in the final version}, each exhibiting different degrees of multimodality \al{TODO: as illustrated in Figure~\ref{}}. Multimodality arises either from goal diversity or, for a fixed goal, from the existence of multiple feasible trajectories leading to successful completion. For example, the \emph{Lift} task is considered successful if the peg is lifted upright from either the blue or the red side, whereas in the \emph{Reach} task the objective is to reach the green sphere while avoiding the gray obstacle, which can be accomplished by approaching from either the left or the right. All environments are equipped with either dense or intermediate reward functions to facilitate fine-tuning. \al{TODO: Further implementation details about the environments are provided in Appendix~\ref{sdf}}. \al{Mention that we desiged an heuristic that allows us to identify the current mode, or the closest one. }

The three tasks differ in their sources and degrees of multimodality. The \emph{Reach} task is the simplest: multimodality arises only at the initial steps, after which the trajectory is effectively committed to a single mode. The \emph{Lift} task is similar in structure but presents greater ambiguity, as the peg can be grasped from multiple regions. This results in a richer mix of modality choices during the initial phase and makes it harder to clearly separate modes. In both tasks, the initial configurations are randomized to increase variability and task difficulty. The \emph{Avoid} task is the most challenging. Here, a large number of modalities emerge later in the episode, with each corresponding to a trajectory of different length. Unlike the first two tasks, only the end-effector position is randomized at reset, while the obstacle remains fixed, further emphasizing the multimodal nature of the trajectories.

\paragraph{Implementation Details}
For each task We collect 1000 demos with a motion planner for each task and pre-train the diffusion model for 1000 epochs. \al{Can we evaluate the difference between a trajectory trained with RL and the motion planning demos to quantify the reward landscape shift?} The baseline models are fine-tuned using only the task rewards, while the regularized version first pre-trains the steering policy with only the mode discovery objective.\al{Some other details}.


\paragraph{Standard Fine-tuning.} We first evaluate standard fine-tuning approaches that do not incorporate any explicit regularization to preserve multimodality. Results are summarized in Table~\ref{tab:baselines}. In the \emph{Reach} task, all methods successfully fine-tune the pre-trained policy without collapsing modes. Both modalities achieve success rates above $0.80$, as reflected in the coverage metric, while aggregate performance across modes remains high. This suggests that the inherent exploration of the diffusion policy is sufficient to discover and fine-tune both solution modes. The \emph{Lift} task presents a different trend. While success rates increase after fine-tuning, none of the methods reliably solve both modalities. The only approaches that maintain higher entropy in the set of explored modes are the steering policy baselines (\texttt{SP}). This indicates that, although steering regularization via a KL divergence with a Normal distribution can help retain action diversity, it also tends to limit the overall improvement in success rates. Finally, in the \emph{Avoid} task, the pre-trained policy already achieves a high success rate. Fine-tuning maintains this level of task success but largely eliminates multimodality. We attribute this collapse to two factors: (i) a mismatch between the reward landscape used for pre-training and that used during fine-tuning, and (ii) the varying trajectory lengths associated with different modalities, which bias the fine-tuning process toward solutions with a shorter horizon.

Taken together, these results confirm that diffusion-based generative policies are effective at capturing multimodal behaviors prior to fine-tuning. However, standard RL fine-tuning tends to destroy this multimodality, especially when the fine-tuning reward landscape deviates significantly from that of pre-training or when rewards are inherently unbalanced across modes. This highlights the need for explicit regularization strategies, motivating our proposed approach as a general mechanism for preserving multimodal behaviors during fine-tuning.



\paragraph{Regularized Fine-tuning.} 
We next evaluate the effect of incorporating our regularization method during fine-tuning, testing the hypothesis that it preserves multimodal behaviors with only a minimal trade-off between diversity and task performance. Results are reported in Table~\ref{tab:our_method}. Overall, fine-tuning with the integration of the proposed intrinsic reward successfully adapts the pre-trained policy while retaining, in most cases, the multimodal structure of its behavior. In the \emph{Reach} task, introducing our regularization has no adverse effect on the final success rate, confirming that diversity can be preserved without compromising performance. \al{EXPECTED: For the \emph{Lift} task, regularization enables the policy to retain both of the original solution modes from the pre-trained policy—an outcome that was not achieved under task-reward-only fine-tuning. This comes at a slight reduction in absolute success rate compared to pure fine-tuning, indicating a trade-off between maximizing task success and preserving multiple modes.} In the more challenging \emph{Avoid} task, our method not only maintains high success rates but also preserves a subset of the original modalities, representing a clear improvement over standard fine-tuning. While some degree of collapse remains, the results demonstrate that our regularization substantially mitigates mode loss even in settings with pronounced reward imbalances.

Taken together, these results demonstrate the effectiveness of integrating our regularization objective into the fine-tuning process, enabling policies to retain diverse behaviors while achieving strong task performance. Beyond improving over standard fine-tuning, these findings highlight the promise of regularization as a general strategy for preserving multimodality, and motivate further investigation into how such approaches can be extended to even more challenging settings with highly unbalanced reward landscapes.

\paragraph{Ablations.} \al{They will hopefully come at the end. The intention is to ablate the role of: the curriculum, of pre-training the steering policy with only intrinsic reward, and the role of beta here. Finally, no alternate training between the steering policy and the policy could be ablated.  }


\begin{table}
\caption{Baselines fine-tuning without regularization. \al{I will remove $\mathcal{H}_\text{succ}$ }}
\label{tab:baselines}
\centering
\begin{tabular}{lccccc}
\toprule
\rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
\midrule
\rowcolor{lightgray} \multicolumn{6}{c}{\emph{Reach}} \\
\midrule
\texttt{PRE} & $0.32 \pm 0.01$ & $0.31 \pm 0.00$ & $0.00 / 2$& $0.99 \pm 0.00$ & $0.85 \pm 0.03$ \\
\midrule
\texttt{RES} & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $2.00 / 2$& $0.98 \pm 0.01$ & $0.98 \pm 0.01$ \\
\texttt{SP} & $0.98 \pm 0.00$ & $0.98 \pm 0.00$ & $2.00 / 2$& $0.97 \pm 0.00$ & $0.97 \pm 0.00$ \\
\texttt{DPPO} & $0.93 \pm 0.01$ & $0.94 \pm 0.02$ & $2.00 / 2$& $0.66 \pm 0.33$ & $0.65 \pm 0.32$ \\
\texttt{DPPO[10]} & $0.99 \pm 0.00$ & $0.99 \pm 0.00$ & $2.00 / 2$& $0.97 \pm 0.03$ & $0.97 \pm 0.03$ \\
\midrule
\rowcolor{lightgray} \multicolumn{6}{c}{\emph{Lift}} \\
\midrule
\texttt{PRE} & $0.14 \pm 0.01$ & $0.15 \pm 0.01$ & $0.00 / 2$& $0.97 \pm 0.01$ & $0.98 \pm 0.01$ \\
\midrule
\texttt{RES} & $1.00 \pm 0.00$ & $0.50 \pm 0.00$ & $1.00 / 2$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
\texttt{SP} & $0.78 \pm 0.03$ & $0.78 \pm 0.03$ & $0.67 / 2$& $0.98 \pm 0.01$ & $0.98 \pm 0.01$ \\
\texttt{DPPO} & $0.99 \pm 0.01$ & $0.57 \pm 0.10$ & $1.00 / 2$& $0.05 \pm 0.03$ & $0.01 \pm 0.02$ \\
\texttt{DPPO[10]} & $1.00 \pm 0.00$ & $0.56 \pm 0.08$ & $1.00 / 2$& $0.02 \pm 0.01$ & $0.00 \pm 0.01$ \\
\midrule
\rowcolor{lightgray} \multicolumn{6}{c}{\emph{Avoid}} \\
\midrule
\texttt{PRE} & $0.94 \pm 0.04$ & $0.86 \pm 0.04$ & $20.00 / 24$& $0.63 \pm 0.00$ & $0.63 \pm 0.01$ \\
\midrule
\texttt{RES} & $0.98 \pm 0.03$ & $0.04 \pm 0.00$ & $1.00 / 24$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
\texttt{SP} & $1.00 \pm 0.01$ & $0.09 \pm 0.02$ & $2.00 / 24$& $0.01 \pm 0.00$ & $0.01 \pm 0.00$ \\
\texttt{DPPO} & $1.00 \pm 0.00$ & $0.26 \pm 0.11$ & $6.33 / 24$& $0.13 \pm 0.15$ & $0.13 \pm 0.15$ \\
\texttt{DPPO[10]} & $1.00 \pm 0.00$ & $0.04 \pm 0.00$ & $1.00 / 24$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Fine-tuning with regularization. \al{I will remove $\mathcal{H}_\text{succ}$ }}
\label{tab:our_method}
\centering
\begin{tabular}{lccccc}
\toprule
\rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
\midrule
\rowcolor{lightgray} \multicolumn{6}{c}{\emph{Reach}} \\
\midrule
\texttt{PRE} & $0.32 \pm 0.01$ & $0.31 \pm 0.00$ & $0.00 / 2$& $0.99 \pm 0.00$ & $0.85 \pm 0.03$ \\
\midrule
\texttt{RES[MD]} & $0.99 \pm 0.00$ & $0.99 \pm 0.00$ & $2.00 / 2$& $1.00 \pm 0.00$ & $1.00 \pm 0.00$ \\
\texttt{SP[MD]} & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $2.00 / 2$& $0.97 \pm 0.01$ & $0.97 \pm 0.01$ \\
\texttt{DPPO[MD]} & $0.98 \pm 0.01$ & $0.98 \pm 0.01$ & $2.00 / 2$& $0.67 \pm 0.43$ & $0.67 \pm 0.43$ \\
\midrule
\rowcolor{lightgray} \multicolumn{6}{c}{\emph{Lift}} \\
\midrule
\texttt{PRE} & $0.14 \pm 0.01$ & $0.15 \pm 0.01$ & $0.00 / 2$& $0.97 \pm 0.01$ & $0.98 \pm 0.01$ \\
\midrule
\texttt{RES [MD]} & $0.99 \pm 0.00$ & $0.99 \pm 0.00$ & $2.00 / 2$& $1.00 \pm 0.00$ & $0.99 \pm 0.00$ \\
\texttt{SP [MD]} & $0.88 \pm 0.07$ & $0.88 \pm 0.07$ & $1.67 / 2$& $0.99 \pm 0.01$ & $0.99 \pm 0.01$ \\
\texttt{DDIM [MD]} & $0.99 \pm 0.00$ & $0.55 \pm 0.07$ & $1.00 / 2$& $0.06 \pm 0.04$ & $0.01 \pm 0.02$ \\
\midrule
\rowcolor{lightgray} \multicolumn{6}{c}{\emph{Avoid}} \\
\midrule
\texttt{PRE} & $0.94 \pm 0.04$ & $0.86 \pm 0.04$ & $20.00 / 24$& $0.63 \pm 0.00$ & $0.63 \pm 0.01$ \\
\midrule
\texttt{RES[MD]} & $0.99 \pm 0.01$ & $0.30 \pm 0.02$ & $7.33 / 24$& $0.53 \pm 0.01$ & $0.53 \pm 0.01$ \\
\texttt{SP[MD]} & $1.00 \pm 0.00$ & $0.42 \pm 0.00$ & $10.00 / 24$& $0.58 \pm 0.00$ & $0.58 \pm 0.00$ \\
\texttt{DPPO[MD]} & $0.94 \pm 0.07$ & $0.43 \pm 0.05$ & $9.67 / 24$& $0.57 \pm 0.01$ & $0.57 \pm 0.01$ \\
\bottomrule
\end{tabular}
\end{table}


% \begin{table}
% \caption{Lift}
% \label{tab:lift}
% \centering
% \begin{tabular}{l|ccccc}
% \toprule
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
% \midrule
% \texttt{PRE} & $0.14 \pm 0.01$ & $0.15 \pm 0.01$ & $0.00 / 2$& $0.97 \pm 0.01$ & $0.98 \pm 0.01$ \\
% \midrule
% \texttt{RES} & $1.00 \pm 0.00$ & $0.50 \pm 0.00$ & $1.00 / 2$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
% \texttt{SP} & $0.78 \pm 0.03$ & $0.78 \pm 0.03$ & $0.67 / 2$& $0.98 \pm 0.01$ & $0.98 \pm 0.01$ \\
% \texttt{DPPO} & $1.00 \pm 0.00$ & $0.56 \pm 0.08$ & $1.00 / 2$& $0.02 \pm 0.01$ & $0.00 \pm 0.01$ \\
% \texttt{DDIM} & $0.99 \pm 0.01$ & $0.57 \pm 0.10$ & $1.00 / 2$& $0.05 \pm 0.03$ & $0.01 \pm 0.02$ \\
% \midrule
% \texttt{RES[MD]} & 0.16 & 0.16 & 0.00 & 0.98 & 0.99 \\
% \texttt{SP[MD]} & 0.12 & 0.12 & 0.00 & 1.00 & 1.00 \\
% \texttt{DDIM[MD]} & 0.36 & 0.36 & 0.00 & 1.00 & 0.18 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}
% \caption{Avoid}
% \label{tab:avoid}
% \centering
% \begin{tabular}{l|ccccc}
% \toprule
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
% \midrule
% \texttt{PRE} & $1.00 \pm 0.00$ & $0.98 \pm 0.02$ & $23.33 / 24$& $0.72 \pm 0.00$ & $0.72 \pm 0.00$ \\
% \midrule
% \texttt{RES} & $0.98 \pm 0.03$ & $0.04 \pm 0.00$ & $1.00 / 24$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
% \texttt{SP} & $1.00 \pm 0.01$ & $0.09 \pm 0.02$ & $2.00 / 24$& $0.01 \pm 0.00$ & $0.01 \pm 0.00$ \\
% \texttt{DPPO} & $1.00 \pm 0.00$ & $0.04 \pm 0.00$ & $1.00 / 24$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
% \texttt{DDIM} & $1.00 \pm 0.00$ & $0.26 \pm 0.11$ & $6.33 / 24$& $0.13 \pm 0.15$ & $0.13 \pm 0.15$ \\
% \midrule
% \texttt{RES [MD]} & $0.99 \pm 0.01$ & $0.30 \pm 0.02$ & $7.33 / 24$& $0.53 \pm 0.01$ & $0.53 \pm 0.01$ \\
% \texttt{SP [MD]} & $1.00 \pm 0.00$ & $0.42 \pm 0.00$ & $10.00 / 24$& $0.58 \pm 0.00$ & $0.58 \pm 0.00$ \\
% \texttt{DDIM [MD]} & $0.94 \pm 0.07$ & $0.43 \pm 0.05$ & $9.67 / 24$& $0.57 \pm 0.01$ & $0.57 \pm 0.01$ \\
% \bottomrule
% \end{tabular}
% \end{table}





% \begin{table}
% \caption{Reach}
% \label{tab:reach}
% \centering
% \begin{tabular}{l|ccccc}
% \toprule
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
% \midrule
% \texttt{PRE} & $0.32 \pm 0.01$ & $0.31 \pm 0.00$ & $0.00 / 2$& $0.99 \pm 0.00$ & $0.85 \pm 0.03$ \\
% \midrule
% \texttt{RES} & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $2.00 / 2$& $0.98 \pm 0.01$ & $0.98 \pm 0.01$ \\
% \texttt{SP} & $0.98 \pm 0.00$ & $0.98 \pm 0.00$ & $2.00 / 2$& $0.97 \pm 0.00$ & $0.97 \pm 0.00$ \\
% \texttt{DPPO} & $0.99 \pm 0.00$ & $0.99 \pm 0.00$ & $2.00 / 2$& $0.97 \pm 0.03$ & $0.97 \pm 0.03$ \\
% \texttt{DDIM} & $0.93 \pm 0.01$ & $0.94 \pm 0.02$ & $2.00 / 2$& $0.66 \pm 0.33$ & $0.65 \pm 0.32$ \\
% \midrule
% \texttt{RES [MD]} & $0.99 \pm 0.00$ & $0.99 \pm 0.00$ & $2.00 / 2$& $1.00 \pm 0.00$ & $1.00 \pm 0.00$ \\
% \texttt{SP [MD]} & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $2.00 / 2$& $0.97 \pm 0.01$ & $0.97 \pm 0.01$ \\
% \texttt{DDIM [MD]} & $0.98 \pm 0.01$ & $0.98 \pm 0.01$ & $2.00 / 2$& $0.67 \pm 0.43$ & $0.67 \pm 0.43$ \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}
% \caption{Lift}
% \label{tab:lift}
% \centering
% \begin{tabular}{l|ccccc}
% \toprule
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
% \midrule
% \texttt{PRE} & $0.14 \pm 0.01$ & $0.15 \pm 0.01$ & $0.00 / 2$& $0.97 \pm 0.01$ & $0.98 \pm 0.01$ \\
% \midrule
% \texttt{RES} & $1.00 \pm 0.00$ & $0.50 \pm 0.00$ & $1.00 / 2$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
% \texttt{SP} & $0.78 \pm 0.03$ & $0.78 \pm 0.03$ & $0.67 / 2$& $0.98 \pm 0.01$ & $0.98 \pm 0.01$ \\
% \texttt{DPPO} & $1.00 \pm 0.00$ & $0.56 \pm 0.08$ & $1.00 / 2$& $0.02 \pm 0.01$ & $0.00 \pm 0.01$ \\
% \texttt{DDIM} & $0.99 \pm 0.01$ & $0.57 \pm 0.10$ & $1.00 / 2$& $0.05 \pm 0.03$ & $0.01 \pm 0.02$ \\
% \midrule
% \texttt{RES[MD]} & 0.16 & 0.16 & 0.00 & 0.98 & 0.99 \\
% \texttt{SP[MD]} & 0.12 & 0.12 & 0.00 & 1.00 & 1.00 \\
% \texttt{DDIM[MD]} & 0.36 & 0.36 & 0.00 & 1.00 & 0.18 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}
% \caption{Avoid}
% \label{tab:avoid}
% \centering
% \begin{tabular}{l|ccccc}
% \toprule
% \rowcolor{lightgray} \textbf{Method} & SR & SR\_M & Cov@0.80 & $\mathcal{H}$ & $\mathcal{H}_\text{succ}$ \\
% \midrule
% \texttt{PRE} & $1.00 \pm 0.00$ & $0.98 \pm 0.02$ & $23.33 / 24$& $0.72 \pm 0.00$ & $0.72 \pm 0.00$ \\
% \midrule
% \texttt{RES} & $0.98 \pm 0.03$ & $0.04 \pm 0.00$ & $1.00 / 24$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
% \texttt{SP} & $1.00 \pm 0.01$ & $0.09 \pm 0.02$ & $2.00 / 24$& $0.01 \pm 0.00$ & $0.01 \pm 0.00$ \\
% \texttt{DPPO} & $1.00 \pm 0.00$ & $0.04 \pm 0.00$ & $1.00 / 24$& $0.00 \pm 0.00$ & $0.00 \pm 0.00$ \\
% \texttt{DDIM} & $1.00 \pm 0.00$ & $0.26 \pm 0.11$ & $6.33 / 24$& $0.13 \pm 0.15$ & $0.13 \pm 0.15$ \\
% \midrule
% \texttt{RES [MD]} & $0.99 \pm 0.01$ & $0.30 \pm 0.02$ & $7.33 / 24$& $0.53 \pm 0.01$ & $0.53 \pm 0.01$ \\
% \texttt{SP [MD]} & $1.00 \pm 0.00$ & $0.42 \pm 0.00$ & $10.00 / 24$& $0.58 \pm 0.00$ & $0.58 \pm 0.00$ \\
% \texttt{DDIM [MD]} & $0.94 \pm 0.07$ & $0.43 \pm 0.05$ & $9.67 / 24$& $0.57 \pm 0.01$ & $0.57 \pm 0.01$ \\
% \bottomrule
% \end{tabular}
% \end{table}



