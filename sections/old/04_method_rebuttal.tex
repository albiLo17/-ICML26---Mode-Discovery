\section{Mode Discovery for \ac{rl} Finetuning}
\label{sec:method}

To fine-tune pre-trained diffusion policies while preserving multimodality, our method builds on three components:
% (i) We first introduce a practical definition of multimodality $\mathcal{M}(\cdot)$ in generative policies by making explicit their dependence on latent input noise and deriving a tractable proxy based on conditional mutual information;
(i) we first introduce a \reb{tractable proxy to measure} multimodality  $\mathcal{M}(\cdot)$ in \reb{pre-trained} generative policies based on %conditional
mutual information;
(ii) we then develop an unsupervised mode-discovery procedure by reparameterizing a steering policy $\pi_{\psi}^{\mathcal{W}}(w \mid s)$ through a latent variable $z \in \mathcal{Z}$, enabling us to uncover and control the behavioral modes of the pre-trained policy during training, while also providing an estimate of multimodality through mutual information.
(iii) Finally, we use this estimate to construct a mutual information–based intrinsic reward and combine it with task rewards, regularizing reinforcement learning fine-tuning to improve task performance while explicitly retaining diverse behaviors. In what follows, we describe each component of the method in detail. %An overview of the method combining the steering policy with the pre-trained diffusion policy is illustrated in Figure~\ref{fig:method}.


%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Multimodality in Generative Policies}

% To define multimodality in generative policies such as diffusion and flow-based models, we exploit the fact that these models generate actions by transforming an input noise vector $w \sim \mathcal{N}(0,I)$ through a denoising process conditioned on $s\in \mathcal{S}$. %, denoted $\pi_\theta(a \mid s,w)$. 
% Multimodality can therefore be understood in terms of the diversity of actions induced by different noise seeds $w$. This motivates the following definition:


% \begin{tcolorbox}[
%   enhanced,
%   breakable,
%   float,
%   floatplacement=h!,
%   title=\textbf{Definition: Multimodal Policy},
%   colframe=myblue,
%   colback=myblue!8,
%   coltitle=white,
%   parbox=false,
%   left=5pt,
%   right=5pt,
%   grow to left by=3pt,
%   grow to right by=3pt,
%   %
%   toprule=2pt,
%   titlerule=1pt,
%   leftrule=1pt,
%   rightrule=1pt,
%   bottomrule=1pt,
% ]
% A policy $\pi_\theta(a \mid s, w)$ is multimodal in state $s\in \mathcal{S}$ if there exist $w_1, w_2 \in \mathcal{W}$, $w_1   \neq w_2$, such that
% \begin{equation}
%     D\!\left(\pi_\theta(a \mid s, w_1), \pi_\theta(a \mid s, w_2)\right) \geq \delta,
%     \label{def:multimodal}
% \end{equation}
% for some distance measure $D$ (e.g.\, total variation, KL, Wasserstein) and threshold $\delta > 0$.
% \end{tcolorbox}

% This distance-based definition captures the intuition that different noise variables $w_1, w_2$ can induce distinct behaviors under the same state $s$. 
% However, because $D$ measures dissimilarity between action distributions, its evaluation is intractable for diffusion and flow-based policies. 


% \paragraph{Mutual Information as a Proxy.}
% To obtain a tractable surrogate, we observe that if a policy is multimodal according to Definition~\ref{def:multimodal}, then the latent $\mathcal{W}$ and the action $\mathcal{A}$ must be statistically dependent given the state $\mathcal{S}$. This implies that the conditional mutual information must be strictly positive (proof in Appendix~\ref{sec:proof_MI_positive})
% \[
%     I(W; A \mid S) = \mathbb{E}_{s \sim p(s)}\!\left[D_{\mathrm{KL}}\!\big(\pi_\theta(a \mid s, w)\,\|\, p(a \mid s)\big)\right] > 0,
% \]
% where $p(a \mid s) = \mathbb{E}_{w \sim p(w)}[\pi_\theta(a \mid s, w)]$ is the marginal action distribution. 
% Multimodality holds whenever $I(W;A\mid S)\geq\delta'$ for some $\delta'>0$ capturing the minimal dependence required. Thus, multimodality $\mathcal{M}(\pi_{\theta})$ can be quantified by the conditional mutual information $I(W;A\mid S)$. 
% Since computing this quantity exactly remains intractable, in the next section we will provide a method to estimate this measure in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Measuring Multimodality in Generative Policies}



\reb{We assume access to a pre-trained generative policy that is inherently multimodal, reflecting the diverse behavioral strategies present in the demonstration dataset. Classical definitions of multimodality characterize modes as local maxima of the explicit action distribution $\pi_\theta(a \mid s)$ \al{cite}, but this view becomes impractical for diffusion or flow-based policies, whose action densities are not available in closed form.}
% \paragraph{Mutual Information as a Proxy.}
To obtain a tractable surrogate, \reb{it is convenient to make explicit the dependence on the policy on the input noise $w \in \mathcal{W}$ and}
observe that \reb{given the assumption that the pre-trained policy is multimodal, then} the latent $\mathcal{W}$ and the action $\mathcal{A}$ must be statistically dependent given the state $\mathcal{S}$. This implies that the conditional mutual information must be strictly positive (proof in Appendix~\ref{sec:proof_MI_positive})
\[
    I(W; A \mid S) = \mathbb{E}_{s \sim p(s)}\!\left[D_{\mathrm{KL}}\!\big(\pi_\theta(a \mid s, w)\,\|\, p(a \mid s)\big)\right] > 0,
\]
where $p(a \mid s) = \mathbb{E}_{w \sim p(w)}[\pi_\theta(a \mid s, w)]$ is the marginal action distribution. 
\reb{Although ${I(W;A\mid S)>0}$ is a valid proxy for multimodality in the pre-trained action distribution, this does not guarantee multimodality in the induced trajectories during fine-tuning as actions multimodality does not necessarily translate into trajectory multimodality. Consequently, this quantity alone is insufficient to guarantee the preservation of behavioral diversity during fine-tuning.} 

\reb{
Drawing inspiration from the skill-discovery literature, which quantifies diversity via the mutual information between a latent variable and the visited states $I(Z;S)$~\citep{gregor2016variational,eysenbach2018diversity,sharma2019dynamics}, we proposed to estimate trajectory multimodality by measuring how the latent input noise $w \in \mathcal{W}$ affects the states reached by the generative policy. Because the transition dynamics induce the Markov structure $\mathcal{W} \to \mathcal{A}_t \to {S}_{t+1}$ given ${S}_t$, the data-processing inequality gives $I({W};{A}_t \mid {S}_t) \;\ge\; I({W};{S}_{t+1} \mid {S}_t)$. Thus, $
I(W;S_{t+1})
=
\mathbb{E}_{p(s_t)}
\!\left[
  I(W;S_{t+1}\mid S_t = s_t)
\right],
$ serves as an information-theoretic lower bound on the action-level dependence and provides a principled surrogate for assessing trajectory-level multimodality.

Unlike skill-discovery methods, whose goal is to \emph{learn} a latent space of skills from scratch, our setting begins with a pre-trained generative policy whose latent noise already encodes multiple behavioral modes. These modes are implicit as their structure is unknown and not explicitly parameterized, but they manifest through the way different noise inputs induce different action sequences and state transitions. Our objective is therefore not to discover new skills, but to \emph{preserve} the multimodal structure already embedded in the diffusion policy during fine-tuning.
} Since computing $I(W;S)$ exactly remains intractable, in the next section we will provide a method to estimate it in practice.


\subsection{Mode Discovery of pre-trained Generative Policies}

% \al{This part could be rewritten by putting emphasis on discovering the latent structure at trajectory level modalities. So connecting different state-dependent modes.}

% \al {I think it might be worth restructuring the logic of this section by first introducing a variational lower bound for computing MI over the latent noise as in skill discovery. However, this is not that useful as it does not allow for controllability and would require the inference model to learn the structure of the noise space at each time step of the MDP, making it prone to overfitting. We thus introduce the latent reparameterization that abstracts temporal clusters in the noise over a trajectory level latent, which makes it easier learning the inference model as well as allows for controllability over the latent space by sampling specific z at the beginning of each episode and allowing for uncovering latent modes. }

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.98\linewidth]{iclr2026/figures/04_method/method.pdf}
%     \caption{ \al{Improve the figure.} \textbf{Unsupervised Mode Discovery via Latent Reparameterization of a Steering Policy.} 
%      A latent variable $z \in \mathcal{Z}$ steers the pre-trained diffusion policy by shaping the noise distribution $\mathcal{W}$ via the steering policy $\pi_\psi(w \mid s, z)$. The input noise $w$ is then mapped through the actor $\pi_\theta(a \mid s, w)$ to produce diverse actions $a \in \mathcal{A}$. This reparameterization of the steering policy via $\mathcal{Z}$ organizes the unstructured noise space $\mathcal{W}$ into controllable behavioral modes. To ensure these modes remain distinct, a discriminator $q_\phi(z \mid s,a)$ is trained to recover $z$ from state–action pairs, yielding a variational estimate of $I(Z;A \mid S)$. This mutual-information signal acts as an intrinsic reward during RL fine-tuning, aligning task adaptation with multimodality preservation.
% }
%     \label{fig:method}
% \end{figure}





%In the previous section, we defined a measure of multimodality $\mathcal{M}$ via the conditional mutual information ${I(W;A\mid S)}$ between the diffusion policy’s latent noise and the actions induced by the policy $\pi_\theta(a \mid s, w)$. 
Directly optimizing \reb{$I(W;S)$} %$I(W;A\mid S)$ 
is impractical as the implicit structure of $\mathcal{W}$ varies at every time-step and for each action-chunk, whereas the multimodal behaviors we are interested in emerge at the trajectory level. 
Maximizing \reb{$I(W;S)$} would therefore encourage the policy to exploit arbitrary noise variations at each time step rather than to capture semantically distinct modes. 
To overcome this problem and simultaneously obtain a structured and controllable representation, we introduce MD-MAD (\emph{\textbf{M}ode \textbf{D}iscovery for \textbf{M}ultimodal \textbf{A}ction \textbf{D}istributions}), which reparameterizes a steering policy with a latent variable $z \in \mathcal{Z}$ that organizes $\mathcal{W}$ into trajectory-level modes.

% \al{Maybe some comments on controllability? Or better, as the modes are implicit in the  }


\paragraph{Latent Reparameterization.}

Let $\pi_\psi^{\mathcal{W}}(w\mid s)$ denote a steering policy that selects the latent noise $w\in\mathcal{W}$ seeding the denoising process. We introduce a latent variable $z\in\mathcal{Z}$ and define a latent-conditioned steering policy $\pi_\psi^{\mathcal{W}}(w\mid s,z)$, which induces the family of action distributions
\begin{equation}
    \pi_{\theta,\psi}(a\mid s,z) \;=\; \int \pi_\theta(a\mid s,w)\,\pi_\psi^{\mathcal{W}}(w\mid s,z)\,dw.
    \label{eq:steered_policy}
\end{equation}
Distinct values of $z$ can therefore select different behaviors (modes) encoded by the \emph{fixed} pretrained policy $\pi_\theta(a\mid s,w)$. 
Under this reparameterization, multimodality is measured by the mutual information \reb{$I(Z;S).$}
%$    I(Z;A\mid S) \;=\; \E_{s\sim p(s)}\!\Big[ D_{\mathrm{KL}}\big(\pi_{\theta,\psi}(a\mid s,z)\,\|\,p(a\mid s)\big) \Big].$
When $\pi_\psi^{\mathcal{W}}(w\mid s,z)$ is deterministic, $I(Z;S) \leq I(W; S)$ by invariance under deterministic transforms, with equality if $Z$ is injective in $W$. Thus, any lower bound on \reb{$I(Z; S)$} is also a valid lower bound on \reb{$I(W; S)$}, providing a direct bridge to our earlier definition. Since the structure of $\mathcal{W}$ is unknown and no mode labels are available,  the steering policy mapping $\mathcal{Z}$ to $\mathcal{W}$  uncovering the behavioral modes implicit in the pretrained policy must be learned in an unsupervised manner.
% The structure of the latent space $\mathcal{Z}$, however, is not known a priori. 
% In particular, we do not have access to labels or language descriptions of the modes present in the demonstrations. 
% Thus, $\mathcal{Z}$ must be discovered in an unsupervised manner, by learning a representation that organizes $z$ into distinct behavioral modes implicit in the pretrained policy. 
%This motivates the use of objectives inspired by unsupervised skill discovery to induce structure in $\mathcal{Z}$. 

\paragraph{Variational Lower Bound.}
% \al{TODO: adjust the lower bound.}
To optimize $\pi_\psi^{\mathcal{W}}$ via \reb{$I(Z;  S)$} %is intractable, since it requires access to the marginal distribution $p(a \mid s)$. 
we follow standard practice in skill discovery~\citep{eysenbach2018diversity}, \reb{which} derives a variational lower bound on the mutual information by introducing an inference model $q_\phi(z \mid s)$ that approximates the posterior over latent codes. This yields
% \begin{align}
%     I(Z; A \mid S) 
%     &= \mathbb{E}_{s,z,a} \left[ \log \frac{p(z \mid s,a)}{p(z)} \right] \\
%     &\geq \mathbb{E}_{s,z,a} \left[ \log q_\phi(z \mid s,a) - \log p(z) \right],
%     \label{eq:variational_MI}
% \end{align}
\begin{align}
    I(Z; S) 
    &= \mathbb{E}_{p(s,z)} \left[ \log \frac{p(z \mid s)}{p(z)} \right] %\\
    % &= \mathbb{E}_{p(s,z)} \left[ \log q_\phi(z \mid s) - \log p(z) \right]
    %    + \mathbb{E}_{p(s)} \left[ \KL\big( p(z \mid s) \,\Vert\, q_\phi(z \mid s) \big) \right] \\
    \geq \mathbb{E}_{p(s,z)} \left[ \log q_\phi(z \mid s) - \log p(z) \right],
    \label{eq:variational_MI}
\end{align}

%where $(s,z)$ are sampled from \al{fix} the steered policy $\pi_\theta(a \mid s,z)$ and prior $p(z)$. 

where $p(s,z)$ denotes the joint distribution induced by sampling 
$z \sim p(z)$ and rolling out a policy $\pi(a \mid s, z)$ in the environment.
\reb{We refer to ~\cite{eysenbach2018diversity} for the derivation of Equation~\ref{eq:variational_MI}.} %The full derivation is provided in Appendix~\ref{sec:MILBO}, while a discussion on connections with the skill discovery literature is in Appendix~\ref{appendix:skill_discovery_discussion}. 
%In practice, $q_\phi(z\mid s,a)$ is trained as $q_\phi(z\mid s)$: although dynamics are not strictly deterministic, state variability at fixed actions is minor and does not drive mode differentiation, so excluding $a$ reduces complexity without compromising the ability of $z$ to capture trajectory-level multimodality. %In practice, the inference model $q_\phi(z \mid s,a)$ is optimized by minimizing the negative log-likelihood of the latent code given only the states: $q_\phi(z \mid s)$. While the environment dynamics are not strictly deterministic, the variability induced by repeating the same action in a given state is typically minor, and thus does not contribute significantly to mode differentiation. Dropping the explicit dependence on $a$ therefore avoids complexity without compromising the ability of $z$ to capture trajectory-level multimodality.

% \al{THIS COULD GO AND WE COULD HAVE A BRIEF COMMENT ON THIS< AS IT IS ALSO IN THE FIGURE: The resulting log-posterior likelihood, ${\log q_\phi(z \mid s)}$, serves as an intrinsic reward that is fed back to the steering policy. Hence, the policy $\pi_\theta$ is trained with standard RL objectives but under a reward signal that directly reflects the ability of its actions to make $z$ identifiable. This joint training dynamic establishes a positive feedback loop: the discriminator $q_\phi$ improves at classifying the latent codes, while the policy is incentivized to generate trajectories that are maximally discriminable with respect to $z$. As a result, the agent is driven to learn a structured and meaningful latent space of modes, where each $z$ corresponds to a consistent and distinguishable mode of behavior.}


% The log-posterior likelihood is used as an intrinsic reward for training the steering policy $\pi_\psi^{\mathcal{W}}$, thereby aligning the RL objective with the identifiability of $z$. This establishes a feedback loop in which $q_\phi$ improves at classifying latent codes while the policy is incentivized to produce trajectories that are consistent and discriminable, yielding a structured latent space where each $z$ corresponds to a distinct mode of behavior.  An overview of the method for mode discovery is illustrated in Figure~\ref{fig:method}.


The log-posterior likelihood is used as an intrinsic reward for training the steering policy $\pi_\psi^{\mathcal{W}}$, thereby aligning the RL objective with the identifiability of $z$. This establishes a feedback loop in which $q_\phi$ improves at classifying latent codes while the $\pi_\psi^{\mathcal{W}}$ is incentivized to \reb{select $w\in \mathcal{W}$ so as to induce} trajectories that are consistent and discriminable. %, yielding a structured latent space where each $z$ corresponds to a distinct mode of behavior.  
An overview of the method for mode discovery is illustrated in Figure~\ref{fig:method}.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{iclr2026/figures/04_method/steering_policy_v2.pdf}
    \caption{ 
    % \textbf{Unsupervised Mode Discovery via Latent Reparameterization of a Steering Policy.}      
    % \al{Reference the figure!} We jointly train an inference model $q_\phi(z \mid s_{t})$ and a steering policy $\pi_\psi(w \mid s, z)$ to discover latent behavioral modes  $z \in \mathcal{Z}$ in the pre-trained actor $\pi_\theta(a \mid s, w)$. 
    % The inference model predicts the probability that the next state belongs to each mode, forcing the steering policy to generate discriminable behaviors associated with latent variables $z$. Each $z$ is sampled from a uniform distribution once at the beginning of an episode, ensuring that modes correspond to self-consistent strategies. 
    % The steering policy uses $z$ to structure the noise space $\mathcal{W}$ and select noise variables $w$, which the frozen actor maps into diverse actions $a \in \mathcal{A}$. 
    % Training the inference model to recover $z$ allows for computing a variational estimate of the conditional mutual information $I(Z;A \mid S)$ (see Equation~\ref{eq:variational_MI}), which serves as an intrinsic reward during RL fine-tuning, aligning task adaptation with multimodality preservation.
    \textbf{Unsupervised Mode Discovery via Latent Reparameterization of a Steering Policy.}  
An inference model $q_\phi(z \mid s)$ and a steering policy $\pi^{\mathcal{W}}_\psi(w \mid s,z)$ are trained jointly to uncover latent modes $z \in \mathcal{Z}$ in the frozen diffusion actor $\pi_\theta(a \mid s,w)$.  
The steering policy structures the noise space $\mathcal{W}$ according to $z$, inducing diverse actions $a \in \mathcal{A}$, while the inference model recovers $z$ to provide a variational estimate of $I(Z;A \mid S)$ (Eq.~\ref{eq:variational_MI}), used as an intrinsic reward during mode discovery and fine-tuning.
    }
    \label{fig:method}
\end{figure}



\subsection{Policy Fine-tuning with Intrinsic Reward}
Recall from Section~\ref{sec:problem_formulation} that we formulated fine-tuning as maximizing task return regularized by a multimodality measure $\mathcal{M}$.  Building on this, the variational lower bound introduced above provides a tractable instantiation of $\mathcal{M}$, which is leveraged as an intrinsic signal to preserve multimodality during fine-tuning.  Concretely, we define the augmented reward
\begin{equation}
    r_{\text{total}}(s,z) = r_{\mathrm{env}}(s,a) 
    + \lambda \Big( \log q_\phi(z \mid s) - \log p(z) \Big),
\end{equation}
where $r_{\mathrm{env}}$ is the environment reward and $\lambda \geq 0$ balances task performance with multimodality preservation. %The extrinsic term promotes task success, while the intrinsic term encourages different latent codes $z$ to induce distinguishable behaviors, thereby preventing mode collapse. 
Directly combining task and intrinsic rewards may lead to premature collapse if the task signal dominates before the multimodal structure is discovered. We therefore adopt a two-stage scheme: first, the steering policy is trained with the intrinsic objective alone to uncover the modes of the pretrained policy; then the environment reward is introduced to guide fine-tuning toward high-return behaviors without destroying diversity.  During mode discovery, we also apply a short-to-long horizon curriculum to stabilize learning.
Algorithm~\ref{alg:mode_finetuning}  summarizes the overall procedure, \reb{which is thoroughly described} in Appendix~\ref{appendix:algorithm}. 
%The steering policy $\pi^{\mathcal{W}}(w\mid s,z)$ is updated using the augmented reward $r_{\text{total}}$, while the discriminator $q_\phi(z \mid s,a)$ is trained to maximize the variational lower bound, providing the intrinsic reward signal. 
While we adopt PPO~\citep{schulman2017proximal} in our experiments, our framework is agnostic to the specific RL algorithm used. % to train the steering policy and is compatible with both on-policy and off-policy methods.


\paragraph{Broader Use of the Framework.}
While the formulation above fine-tunes the generative model indirectly via the steering policy, the framework is not limited to this case. Because the steering head actively explores diverse input-noise regions while pursuing reward, it can be combined with direct fine-tuning of the diffusion weights, acting as a structured exploration agent. At test time, the steering policy can either be retained—allowing explicit control over the behavioral mode—or removed, reverting to random sampling from the noise prior. Furthermore, while outside the present study, the learned latent space $\mathcal{Z}$ provides a natural basis for grounding semantic labels (e.g.\, language instructions) when limited annotations are available. 


\input{iclr2026/sections/04Algo_method}

% \al{Introduce here the fact that we will focus on diffusion policies but it is straightforward to extend to flow based methods, which are also by definition deterministic.}

