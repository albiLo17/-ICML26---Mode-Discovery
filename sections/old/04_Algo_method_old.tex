
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   VERY LONG AND EXTENSVE PPO ALGORITHM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{algorithm}[H]
% \caption{PPO for Steering Diffusion via Latent Modes and MI Reward (frozen $\pi_{\theta}$)}
% \begin{algorithmic}[1]
% \Require Frozen diffusion policy $\pi_{\theta}(\ra \mid \rs,\rw)$; steering policy $\pi^{\mathcal{W}}_{\psi}(\rw \mid \rs,\rz)$; discriminator $q_{\phi}(\rz \mid \tilde{\rs})$; critic $V_{\omega}(\rs,\rz)$; prior $p_{\mathcal{Z}}$; PPO epochs $K$, minibatch $M$, clip $\epsilon$, discount $\gamma$, GAE $\lambda$, weights $c_V, c_{\text{ent}}$, intrinsic weight $\beta$
% \State \textbf{Per-episode latent:} at episode start, sample $\rz \sim p_{\mathcal{Z}}$
% \State \textbf{Rollout:} collect tuples $(\rs,\rz,\rw,\ra,r^{\text{env}},\rs')$ where
%        $\rw \sim \pi^{\mathcal{W}}_{\psi}(\cdot \mid \rs,\rz)$ and
%        $\ra \sim \pi_{\theta}(\cdot \mid \rs,\rw)$;
%        store $\log \pi^{\mathcal{W}}_{\psi_{\text{old}}}(\rw \mid \rs,\rz)$
% \State \textbf{Intrinsic reward (MI lower bound):}
%        for each stored step, form $\tilde{\rs}$ (either $\rs$ or $(\rs,\ra)$) and compute
%        \[
%          r^{\text{int}} \;\gets\; \log q_{\phi}(\rz \mid \tilde{\rs}) \;-\; \log p_{\mathcal{Z}}(\rz)
%        \]
%        then use the combined reward $r \gets r^{\text{env}} + \beta\, r^{\text{int}}$
% \State \textbf{GAE (indexed):}
%        on each temporally ordered slice $\{(\rs_t,\rz,\rw_t,\ra_t,r_t,\rs_{t+1})\}_{t=0}^{T-1}$, compute
%        $\hat v_t \!=\! V_{\omega}(\rs_t,\rz)$, $\hat v_T \!=\! V_{\omega}(\rs_T,\rz)$,
%        $\delta_t \!=\! r_t + \gamma \hat v_{t+1} - \hat v_t$,
%        $\hat A_T\!=\!0$, $\hat A_t \!=\! \delta_t + \gamma\lambda \hat A_{t+1}$ (for $t{=}T{-}1\dots 0$),
%        targets $\hat R_t \!=\! \hat A_t + \hat v_t$
% \For{$k=1$ to $K$} \Comment{optimize steering policy, critic, and discriminator}
%   \State Sample minibatch $\mathcal{B}$ of size $M$ from the buffer
%   \State \textbf{PPO ratios for steering:}\quad
%         $\ell(\rs,\rz,\rw) \gets
%         \exp\!\big(\log \pi^{\mathcal{W}}_{\psi}(\rw \mid \rs,\rz) - \log \pi^{\mathcal{W}}_{\psi_{\text{old}}}(\rw \mid \rs,\rz)\big)$
%   \State \textbf{Losses:}
%   \[
%   \begin{aligned}
%     L_{\text{policy}}(\psi)
%       &\,=\, -\,\E_{(\rs,\rz,\rw,\hat A)\in\mathcal{B}}
%          \Big[\min\!\big(\ell\,\hat A,\; \text{clip}(\ell,1{-}\epsilon,1{+}\epsilon)\,\hat A\big)\Big],\\[2pt]
%     L_{V}(\omega)
%       &\,=\, \E_{(\rs,\rz,\hat R)\in\mathcal{B}}\big[(V_{\omega}(\rs,\rz) - \hat R)^2\big],\\[2pt]
%     L_{\text{ent}}(\psi)
%       &\,=\, -\,\E_{(\rs,\rz)\in\mathcal{B}}\big[\mathcal{H}(\pi^{\mathcal{W}}_{\psi}(\cdot \mid \rs,\rz))\big],\\[2pt]
%     L_{\text{disc}}(\phi)
%       &\,=\, -\,\E_{(\tilde{\rs},\rz)\in\mathcal{B}}\big[\log q_{\phi}(\rz \mid \tilde{\rs})\big],\\[2pt]
%     L_{\text{ppo}}(\psi,\omega)
%       &\,=\, L_{\text{policy}}(\psi) \;+\; c_V\,L_V(\omega) \;+\; c_{\text{ent}}\,L_{\text{ent}}(\psi).
%   \end{aligned}
%   \]
%   \State \textbf{Updates:}\quad
%          update $(\psi,\omega)$ by gradient steps minimizing $L_{\text{ppo}}(\psi,\omega)$; \quad
%          update $\phi$ minimizing $L_{\text{disc}}(\phi)$
% \EndFor
% \end{algorithmic}
% \end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[H]
% \vspace{-1.0em}
% \centering
% \begin{minipage}{\linewidth}
% \begin{algorithm}[H]
%   \caption{Steering Diffusion via Latent Modes and Mutual Information}
%   \begin{algorithmic}[1]
%     \State \textbf{Input:} frozen diffusion policy $\pi_{\theta}(\ra \mid \rs,\rw)$, steering policy $\piw(\rw \mid \rs,\rz)$, discriminator $q_{\phi}(\rz \mid \rs,\ra)$, critic $V_{\omega}(\rs,\rz)$
%     \For{episode $=1,\dots,T$}
%       \State Sample $\rz \sim p(\rz)$
%       \State Sample $\rw \sim \piw(\rw \mid \rs,\rz)$
%       \State Sample $\ra \sim \pi_{\theta}(\ra \mid \rs,\rw)$
%       \State Observe $r^{\text{env}}, \rs'$
%       \State Compute $r^{\text{int}} = \log q_{\phi}(\rz \mid \rs,\ra) - \log p(\rz)$
%       \State Set $r = r^{\text{env}} + \beta r^{\text{int}}$
%       \State Compute advantages $\hat A_t$ and targets $\hat R_t$ via GAE
%       \State Update $\piw, V_{\omega}$: PPO loss
%       \State Update $q_{\phi}$: $L_{\text{disc}} = - \E[\log q_{\phi}(\rz \mid \rs,\ra)]$
%     \EndFor
%   \end{algorithmic}
% \end{algorithm}
% \vspace{-1.5em}
% \end{minipage}
% \end{figure}

% \begin{figure}[H]
% \vspace{-1.0em}
% \centering
% \begin{minipage}{\linewidth}
% \begin{algorithm}[H]
%   \caption{Mode Discovery for Policy Fine-tuning}
%   \begin{algorithmic}[1]
%     \State \textbf{Input:} frozen diffusion policy $\pi_{\theta}(\ra \mid \rs,\rw)$, steering policy $\piw(\rw \mid \rs,\rz)$, discriminator $q_{\phi}(\rz \mid \rs,\ra)$, critic $V_{\omega}(\rs,\rz)$
%     \For{episode $=1,\dots,T$}
%       \State Sample latent $\rz \sim p(\rz)$; 
%       \State Rollout with $\rw \sim \piw(\rs,\rz)$, $\ra \sim \pi_{\theta}(\rs,\rw)$; 
%       \State $r = \log q_{\phi}(\rz \mid \rs,\ra) - \log p(\rz)$;\;\
%       \State Compute advantages $\hat A$ and targets $\hat R$ via GAE
%       \State Get losses: \[
%   \begin{aligned}
%   L_{\piwg}(\psi)
%     &\,=\, -\,\E_{(\rs,\rz,\rw,\hat A)\in\mathcal{B}} \Bigg[
%       \min\!\Bigg(
%         \frac{\piw(\rw \mid \rs,\rz)}{\pi^{\mathcal{W}}_{\psi_{\text{old}}}(\rw \mid \rs,\rz)}\,\hat A,\;
%         \text{clip}\!\left(
%           \frac{\piw(\rw \mid \rs,\rz)}{\pi^{\mathcal{W}}_{\psi_{\text{old}}}(\rw \mid \rs,\rz)},
%           1{-}\epsilon,\,1{+}\epsilon
%         \right)\hat A
%       \Bigg)\Bigg],\\[2pt]
%     L_{V}(\omega)
%       &\,=\, \E_{(\rs,\rz,\hat R)\in\mathcal{B}}\big[(V_{\omega}(\rs,\rz) - \hat R)^2\big].
%   \end{aligned}
%   \]
%   \State Update $\piw,\; V_{\omega}$: \; $
%           \min_{\psi,\omega}\; L_{\piwg}(\psi) + c_V L_V(\omega) + c_{\mathcal{H}} L_{\mathcal{H}}(\psi)
%         $
%       \State Update $q_{\phi}$:
%         \[
%           \min_{\phi}\; L_{q}(\phi) = -\E[\log q_{\phi}(\rz \mid \rs,\ra)]
%         \]
%     \EndFor
%   \end{algorithmic}
% \end{algorithm}
% \vspace{-1.5em}
% \end{minipage}
% \end{figure}

% \begin{figure}[t]
% \centering
% \begin{minipage}{\linewidth}
% \begin{algorithm}[H]
%   \caption{Mode Discovery and Fine-tuning with Multimodality Preservation}
%   \begin{algorithmic}[1]
%     \State \textbf{Input:} frozen diffusion policy $\pi_{\theta}(a \mid s,w)$, 
%            steering policy $\pi^{\mathcal{W}}_{\psi}(w \mid s,z)$, 
%            discriminator $q_{\phi}(z \mid s,a)$, 
%            critic $V_{\omega}(s,z)$
%     \State \textbf{Initialize:} latent prior $p(z)$
%     \vspace{0.5em}

%     % ----------------------
%     \State \textbf{Stage 1: Mode Discovery (Intrinsic Reward Only)}
%     \For{iteration $=1,\dots,T_{\text{disc}}$}
%       \State Collect trajectories with $z \sim p(z)$, 
%              $w \sim \pi^{\mathcal{W}}_{\psi}(w \mid s,z)$, 
%              $a \sim \pi_{\theta}(a \mid s,w)$
%       \State Assign intrinsic reward: 
%       \[
%           r_{\text{int}}(s,a,z) = \log q_{\phi}(z \mid s,a) - \log p(z)
%       \]
%       \State Estimate advantages $\hat A$ and returns $\hat R$ (e.g. with GAE)
%       \State Update steering policy $\pi^{\mathcal{W}}_{\psi}$ and critic $V_{\omega}$:
%       \[
%           \min_{\psi,\omega}\; L_{\pi}^{\text{PPO}}(\psi;\hat A) 
%           + c_V L_V(\omega;\hat R) 
%           + c_{\mathcal{H}} L_{\mathcal{H}}(\psi)
%       \]
%       \State Update discriminator $q_{\phi}$:
%       \[
%           \min_{\phi}\; L_{q}(\phi) = -\E[\log q_{\phi}(z \mid s,a)]
%       \]
%     \EndFor
%     \vspace{0.5em}

%     % ----------------------
%     \State \textbf{Stage 2: Fine-tuning with Task Reward}
%     \For{iteration $=1,\dots,T_{\text{ft}}$}
%       \State Collect trajectories with $z \sim p(z)$, 
%              $w \sim \pi^{\mathcal{W}}_{\psi}(w \mid s,z)$, 
%              $a \sim \pi_{\theta}(a \mid s,w)$
%       \State Assign augmented reward: 
%       \[
%           r_{\text{total}}(s,a,z) = r_{\mathrm{env}}(s,a) + 
%           \lambda \Big( \log q_{\phi}(z \mid s,a) - \log p(z) \Big)
%       \]
%       \State Estimate advantages $\hat A$ and returns $\hat R$
%       \State Update steering policy and critic as in Stage 1, using $r_{\text{total}}$
%       \State Update discriminator $q_{\phi}$ as in Stage 1
%     \EndFor

%   \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \end{figure}



\begin{algorithm}[t!]
\small
\caption{Mode Discovery and Fine-Tuning of Diffusion Policies \al{Fix and shorten.}}
\label{alg:mode_finetuning}
\begin{algorithmic}[1]
\State \textbf{Inputs:} pre-trained diffusion policy $\pi_{\theta}(a\mid s,w)$; steering policy $\pi^{\mathcal{W}}_{\psi}(w\mid s,z)$; discriminator $q_{\phi}(z\mid s,a)$; critic $V_{\omega}(s,z)$; latent prior $p(z)$ %; discriminator noise std $\sigma_{\text{reg}}$
\State \textbf{Init:} $\psi,\phi,\omega$; set $\lambda \ge 0$

\Statex
\State \textbf{Stage 1: Mode discovery}
\For{$i = 1$ \textbf{to} $E_{\text{disc}}$} \Comment{epochs}
  \For{$j = 1$ \textbf{to} $N_{\text{episodes}}$} \Comment{episodes per epoch}
    \State Sample $z \sim p(z)$; roll out on-policy:
    \State \quad $w_t \sim \pi^{\mathcal{W}}_{\psi}(w\mid s_t,z)$,\quad $a_t \sim \pi_{\theta}(a\mid s_t,w_t)$,\quad $s_{t+1}\sim p(\cdot\mid s_t,a_t)$
    \State Intrinsic reward: $r^{\text{int}}_t \gets \log q_{\phi}(z\mid {s}_{t+1},a_t) - \log p(z)$
  \EndFor
  \State Update actor and critic  using $r_t^{int}$ (PPO): \quad
 $ 
    \min_{\psi,\omega}\; L_{\pi}^{\text{PPO}}(\psi)\;+\;c_V\,L_V(\omega)\;+\;c_{\mathcal{H}}\,L_{\mathcal{H}}(\psi)
$
  \State Update inference model: \quad
  % $ 
  %   \min_{\phi}\; L_q(\phi)\;=\;-\mathbb{E}\big[\log q_{\phi}(z\mid \tilde{s},a)\big],\quad \tilde{s}=s+\xi,\ \xi\sim\mathcal{N}(0,\sigma_{\text{reg}}^2 I)
  % $ 
    $ 
    \min_{\phi}\; L_q(\phi)\;=\;-\mathbb{E}\big[\log q_{\phi}(z\mid s ,a)\big]
  $ 
\EndFor

\Statex
\State \textbf{Stage 2: Fine-tuning with intrinsic reward}
\For{$i = 1$ \textbf{to} $E_{\text{ft}}$} \Comment{epochs}
  \For{$j = 1$ \textbf{to} $N_{\text{episodes}}$} \Comment{episodes per epoch}
    \State Sample $z \sim p(z)$; roll out on-policy:
    \State \quad $w_t \sim \pi^{\mathcal{W}}_{\psi}(w\mid s_t,z)$,\quad $a_t \sim \pi_{\theta}(a\mid s_t,w_t)$,\quad $s_{t+1}\sim p(\cdot\mid s_t,a_t)$
    \State Augmented reward:
    $ 
      r^{\text{tot}}_t \gets r_{\text{env}}(s_t,a_t)\;+\;\textcolor{blue}{\lambda\big(\log q_{\phi}(z\mid {s}_{t+1},a_t) - \log p(z)\big)},
  $ 
  \EndFor
  \State Update actor and critic using \textcolor{blue}{$r_t^{tot}$} (PPO): \quad
  $ 
    \min_{\psi,\omega}\; L_{\pi}^{\text{PPO}}(\psi)\;+\;c_V\,L_V(\omega)\;+\;c_{\mathcal{H}}\,L_{\mathcal{H}}(\psi)
  $ 
  \State Update  :\quad 
    $ 
    \min_{\phi}\; L_q(\phi)\;=\;-\mathbb{E}\big[\log q_{\phi}(z\mid s ,a)\big]
  $ 
\EndFor
\end{algorithmic}
\end{algorithm}


% \begin{figure*}[t]
% \centering
% \begin{minipage}{0.48\linewidth}
% \begin{algorithm}[H]
%   \caption{Mode Discovery via Steering Diffusion}
%   \begin{algorithmic}[1]
%     \State \textbf{Input:} frozen diffusion policy $\pi_{\theta}(\ra \mid \rs,\rw)$, steering policy $\piw(\rw \mid \rs,\rz)$, discriminator $q_{\phi}(\rz \mid \rs,\ra)$, critic $V_{\omega}(\rs,\rz)$
%     \For{episode $=1,\dots,T$}
%       \State Sample latent $\rz \sim p(\rz)$; 
%       \State Rollout with $\rw \sim \piw(\rs,\rz)$, $\ra \sim \pi_{\theta}(\rs,\rw)$; 
%       \State $r = \log q_{\phi}(\rz \mid \rs,\ra) - \log p(\rz)$;\;\
%       \State Compute advantages $\hat A$ and targets $\hat R$ via GAE
%       \State Get losses: \[
%   \begin{aligned}
%    \ell(\rs,\rz,\rw) &\gets
%         \exp\!\big(\log \pi^{\mathcal{W}}_{\psi}(\rw \mid \rs,\rz) - \log \pi^{\mathcal{W}}_{\psi_{\text{old}}}(\rw \mid \rs,\rz)\big) ,\\[2pt]
%     L_{\piwg}(\psi)
%       &\,=\, -\,\E_{(\rs,\rz,\rw,\hat A)\in\mathcal{B}}
%          \Big[\min\!\big(\ell\,\hat A,\; \text{clip}(\ell,1{-}\epsilon,1{+}\epsilon)\,\hat A\big)\Big],\\[2pt]
%     L_{V}(\omega)
%       &\,=\, \E_{(\rs,\rz,\hat R)\in\mathcal{B}}\big[(V_{\omega}(\rs,\rz) - \hat R)^2\big].
%   \end{aligned}
%   \]
%   \State Update $\piw,\; V_{\omega}$:
%         \[
%           \min_{\psi,\omega}\; L_{\piwg}(\psi) + c_V L_V(\omega) + c_{\mathcal{H}} L_{\mathcal{H}}(\psi)
%         \]
%       \State Update $q_{\phi}$:
%         \[
%           \min_{\phi}\; L_{q}(\phi) = -\E[\log q_{\phi}(\rz \mid \rs,\ra)]
%         \]
%     \EndFor
%   \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \hfill
% \begin{minipage}{0.48\linewidth}
% \begin{algorithm}[H]
%   \caption{Steering Diffusion via Latent Modes}
%   \label{alg:steering}
%   \begin{algorithmic}[1]
%     \State Input: frozen $\pi_\theta$, steering $\piw$, discriminator $q_\phi$, critic $V_\omega$
%     \For{episode $=1,\dots,T$}
%       \State Sample $\rz \sim p(\rz)$, $\rw \sim \piw(\rs,\rz)$, $\ra \sim \pi_\theta(\rs,\rw)$
%       \State Compute $r^{\text{int}} = \log q_\phi(\rz|\rs,\ra) - \log p(\rz)$
%       \State Total reward $r = r^{\text{env}} + \beta r^{\text{int}}$
%       \State Update $\piw, V_\omega$: PPO loss
%       \State Update $q_\phi$: $-\E[\log q_\phi(\rz|\rs,\ra)]$
%     \EndFor
%   \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \caption{(Left) Baseline PPO update. (Right) Our method with a steering policy and MI reward.}
% \label{fig:algos}
% \end{figure*}



% % ---------------------------
% % Discrete skill case
% % ---------------------------
% \begin{equation}
% \mathcal{L}_{\text{disc}}^{\text{discrete}}
% = - \frac{1}{B} \sum_{i=1}^B \log q_\phi\bigl(z_i \mid s_i\bigr),
% \end{equation}

% % ---------------------------
% % Continuous skill case (Gaussian)
% % ---------------------------
% \begin{equation}
% \mathcal{L}_{\text{disc}}^{\text{cont}}
% = \frac{1}{2B} \sum_{i=1}^B \sum_{j=1}^d 
% \left[
% \left(\frac{z_{ij} - \mu_{ij}}{\exp(\log\sigma_{ij})}\right)^2
% + 2 \log\sigma_{ij}
% + \log (2\pi)
% \right],
% \end{equation}

% \begin{equation}
% \mathcal{L}_{\text{disc}}
% = - \mathbb{E}_{(s,z) \sim \mathcal{D}} 
% \big[ \log q_\phi(z \mid s) \big],
% \end{equation}

% % =========================================
% % PPO with explicit reward dependence
% % =========================================

% % Trajectory and rewards
% \newcommand{\one}{\mathbf{1}}

% \begin{align}
% G_t 
% &= \sum_{k=0}^{\infty} \gamma^{k}\, r_{t+k} \quad &&\text{(discounted return from time $t$)} \label{eq:return}\\[4pt]
% A_t^{\text{MC}}
% &= G_t - V_\psi(s_t) \quad &&\text{(MC advantage; depends on rewards via $G_t$)} \label{eq:adv-mc}
% \end{align}

% % (Optional) GAE advantages (also explicitly built from rewards)
% \begin{align}
% \delta_t 
% &= r_t + \gamma V_\psi(s_{t+1}) - V_\psi(s_t), \label{eq:td-error}\\[4pt]
% A_t^{\text{GAE}(\lambda)} 
% &= \sum_{l=0}^{\infty} (\gamma\lambda)^l \, \delta_{t+l}. \label{eq:gae}
% \end{align}

% % Policy ratio
% \begin{equation}
% r_t(\theta) \;=\; 
% \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}. \label{eq:ratio}
% \end{equation}

% % Clipped surrogate (maximize)
% \begin{equation}
% \mathcal{L}_{\text{clip}}(\theta) \;=\;
% \E_t \!\left[
% \min\!\Big(
% r_t(\theta)\, A_t,\;
% \operatorname{clip}\!\big(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\, A_t
% \Big)
% \right], 
% \quad A_t \in \{A_t^{\text{MC}},\, A_t^{\text{GAE}(\lambda)}\}. \label{eq:ppo-clip}
% \end{equation}

% % Value loss and entropy bonus
% \begin{align}
% \mathcal{L}_{V}(\psi) 
% &= \E_t \!\left[\big( V_\psi(s_t) - G_t \big)^2\right], \label{eq:value-loss}\\[4pt]
% \mathcal{L}_{\text{ent}}(\theta) 
% &= \E_t \!\left[ \mathcal{H}\!\left(\pi_\theta(\cdot \mid s_t)\right) \right]. \label{eq:entropy}
% \end{align}

% % Full PPO objective (maximize). If you prefer a minimization loss, negate it.
% \begin{equation}
% \mathcal{L}_{\text{PPO}}(\theta,\psi)
% \;=\; 
% \mathcal{L}_{\text{clip}}(\theta)
% \;-\; c_V\, \mathcal{L}_{V}(\psi)
% \;+\; c_H\, \mathcal{L}_{\text{ent}}(\theta). \label{eq:ppo-full}
% \end{equation}

% % =========================================
% % Critic training loss in PPO
% % =========================================

% % % (1) Monte-Carlo return target
% % \begin{equation}
% % \mathcal{L}_{V}^{\text{MC}}(\psi)
% % = \mathbb{E}_t \Big[ \big( V_\psi(s_t) - G_t \big)^2 \Big],
% % \quad
% % G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}.
% % \end{equation}

% % (2) GAE(λ) target (λ-return)
% \begin{align}
% \delta_t 
% = r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t), \label{eq:td-error}\\[4pt]
% A_t^{\text{GAE}(\lambda)} 
% = \sum_{l=0}^{\infty} (\gamma\lambda)^l \, \delta_{t+l}. \label{eq:gae}
% \end{align}

% \begin{equation}
% \hat{V}^{\lambda}_t \;\equiv\; V_\theta(s_t) + A_t^{\text{GAE}(\lambda)},
% \end{equation}
% \begin{equation}
% \mathcal{L}_{V}^{\lambda}(\theta)
% = \mathbb{E}_t \Big[ \big( V_\theta(s_t) - \hat{V}^{\lambda}_t \big)^2 \Big].
% \end{equation}