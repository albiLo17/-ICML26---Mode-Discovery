\section{Mode Discovery for \ac{rl} Finetuning}
\label{sec:method}



\begin{figure}[t] % t for top, h for here, b for bottom
\centering
\includegraphics[width=0.98\linewidth]{iclr2026/figures/04_method/method.pdf}
\caption{An illustrative example of multimodal trajectories in a manipulation task.}
\label{fig:method}
\end{figure}


\begin{itemize}
    \item Insert method picture and provide an overview
    \item Multimodality definition and MI as proxy
    \item Mode discovery and MI Lower Bound
    \item Fine-tuning with Intrinsic Reward
\end{itemize}

To address the problem of fine-tuning pre-trained diffusion policies while maintaining multimodality, we first provide a more practical definition of multimodal policy for this class of models and motivate its applicability by showing how it can lead to a measure of multimodality without knowing a priori the modality of the policy. Given such a definition of multimodality, we draw inspiration from the skill discovery literature to design a fine-tuning method or training scheme that preserves multimodal modes, presented in Figure~\ref{fig:method}.

\subsection{Multimodality in the Action Distribution}


To address the limitations of the classical definition of multimodality based on identifying local maxima in the action distribution $\pi_\theta(a \mid s)$, we adopt a latent-variable perspective inspired by recent work on steering diffusion policies~\cite{wagenmaker2025steering}.  In diffusion policies, actions are generated by transforming an initial latent noise vector $w \sim \mathcal{N}(0, \mathcal{I})$ through a denoising process conditioned on the state $s$, which we denote explicitly as $\pi_\theta(a \mid s, w)$. We leverage this policy representation to propose the following definition of multimodality: 


\begin{tcolorbox}[
  enhanced,
  breakable,
  float,
  floatplacement=h!,
  title=\textbf{Definition:} Multimodal Policy,
  colframe=myblue,
  colback=myblue!8,
  coltitle=white,
  parbox=false,
  left=5pt,
  right=5pt,
  grow to left by=3pt,
  grow to right by=3pt,
  %
  %
  toprule=2pt,
  titlerule=1pt,
  leftrule=1pt,
  rightrule=1pt,
  bottomrule=1pt,
]

A policy $\pi_\theta(a \mid s)$ has multiple modes for some $s$, if the policy latent space $\mathcal{W}$ is such that:

\begin{equation}
    \exists w_1, w_2 \sim \mathcal{W}, w_1 \neq w_2, D(\pi_\theta(a \mid s, w_1), \pi_\theta(a \mid s, w_2))) \geq \delta
\end{equation}

for some $\delta > 0$, where $D$ is a distance measure between distributions, and $\pi_\theta(a \mid s, w)$ is a pre-trained diffusion policy where the action generation is deterministic given the input noise $w \sim \mathcal{W}$.


\end{tcolorbox}



This definition captures multimodality by requiring that there exist at least two distinct latent noises $w_1, w_2 \sim \mathcal{W}$ which induce sufficiently different action distributions. The function $D$ denotes a distance measure between distributions over actions, such as the total variation distance, Kullback–Leibler (KL) divergence, or Wasserstein distance. The threshold $\delta > 0$ quantifies the degree of dissimilarity required to consider two latent-induced distributions as distinct modes. %Intuitively, this definition reflects the idea that multimodality corresponds to the presence of semantically or behaviorally diverse policy outputs that arise from different latent factors, even though the underlying diffusion policy remains fixed. 
However, the above definition of multimodality is difficult to measure in practice. In the next section, we will draw connections to the skill discovery literature, and we propose to address this issue by leveraging the mutual information between the latent input noise and the generated actions as a proxy for measuring multimodality.

\subsection{Mutual Information as a Proxy for Multimodality}


The definition of multimodality we adopted is centered around the existence of distinct latent-conditioned behaviors: a policy $\pi(a \mid s, w)$ is multimodal at state $s$ if distinct latent noises $w_1, w_2$ induce sufficiently different action distributions. While conceptually clear, this notion is challenging to quantify directly in practice, as it relies on a distace measure $D$ such as a KL divergence, which is intractable to compute for diffusion policies.

To obtain a tractable surrogate, we draw inspiration from the unsupervised skill discovery literature and observe that a latent variable $W$ that meaningfully controls distinct behaviors must exhibit high statistical dependence with the resulting actions $A$ under a fixed context $s \in \mathcal{S}$. This motivates the use of the conditional mutual information $I(W; A \mid S=s)$, which measures the average information that $W$ provides about $A$, given $s$,  as a proxy objective for multimodality. Specifically, high mutual information implies that different values of $w$ result in distinct, identifiable action distributions, capturing our multimodality objective.

\paragraph{Multimodality Implies Positive Mutual Information}

We now provide an intuition of why our definition of multimodality implies that the conditional mutual information $I(W; A \mid S)$ is strictly positive. A more formal proof can be found in Appendix~\ref{sec:proof_MI_positive}. Recall that by definition, a policy $\pi(a \mid s)$ is said to be \emph{multimodal} at state $s$ if there exist $w_1, w_2 \sim \mathcal{W}$, with $w_1 \neq w_2$, such that:
\begin{equation}
    D\left( \pi(a \mid s, w_1), \pi(a \mid s, w_2) \right) \geq \delta,
\end{equation}
for some $\delta > 0$ and distance measure $D$, e.g., total variation or KL divergence. Assume the latent variable $W$ is sampled from a distribution $p(w)$ with full support over $\mathcal{W}$, and actions are sampled from $\pi(a \mid s, w)$. The conditional mutual information is defined as (see Appendix~\ref{sec:app_MI_KL} for the derivation of this expression):
\begin{equation}
    I(W; A \mid S) = \mathbb{E}_{s \sim p(s)} \left[ D_{\mathrm{KL}} \left( \pi(a \mid s, w) \,\|\, p(a \mid s) \right) \right],
\end{equation}
where $p(a \mid s) = \mathbb{E}_{w \sim p(w)} \left[ \pi(a \mid s, w) \right]$ is the marginal action distribution. Now, suppose that for some $s$, there exist $w_1 \neq w_2$ such that $D_{\mathrm{KL}}\left( \pi(a \mid s, w_1) \,\|\, \pi(a \mid s, w_2) \right) \geq \delta > 0$. Then, $\pi(a \mid s, w)$ is not constant in $w$, and so the marginal $p(a \mid s)$ is a non-degenerate mixture. By convexity of KL divergence and Jensen's inequality, we have:
\begin{equation}
    \mathbb{E}_{w \sim p(w)} \left[ D_{\mathrm{KL}} \left( \pi(a \mid s, w) \,\|\, p(a \mid s) \right) \right] > 0,
\end{equation}
which implies that $I(W; A \mid S = s) > 0$. Hence, if the policy is multimodal at any state $s$ in the support of $p(s)$, then the conditional mutual information $I(W; A \mid S)$ is strictly positive. This establishes that multimodality, as defined via distinguishable latent-conditioned behaviors, necessarily implies non-zero mutual information. Thus, $I(W; A \mid S)$ serves as a proxy for multimodality, quantifying the diversity and informativeness of the latent variable with respect to behavior. 


\subsection{Mode Discovery}

Under the assumption that the policy is multimodal, it is possible to learn a steering policy to control the policy's behavior towards distinct modalities. More specifically, a steering policy, denoted $\pi^{\mathcal{W}}_\theta(w \mid s)$, modifies the input noise distribution in a way that influences the resulting action, thereby enabling control over the generative process without modifying the pre-trained diffusion model itself. While prior work leverages a steering policy $\pi^{\mathcal{W}}_\theta(w \mid s)$ to direct the policy toward high-reward behaviors, we instead propose to use it to control different modes of the pre-trained policy, thus controlling the multimodality. To do so, we introduce a reparameterization of the steering policy via a learned latent variable $z \in \mathcal{Z}$, leading to a latent-conditioned steering policy $\pi^{\mathcal{W}}_\theta(w \mid s, z)$. This induces a corresponding action distribution:
\begin{equation}
    \pi_\theta(a \mid s, z) = \int \pi_\theta(a \mid s, w)\ \pi^{\mathcal{W}}_\theta(w \mid s, z)\, dw.
\label{eq:steered_policy}
\end{equation}

This formulation allows distinct latent codes $z$ to induce different action distributions, effectively steering the policy into different modes of behavior. The key insight is that the diffusion policy $\pi_\theta(a \mid s, w)$ remains fixed, and control is exerted solely through the latent-conditioned input noise distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Rewriting the mutual information via the steering latent.}
To quantify the effectiveness of this steering mechanism in capturing and controlling different behavioral modes, we revisit the mutual information objective introduced earlier. While the original formulation considers the dependence between the input noise $W$ and the resulting action $A$ via $I(W; A \mid S = s)$, the reparameterized formulation in terms of $z$ naturally induces an alternative perspective. Specifically, the steered action distribution $\pi_\theta(a \mid s, z)$ defined in Eq.~\eqref{eq:steered_policy} allows us to consider the conditional mutual information $I(Z; A \mid S = s)$ instead. 

Importantly, when the steering policy $\pi^{\mathcal{W}}_\theta(w \mid s, z)$ is deterministic, the two mutual information quantities are equal due to the invariance of mutual information under deterministic transformations:
\[
I(W; A \mid S = s) = I(Z; A \mid S = s).
\]
This equivalence justifies using $I(Z; A \mid S = s)$ as a tractable surrogate for measuring and preserving multimodality, now expressed over a structured latent space $\mathcal{Z}$. Furthermore, this objective can be incorporated as an intrinsic reward signal to encourage the discovery and retention of diverse behaviors when fine-tuning the policy using reinforcement learning. However, the latent space $\mathcal{Z}$ is typically unknown.  

\paragraph{Parallel with Skill Discovery.} 


In unsupervised skill discovery, the objective is to discover distinct behaviors by maximizing the mutual information between latent skills and states:

\begin{equation}
    \max I(Z; S),
\end{equation}

where $Z$ indexes latent skills, and $S$ denotes the state visited by executing the skill-conditioned policy. This encourages $Z$ to capture how different policies influence the state space. In contrast, our focus is on uncovering the diverse modes already present in a pre-trained policy, where the state distribution is fixed and implicitly determined by the policy’s action distribution. Consequently, our objective shifts from exploring diversity in the state space to controlling diversity in the action space, and more precisely, in the latent input space that drives the generative behavior of the diffusion policy. Thus, we consider:

\begin{equation}
    \max I(Z; A \mid S),
\end{equation}

which measures the diversity of actions produced from the same state when varying $Z$. This formulation directly aligns with the operational notion of multimodality: different values of $z$ should yield different action distributions under the same context $s$. \al{It is worth also discussing the advantages of searching in the latent space  $\mathcal{Z}$ VS searching in the latent noise space  $\mathcal{W}$ of a pre-trained policy VS  searching in the full state space  $\mathcal{S}$ of the environment.}


\paragraph{Variational Lower Bound for Mutual Information}

To summarize our formulation so far, we proposed to represent multimodality through the policy latent noise $\mathcal{W}$, and introduced a deterministic mapping (the steering policy) from a structured latent space $\mathcal{Z}$ to $\mathcal{W}$, enabling control over the behavior of a fixed, pre-trained diffusion policy. Multimodality is captured via the conditional mutual information $I(Z; A \mid S)$, which quantifies the influence of $Z$ on the action distribution under a given state $s$. A high value of this mutual information implies that distinct latent codes $z$ induce clearly distinguishable behaviors, corresponding to our operational definition of multimodality.

Crucially, however, the latent space $\mathcal{Z}$ is not given: it must be discovered. If we wish to steer the behavior of the policy through $z$, we must first learn a latent space that enables such control. This motivates maximizing the mutual information $I(Z; A \mid S)$ with respect to both the steering policy and the latent space itself. Unfortunately, this objective is intractable in its exact form due to the difficulty of computing or sampling from the marginal action distribution $p(a \mid s)$.

The objective to be maximized is the following (derivation similar to Appendix~\ref{sec:app_MI_KL}):
\begin{equation}
    I(Z; A \mid S) = \mathbb{E}_{s \sim p(s)} \left[ D_{\mathrm{KL}}\left( \pi(a \mid s, z) \,\|\, p(a \mid s) \right) \right],
\end{equation}
where $p(a \mid s) = \mathbb{E}_{z \sim p(z)}[\pi(a \mid s, z)]$ denotes the marginal action distribution under the latent-conditioned policy. This expression reflects the expected divergence between behavior induced by a particular latent code and the average behavior, thereby measuring the distinctiveness of behaviors associated with each $z$. To make this objective tractable, we derive a variational lower bound of the mutual information by introducing an inference model $q_\phi(z \mid s, a)$ that approximates the posterior over $z$ given observed state-action pairs. Using standard variational arguments, we obtain:
\begin{align}
    I(Z; A \mid S) 
    &= \mathbb{E}_{s \sim p(s), z \sim p(z), a \sim \pi(a \mid s, z)} \left[ \log \frac{p(z \mid s, a)}{p(z)} \right] \\
    &= \mathbb{E}_{s, z, a} \left[ \log q_\phi(z \mid s, a) - \log p(z) \right] + \mathrm{KL}(p(z \mid s, a) \,\|\, q_\phi(z \mid s, a)) \\
    &\geq \mathbb{E}_{s, z, a} \left[ \log q_\phi(z \mid s, a) - \log p(z) \right],
\end{align}
where the final line corresponds to a variational lower bound on the mutual information. A full derivation of this lower bound is in Appendix ~\ref{sec:MILBO}

This bound provides a practical and differentiable training objective that encourages the learned latent variable $z$ to capture distinct, identifiable modes of the policy’s behavior. The term ${\log q_\phi(z \mid s, a)}$ also admits a natural interpretation as an intrinsic reward signal, which we now integrate into reinforcement learning for latent space discovery.


\subsection{Policy Fine-tuning with Intrinsic Reward}


This lower bound can be used to design an augmented RL objective that encourages multimodality during online fine-tuning. Let $\pi_\theta(a \mid s, z)$ be the diffusion policy, and let $r_{\mathrm{env}}(s, a)$ denote the environment reward. We define the total reward:

\begin{equation}
    r_{\text{total}}(s, a, z) = r_{\mathrm{env}}(s, a) + \lambda \left( \log q_\phi(z \mid s, a) - \log p(z) \right),
\end{equation}

where $\lambda$ is a weighting factor balancing extrinsic and intrinsic objectives.

This intrinsic term serves two purposes:

1. \textbf{Preservation of pre-trained multimodality}: When fine-tuning a pretrained diffusion policy, the latent space $\mathcal{Z}$ typically captures structured behavioral modes. The intrinsic reward encourages these modes to remain distinguishable, preventing mode collapse.
   
2. \textbf{Discovery of new modes}: If the latent space is unstructured or randomly initialized, optimizing this mutual information encourages the emergence of distinct behaviors aligned with controllable variation in action-space.

This can be integrated within any \ac{rl} algorithm; in the context of this work, we focus on online \ac{rl} algorithms such as PPO \al{cite}. We first discover the modalities of the pre-trained model using algorithm~\ref{} based on the mutual informaiton. Once that is traned, the original policy can be fine-tuned following a similar approach as before, but where


\input{iclr2026/sections/04Algo_method}

Some of the tricks are pre-trainining hte mode discovery, with or without the task objective, of course with the task objective there is an incentive already so it might be better. The dimension of hte latent space is an added hyperparameter, as well as the coefficient of the MI loss in front of the 