\section{Conclusions, Limitations and Future Work}
We studied the problem of fine-tune pre-trained generative policies with \ac{rl} while preserving multimodal action distributions. Focusing on diffusion policies trained from demonstrations, we showed that standard fine-tuning often collapsed multimodality to a dominant behavior when the fine-tuning reward landscape diverged from the demonstrations. To address this, we proposed using conditional mutual information as a proxy for multimodality and introduced MD\textendash MAD, an unsupervised mode-discovery method based on a latent reparameterization of a steering policy. We then used the steering policy together with the mutual-information estimate to provide an intrinsic reward that regularized \ac{rl} fine-tuning toward retaining diverse behaviors.  We benchmarked the method across different robotics environments, and showcased that the proposed regularization mitigated collapse under reward imbalance, supporting MD\textendash MAD as a practical approach to fine-tuning generative policies without sacrificing behavioral diversity.


\paragraph{Limitations and Future Work.}
Our study revealed several trade-offs and open directions. The intrinsic-reward regularization required careful tuning, as excessive weight slowed learning and reduced task success. Maintaining an inference model during fine-tuning also introduced instabilities, as it needed to track the policy’s shifting state distribution. This was further exacerbated by the sensitivity of the inference model to small state perturbations. A promising next step to address these limitations is to explore techniques from skill discovery that replace mutual-information estimators with metric representations to improve robustness and generalization. 

\reb{Moreover, scaling MD-MAD to large, heterogeneous datasets may require richer latent parametrizations, such as hierarchical or structured latent spaces, to capture dataset-level multimodality, which we identify as an important avenue for future work.}

One of the major failure cases of our proposed method was the inability to retain all modalities in the \emph{Avoid} environment. We hypothesize that using a single latent per trajectory limited adaptation when multimodality emerged late in an episode and could be addressed with hierarchical or time-varying latents that permit mode switches within a rollout. A second failure case arose with highly stochastic action generation (e.g.\, DDPM sampling), where mapping modes to input noise for maximizing mutual information was hindered by the sampling stochasticity, reducing the ability to steer the policy towards consistent behaviors. Exploring stage-aware steering at later diffusion steps or adaptive noise schedules may help mitigate this limitation. 

Finally, although the formulation was independent of language supervision, the learned latent space is amenable to post-hoc semantic grounding. Aligning modes with language via preference learning or VLA mappings and developing a joint inference model that preserves diversity while enabling reliable semantic labels are compelling directions for future work.

\paragraph{Limitations and Future Work.}
\reb{In its current form, MD-MAD is designed as an action-level mode extractor for task-conditioned policies: it assumes access to task labels and focuses on discovering and preserving \emph{within-task} behavioral modes, rather than modeling dataset-level task structure.} 
Moreover, scaling MD-MAD to large, heterogeneous datasets may require richer latent parametrizations, such as hierarchical or structured latent spaces, to capture dataset-level multimodality, \reb{for instance by separating dataset- or task-level variability from task-specific behavioral modes. In all experiments, we employ a single categorical latent $\mathcal{Z}$ indexing discrete behavioral modes within each task. We deliberately use a mildly overparameterized codebook, and our results (e.g., on the Mixture-of-Gaussians benchmark) indicate that MD-MAD can reliably collapse redundant codes and recover the relevant modes, suggesting that overparameterization is not critical in practice. Exploring more expressive continuous or hybrid latent spaces, possibly combined with regularization strategies from recent skill-discovery methods to represent richer or unbounded behavioral diversity, is an important avenue for future work.} 

One of the major failure cases of our proposed method was the inability to retain all modalities in the \emph{Avoid} environment. We hypothesize that using a single latent per trajectory limited adaptation when multimodality emerged late in an episode and could be addressed with hierarchical or time-varying latents that permit mode switches within a rollout. \reb{More generally, our formulation currently introduces $\mathcal{Z}$ as a single categorical index that governs the diffusion noise $\mathcal{W}$; this avoids trivial autoencoding of $\mathcal{W}$ but does not preclude degeneracies in which multiple $z$-values collapse to the same behavior. Such mode collapse is partly due to known limitations of mutual-information-based diversity objectives and we do observe it in some settings. A straightforward extension is to add a KL or entropy regularization term that encourages the inferred posterior $q_\phi(z \mid \tau)$ to match a uniform prior, promoting balanced, controllable usage of all codes. Stronger notions of semantic controllability—such as enforcing independence or disentanglement across latent components via TC-style regularizers—would require extending $\mathcal{Z}$ to a factorized or hierarchical discrete structure, which we leave for future work.} 

A second failure case arose with highly stochastic action generation (e.g.\, DDPM sampling), where mapping modes to input noise for maximizing mutual information was hindered by the sampling stochasticity, reducing the ability to steer the policy towards consistent behaviors. \reb{This highlights that our current diversity regularizer primarily evaluates trajectory-level consistency under fixed sampling procedures and may be brittle when the diffusion sampling noise dominates the controllable signal in $\mathcal{W}$.} Exploring stage-aware steering at later diffusion steps or adaptive noise schedules may help mitigate this limitation. \reb{Overall, our study should be viewed as an initial feasibility demonstration of using diversity regularization to preserve multimodality during fine-tuning; systematically addressing degeneracies, semantic disentanglement of $\mathcal{Z}$, and hierarchical dataset-level structure remains promising future work.}


\section{Reproducibility Statement}

We have made extensive efforts to ensure the reproducibility of our results. All algorithmic details, including model architectures, training procedures, and hyperparameters, are described in the main text and appendix. If any hyperparameter is not explicitly documented in the paper, it will be fully specified in the released code repository. Upon acceptance, we will release the complete codebase, together with configuration files, pretrained checkpoints, and evaluation scripts, to allow exact replication of our experiments. Additionally, proofs of theoretical claims and ablation studies supporting our design choices are included in the appendix.