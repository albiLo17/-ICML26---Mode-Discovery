


\section{Experiments}

Our experimental evaluation is centered around three main questions: (i) Is the mutual information in Equation~\ref{eq:variational_MI} a valid estimate of multimodality? (ii) Do existing fine-tuning techniques preserve multimodality? (iii) Does our method retain multimodality without sacrificing task performance? We evaluate across increasingly challenging domains: a 2D Gaussian-mixture reward landscape; multimodal manipulation in ManiSkill~\citep{tao2024maniskill3} and D3IL~\citep{jia2024towards}; and high-dimensional sequential control (ANYmal locomotion, Franka Kitchen~\citep{fu2020d4rl}). We also ablate key design choices.
 %We evaluated these questions in a progressively more challenging set of environments:  an 2D Gaussian-mixture reward landscape, multimodal manipulation tasks from ManiSkill~\citep{tao2024maniskill3} and D3IL~\citep{jia2024towards}, and additional high-dimensional and sequential domains including ANYmal locomotion and Franka Kitchen~\citep{fu2020d4rl}. We finally included ablations on key design choices. 

% written in question formal
% In this experimental evaluation, we address the following research questions:
% (i) Is the mutual information defined in Equation~\ref{eq:variational_MI} a valid and reliable measure of multimodality?
% (ii) To what extent do existing fine-tuning techniques preserve multimodality in the action distribution?
% (iii) Does our proposed method improve the retention of multimodality without degrading the overall task performance of the policy?
% (iv) How does the dimensionality of the latent space $\mathcal{Z}$ affect the quality of mode discovery and multimodality preservation?

% We evaluated our method in two distinct scenarios: an illustrative setting generating a 2D Gaussian mixture reward landscape, 2) diverse ManiSkill~\citep{tao2024maniskill3} and D3IL~\citep{jia2024towards} \textit{multimodal} tasks. 
% Our experimental evaluation investigates the mode-collapse effect of \ac{rl} fine-tuning under different techniques, and assesses the effectiveness of our approach in uncovering modes of pre-trained policies and preserving multimodal behaviors after fine-tuning. We benchmark task performance relative to competitive baselines, and analyze how our intrinsic reward balances success rates with diversity preservation. In addition, we conduct ablation studies on the most critical design choices to isolate their impact.


\paragraph{Baselines.}
% Following the characterization introduced in Section~\ref{sec:fine-tuning_tech},
We benchmark our approach against representative strategies for on-policy fine-tuning of generative policies, focusing on diffusion models but noting that analogous evaluations apply to flow-matching policies. We include \texttt{DPPO}~\citep{ren2024diffusion}, as a direct finetuning approach, Policy Decorator~\citep{yuan2024policy} as a residual fine-tuning approach (\texttt{RES}), and we consider \texttt{DSRL}~\cite{wagenmaker2025steering} as a steering policy-based approach. For \texttt{DPPO} we select DDIM parameterization that reduces stochasticity while balancing $\eta>0$ and the number of diffusion steps for stable weight updates. We further include a DDPM-based version that samples with the full denoising chain and fine-tunes the last $10$ diffusion steps for completeness (\texttt{DPPO[10]}).
%For \texttt{DPPO}, we consider the DDIM parameterization reduce stochasticity, while keeping a balance between $\eta>0$ and the number of reverse diffusion steps to allow weight fine-tuning. We further include the DDPM parameterization\texttt{DPPO[10]}, fine-tuning the last $10$ steps to examine the effect of decreasing the number of reverse diffusion steps.
% Specifically, we consider (i) direct fine-tuning, (ii) residual corrections, and (iii) steering methods, none of which explicitly seek to preserve multimodality.
% As a direct fine-tuning approach, we include \texttt{DPPO}~\citep{ren2024diffusion}, which optimizes the diffusion policy weights with PPO.
% We consider the DDIM parameterization of the generative process to ensure non-memoryless noise schedules, while keeping a balance between $\eta>0$ and the number of reverse diffusion steps to allow weight fine-tuning.  To examine the effect of decreasing the number of reverse diffusion steps, we also consider the original hyperparameters of the \texttt{DPPO} baseline that uses the full denoising chain for action sampling with DDPM parameterization, and fine-tunes the last $10$ steps, denoted \texttt{DPPO[10]}, which makes the generation process non-memoryless. \\
% As a residual fine-tuning approach (\texttt{RES}), we evaluate Policy Decorator~\citep{yuan2024policy}, where a lightweight residual network is trained on top of the frozen pre-trained diffusion model.
% This allows task adaptation while limiting catastrophic interference with the base model. 
% Finally, we consider~\cite{wagenmaker2025steering} as a steering-based policy \texttt{DSRL}, which adapts the latent noise distribution $w$ to bias the pre-trained policy toward high-reward behaviors. 
% This category operates entirely in the latent space and, like the others, does not include any explicit mechanism for mode discovery or diversity preservation.  \\
Importantly, our approach is orthogonal to these categories and can be combined with any of them.
Therefore, we report results both for the standalone baselines and their variants augmented with our multimodality regularizer, denoted as \texttt{X[\methodname{}]}, where \texttt{X} indicates the corresponding baseline. Full implementation details for all baselines and their regularized variants are provided in Appendix~\ref{appendix:implementation_details}.

\paragraph{Evaluation Metrics.}
% We assume access in simulation to the ground truth modes of the trajectories executed by the policy, and we evaluate fine-tuned policies along two axes: \emph{task success} and \emph{behavioral diversity}.



We assume access to the ground truth modes of the trajectories executed by the policy in simulation, and we evaluate fine-tuned policies along two axes: \emph{task success} and \emph{behavioral diversity}. We report the overall success rate $\mathrm{SR}$, and two mode-aggregated measures of the success rate to integrate behavioral diversity: the success rate weighted for each mode
$
{\mathrm{SR}_{\text{M}}=\tfrac{1}{K}\sum_{i=1}^K \mathrm{SR}_i,}
$
which guards against degenerate solutions (e.g., \(100\%\) success on a single mode but failure on others), and
mode coverage ${\mathrm{mc}@\tau=\tfrac{1}{K}\sum_{i=1}^K \mathbf{1}\{\mathrm{SR}_i \ge \tau \}}$,
the fraction of modes solved above threshold \(\tau=0.8\). 
We further compute the entropy of the empirical distribution over modes among all rollouts: $H(\pi)=-\sum_{i=1}^K p_i\log p_i$, where $p_i$ is the fraction of episodes in mode $i$. %A higher entropy reflects more balanced usage of the available modes, whereas a reduction after fine-tuning is indicative of mode collapse. 
All metrics are computed from $N=1024$ evaluation episodes with fixed seeds for fair comparison, and we report both the mean and standard deviation over three independent runs with different random seeds. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.88\textwidth]{figures/01_intro/multimodality_comparison_v4.pdf}
  \caption{ 
  \textbf{Qualitative trajectories for two rotated reward landscapes.} Each panel overlays trajectories from the origin in a four-goal reward landscape (bright peaks). \emph{Left:} the pre-trained policy covers all modes. \emph{Middle:} \texttt{DPPO} fine-tuning collapses to a subset of modes after reward rotation. \emph{Right:} \texttt{\methodname{}} fine-tunes the policy while maintaining full mode coverage.}
  \label{fig:multimodality_comparison}
\end{figure*}

% \renewcommand{\arraystretch}{1.15} % Adjust row spacing
% \setlength{\arrayrulewidth}{0.04pt} % Adjust line thickness




\subsection{2D Gaussian Mixture}
\label{sec:2D_Gaussians}
To study the proposed questions in a controlled setting, we designed a 2D navigation environment where the reward landscape is a mixture of $4$ Gaussians centered at fixed goal locations.
% The agent starts at the origin and moves by 2D displacements, receiving rewards defined by the Gaussian peaks.  
%We study both a \emph{balanced} variant, where all goals have equal weight, and an \emph{unbalanced} variant, where mode weights are randomized and normalized via a softmax, producing uneven but non-degenerate reward magnitudes.  
Further details and illustrations about the environment are provided in Appendix~\ref{appendix:gaussian_env}.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.99\columnwidth]{figures/2D_Mixture/action_distribution.pdf}
%   \caption{(Top) Trajectories generated from pre-trained policies. % on demonstration datasets containing different degrees of multimodality. 
%   (Bottom) Monte Carlo estimate of the action distribution ($\Delta x, \Delta y$) at $t=0$.}
%   \label{fig:q1-env-trajectories}
% \end{figure}

% % \vspace{-1.5em}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.99\columnwidth]{figures/2D_Mixture/2D_Modes.pdf}
%   \caption{Rollouts generated by steering the policy with latent codes \(z\in\{0,1,2,3\}\).}
%   \label{fig:q1-modes-by-z}
% \end{figure}


% \vspace{-0.4em}


% To study the proposed questions within a controlled setting, we designed a simple two-dimensional navigation task where the reward landscape is defined by a mixture of $4$ Gaussians. The agent’s state is its 2D position $(x,y)$, and the episode always start at the center of the environment, corresponding to $(0,0)$. Actions are modeled as small displacements $\Delta x, \Delta y$. The reward at position $\text{pos}=(x,y)$ is computed as a sum of unnormalized Gaussians centered at a fixed set of goal locations $\{(c_x,c_y)\}$:
% \begin{equation}
%     r(x,y) = \sum_{(c_x,c_y) \in \mathcal{C}} \exp\!\left(-\tfrac{(x-c_x)^2+(y-c_y)^2}{2\sigma^2}\right),
% \end{equation}
% where $\sigma$ controls the spread of each mode.  
% %The environment is multimodal by construction: high reward can be obtained by reaching any of the Gaussian peaks. 
% An episode is considered successful if the agent reaches within a fixed distance threshold of one of the goal centers. \al{Include an explanation of the different variations of the reward landscapes and create a figure showing them.}


% \begin{figure*}[t]
%   \centering
%   % ---------- LEFT: 2 rows × 3 cols ----------
%   \begin{minipage}[t]{0.64\textwidth}
%     \centering
%     % Row 1
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/expert_trajectories_1.png}
%       \caption{Dataset (1 mode)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/expert_trajectories_2.png}
%       \caption{Dataset (2 modes)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/expert_trajectories_4.png}
%       \caption{Dataset (4 modes)}
%     \end{subfigure}

%     \vspace{0.6em}

%     % Row 2
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/final_frame_p1.png}
%       \caption{Policy (1 mode)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/final_frame_p2.png}
%       \caption{Policy (2 modes)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%     \includegraphics[width=\linewidth]{figures/2D_Mixture/final_frame_p4.png}
%       \caption{Policy (4 modes)}
%     \end{subfigure}

%     \captionof{figure}{Policies with different numbers of modes in the Gaussian-mixture landscape. \al{Substitute the dataset with the monte-carlo estimate of the action distribution.}}
%     \label{fig:q1-env-trajectories}
%   \end{minipage}
%   \hfill
%   % ---------- RIGHT: 2 rows × 2 cols (modes by z) ----------
%   \begin{minipage}[t]{0.33\textwidth}
%     \centering

%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z0.png}
%       \caption{$z=0$}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z1.png}
%       \caption{$z=1$}
%     \end{subfigure}

%     \vspace{0.6em}

%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z2.png}
%       \caption{$z=2$}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z3.png}
%       \caption{$z=3$}
%     \end{subfigure}


% \begin{figure*}[t]
%   \centering
%   % ---------- LEFT: 2 rows × 3 cols ----------
%   \begin{minipage}[t]{0.64\textwidth}
%     \centering
%     % Row 1
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=0.9\linewidth]{figures/2D_Mixture/final_frame_p1.png}
%       \caption{Policy (1 mode)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=0.9\linewidth]{figures/2D_Mixture/final_frame_p2.png}
%       \caption{Policy (2 modes)}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%     \includegraphics[width=0.9\linewidth]{figures/2D_Mixture/final_frame_p4.png}
%       \caption{Policy (4 modes)}
%     \end{subfigure}


%     \vspace{0.6em}

%     % Row 2
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/action_dist_t0_s0_model_1.pdf}
%       \caption{1 mode}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/action_dist_t0_s0_model_2.pdf}
%       \caption{2 modes.}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.32\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/action_dist_t0_s0_model_3.pdf}
%       \caption{4 modes.}
%     \end{subfigure}
%     \captionof{figure}{(Top) Trajectories generated from policies pre-trained. % on demonstration datasets containing different degrees of multimodality. 
%     (Bottom) Monte Carlo estimate of the action distribution ($\Delta x, \Delta y$) at $t=0$.}
%     \label{fig:q1-env-trajectories}
%   \end{minipage}
%   \hfill
%   % ---------- RIGHT: 2 rows × 2 cols (modes by z) ----------
%   \begin{minipage}[t]{0.33\textwidth}
%     \centering

%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z0.png}
%       \caption{$z=0$}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z1.png}
%       \caption{$z=1$}
%     \end{subfigure}

%     \vspace{0.6em}

%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z2.png}
%       \caption{$z=2$}
%     \end{subfigure}\hfill
%     \begin{subfigure}[t]{0.48\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/2D_Mixture/mode_z3.png}
%       \caption{$z=3$}
%     \end{subfigure}
%     \captionof{figure}{Rollouts generated by steering the policy with latent codes \(z\in\{0,1,2,3\}\).}
%     \label{fig:q1-modes-by-z}
%   \end{minipage}
% \end{figure*}



\paragraph{Mutual Information as a Proxy for Multimodality.}  
We first evaluate if mutual information provides a reliable proxy for multimodality. To this end, we construct $3$ expert datasets in the Gaussian-mixture environment containing one, two, or four goal modes, and train separate policies on each dataset. Visualizations of the demonstrations,  rollouts of policies trained on each dataset alongside Monte Carlo estimates of their action distributions are shown in the Appendix in Figure~\ref{fig:q1-env-trajectories}. %). Figure~\ref{fig:q1-env-trajectories} shows rollouts of policies trained on each dataset (top row) alongside Monte Carlo estimates of their action distributions at $t=0$ (bottom row). 
% As expected, policies trained on multimodal datasets exhibit multi-peaked action distributions.
We hypothesize that a valid multimodality metric $\mathcal{M}$ should increase with the number of modes. To test this, we estimate $\mathcal{M}$ with Equation~\ref{eq:variational_MI} by jointly training a steering policy and an inference model $q_\phi$ over a discrete latent space $\mathcal{Z}=\{0,1,2,3\}$. %The policy generates trajectories conditioned on $z$, $q_\phi$ predicts $z$ from state–action pairs, and $\mathcal{M}$ is then estimated from $q_\phi$ on $1024$ trajectories with random latent codes.
% Our hypothesis is that a valid multimodality metric $\mathcal{M}$ should systematically increase in value with the number of available modes. We estimate $\mathcal{M}$ using the variational formulation in Equation~\ref{eq:variational_MI}, by jointly training a steering policy and an inference model $q_\phi$ over a predefined discrete latent space $Z \equiv \{0,1,2,3\}$. The steering policy generates trajectories conditioned on $z$, while the inference model is trained to recover $z$ from state–action pairs. After training, $\mathcal{M}$ is estimated from $q_\phi$ by evaluating $1024$ trajectories produced by the steering policy with randomly sampled latent codes.

% To evaluate whether mutual information provides a reliable measure of multimodality, we construct three expert datasets that exhibit one, two, or four goal modes in the Gaussian-mixture environment and train separate policies on each dataset. If $\mathcal{M}$ is a valid multimodality metric, we expect its value to increase systematically with the number of available modes. We compute $\mathcal{M}$ using the variational estimate in Equation~\ref{eq:variational_MI}, which is also used to train a discrete latent space $Z \equiv \{0,1,2,3\}$ and the inference model $q_\phi$ over latent codes. In this setting, the steering policy is trained solely with the mode discovery objective (without task reward), allowing us to directly test whether maximizing $\mathcal{M}$ corresponds to capturing and differentiating the demonstrated modes. Figure~\ref{fig:q1-env-trajectories} illustrates multiple rollouts of pre-trained policies obtained from different expert datasets (top row), together with the corresponding Monte Carlo estimates of their action distributions at $t=0$ (bottom row). As expected, policies trained on datasets containing more than one modality exhibit multi-peaked action distributions. %Figure~\ref{fig:q1-env-trajectories} shows the expert datasets (top row) and the corresponding learned policies (bottom row). As expected, the policies reproduce the number of modes present in the demonstrations by consistently reaching the corresponding Gaussian peaks.


% \renewcommand{\arraystretch}{1.} % Adjust row spacing
% \setlength{\arrayrulewidth}{0.05pt} % Adjust line thickness

% Wrapped table: placed here so it is anchored next to text.
\begin{wraptable}[8]{r}{0.48\columnwidth}
  \vspace{-0.7\baselineskip}
  \centering
  \caption{Mutual information and inference-model loss.}
  \label{tab:mi_disc}
  \begin{tabular}{lcc}
  \toprule
  \textbf{Policy} & \textbf{$\mathcal{M}$} & $\mathbf{L_q(\phi)}$ \\
  \midrule
  \tablerowcolors
  1 mode  & $0.00 \scriptscriptstyle\pm 0.00$  & $1.38 \scriptscriptstyle\pm 0.00$ \\
  2 modes & $0.58 \scriptscriptstyle\pm 0.02$  & $0.82 \scriptscriptstyle\pm 0.02$ \\
  4 modes & $1.06 \scriptscriptstyle\pm 0.00$  & $0.33 \scriptscriptstyle\pm 0.02$ \\
  \bottomrule
  \end{tabular}
  \vspace{-0.7\baselineskip}
\end{wraptable}

Table~\ref{tab:mi_disc} reports the estimated mutual information and inference-model loss from $q_\phi$ on $1024$ trajectories with randomly sampled $z\in\mathcal{Z}$. As expected, mutual information increases with the number of modes, while the inference loss decreases, indicating that $q_\phi$ reliably discriminates latent codes when multimodality exists. These results support mutual information as a proxy for measuring multimodality, when such multimodality exists. Figure~\ref{fig:q1-modes-by-z} further shows that conditioning the steering policy on individual $z$ produces distinct, coherent trajectories, confirming that the latent space organizes noise into meaningful and controllable behavioral modes.

% Table~\ref{tab:mi_disc} reports the estimated mutual information alongside the inference-model loss computed from $q_\phi$ on $1024$ trajectories generated by the policy with randomly sampled $z\in \mathcal{Z}$. As hypothesized, the mutual-information estimate increases monotonically with the number of demonstrated modes. The inference-model loss follows the opposite trend, decreasing as the number of modes grows, reflecting that the inference model can reliably identify distinct latent codes when structure is present, but struggles in the unimodal case where no meaningful distinctions exist. Together, these results validate our use of conditional mutual information as a proxy for multimodality, providing a valuable training signal for retaining multimodal behaviors. Figure~\ref{fig:q1-modes-by-z} further illustrates that conditioning on individual latent codes $z \in Z$ yields coherent and distinct trajectories, demonstrating that the learned latent space organizes the noise distribution into behaviorally meaningful modes.

% It is important to note, however, that this estimate of the mutual information is not an absolute metric: its value depends on the learned latent representation and the capacity of the discriminator, and is therefore not directly comparable across different models or training runs. Instead, it should be regarded as a relative proxy that reflects the preservation of multimodality within a given training setup. In the following experiments, we empirically demonstrate its effectiveness as a fine-tuning signal.



\renewcommand{\arraystretch}{1.10} % Adjust row spacing
\setlength{\arrayrulewidth}{0.04pt} % Adjust line thickness


\begin{table}[!t]
  \centering
  \caption{Evaluation on the Gaussian-mixture environment under fine-tuning reward landscape \textbf{G1}.}
  \label{tab:toy_results_g1}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcccc}
  \toprule
   \textbf{Method} & $\mathrm{SR}$ $(\uparrow)$ & $\mathrm{SR}_{\mathrm{M}}$ $(\uparrow)$ & $\mathrm{mc}@80$ $(\uparrow)$ & $\mathcal{{H}}$ $(\uparrow)$ \\
  \midrule
  \rowcolor{lightgray!50}\texttt{RES} & $0.98 \scriptscriptstyle\pm 0.02$ & $0.98 \scriptscriptstyle\pm 0.02$ & $4.00 / 4$  & $1.00 \scriptscriptstyle\pm 0.00$  \\
  \texttt{DSRL} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.25 \scriptscriptstyle\pm 0.00$ & $1.00 / 4$& $0.00 \scriptscriptstyle\pm 0.00$  \\
  \rowcolor{lightgray!50}\texttt{DPPO} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.58 \scriptscriptstyle\pm 0.12$ & $2.33 / 4$& $0.40 \scriptscriptstyle\pm 0.03$  \\
  \texttt{DPPO[10]} & $0.66 \scriptscriptstyle\pm 0.32$ & $0.16 \scriptscriptstyle\pm 0.08$ & $0.33 / 4$& $0.00 \scriptscriptstyle\pm 0.00$  \\
  \midrule
  \rowcolor{lightgray!50}\texttt{RES[\methodname{}]} & \textbf{$1.00 \scriptscriptstyle\pm 0.00$} & \textbf{$1.00 \scriptscriptstyle\pm 0.00$} & \textbf{$4.00 / 4$} &\textbf{ $0.99 \scriptscriptstyle\pm 0.00$} \\
  \texttt{DSRL[\methodname{}]} & $0.33 \scriptscriptstyle\pm 0.47$ & $0.33 \scriptscriptstyle\pm 0.47$ & $1.33 / 4$ & $0.46 \scriptscriptstyle\pm 0.41$  \\
  \rowcolor{lightgray!50}\texttt{DPPO[\methodname{}]} & $1.00 \scriptscriptstyle\pm 0.00$ & $1.00 \scriptscriptstyle\pm 0.00$ & $4.00 / 4$ & $0.99 \scriptscriptstyle\pm 0.00$  \\
  \bottomrule
  \end{tabular}%
  }
  \end{table}
  

\paragraph{Fine-Tuning Evaluation.}
We evaluate the performance of existing fine-tuning methods against \methodname{} in preserving multimodality when the reward used for adaptation differs from the one implicitly encoded in the demonstrations.  
To simulate this mismatch, we define two shifted reward landscapes obtained by rotating the Gaussian peaks used for demonstrations by ${\tfrac{\pi}{8}}$ and ${\tfrac{\pi}{4}}$, denoted as \textbf{G1} and \textbf{G2}.  

Tables~\ref{tab:toy_results_g1} and~\ref{tab:toy_results_g2} report results for \ac{rlft} with \textbf{G1} and \textbf{G2}, respectively. Among baselines, the residual policy (\texttt{RES}) performs best, solving \textbf{G1} and retaining two modes in \textbf{G2}, as tuning the magnitude of the residual corrections to small values helps preserve multimodality. Gradient-based methods (\texttt{DPPO}, \texttt{DPPO[10]}) improve task success, with \texttt{DPPO} aided by extra denoising steps, but both collapse to fewer modes when rewards diverge from demonstrations. Steering alone (\texttt{DSRL}) is least effective, with limited success in \textbf{G1} and full performance drop in \textbf{G2}. %Multimodality retention further degrades in the unbalanced setting, where reward bias toward specific goals causes even strong baselines to collapse to dominant peaks. 
These results suggest that standard fine-tuning techniques fail to fully preserve the original multimodality when the reward landscape deviates from the distribution underlying the demonstrations.

In contrast, the \texttt{[\methodname{}]} variants preserve diversity more consistently: \texttt{RES[\methodname{}]} recovers full mode coverage across both goals, and \texttt{DPPO[\methodname{}]} shows similar gains. For the \texttt{DSRL} method,  \texttt{[\methodname{}]} mitigates but does not prevent collapse, indicating that additional fine-tuning of the original policy is required in this case. Overall, \methodname{} regularized fine-tuning preventing the RL objective to bias the policy toward fewer behaviors.  Qualitative visualizations of the trajectories learned by the \texttt{DPPO} and \texttt{RES[\methodname{}]} policies are shown in Figure~\ref{fig:multimodality_comparison}. Appendix~\ref{appendix:dim_z} reports ablations on the role of the dimensionality of $\mathcal{Z}$.





\begin{table}[!t]
  \centering
  \caption{Evaluation on the Gaussian-mixture environment under fine-tuning reward landscape \textbf{G2}.}
\label{tab:toy_results_g2}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcccc}
  \toprule
  \textbf{Method} & $\mathrm{SR}$ $(\uparrow)$ & $\mathrm{SR}_{\mathrm{M}}$ $(\uparrow)$ & $\mathrm{mc}@80$ $(\uparrow)$ & $\mathcal{{H}}$ $(\uparrow)$ \\
  \midrule
  \rowcolor{lightgray!50}\texttt{RES} & $0.92 \scriptscriptstyle\pm 0.12$ & $0.50 \scriptscriptstyle\pm 0.00$ & $2.00 / 4$& $0.59 \scriptscriptstyle\pm 0.13$ \\
  \texttt{DSRL} & $0.33 \scriptscriptstyle\pm 0.47$ & $0.08 \scriptscriptstyle\pm 0.12$ & $0.33 / 4$& $0.00 \scriptscriptstyle\pm 0.00$ \\
  \rowcolor{lightgray!50}\texttt{DPPO} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.42 \scriptscriptstyle\pm 0.12$ & $1.67 / 4$& $0.02 \scriptscriptstyle\pm 0.02$ \\
  \texttt{DPPO[10]} & $0.32 \scriptscriptstyle\pm 0.22$ & $0.11 \scriptscriptstyle\pm 0.05$ & $0.00 / 4$& $0.60 \scriptscriptstyle\pm 0.22$ \\
  \midrule
  \rowcolor{lightgray!50}\texttt{RES[\methodname{}]} & \textbf{$1.00 \scriptscriptstyle\pm 0.00$} & \textbf{$1.00 \scriptscriptstyle\pm 0.00$} & \textbf{$4.00 / 4$ }& \textbf{$0.94 \scriptscriptstyle\pm 0.00$} \\
  \texttt{DSRL[\methodname{}]} & $0.33 \scriptscriptstyle\pm 0.04$ & $0.08 \scriptscriptstyle\pm 0.12$ & $0.33 / 4$ & $0.84 \scriptscriptstyle\pm 0.14$ \\
  \rowcolor{lightgray!50}\texttt{DPPO[\methodname{}]} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.75 \scriptscriptstyle\pm 0.00$ & $3.00 / 4$ & $0.74 \scriptscriptstyle\pm 0.00$ \\
  \bottomrule
  \end{tabular}%
  }
  \end{table}





% \renewcommand{\arraystretch}{1.15} % Adjust row spacing
% \setlength{\arrayrulewidth}{0.04pt} % Adjust line thickness

\begin{table*}[t]
\centering
\begin{minipage}{0.465\linewidth}
\centering
\caption{Baselines fine-tuning. }
\label{tab:baselines}
\resizebox{\linewidth}{!}{%
% \setlength{\tabcolsep}{3.5pt} % <-- reduce horizontal space between columns
\centering
\begin{tabular}{lcccc}
\toprule
 \textbf{Method} & $\mathrm{SR}$$(\uparrow)$ & $\mathrm{SR}_{\mathrm{M}}$ $(\uparrow)$& $\mathrm{mc}@0.80$ $(\uparrow)$& $\mathcal{H}$ $(\uparrow)$\\
\midrule
\rowcolor{lightgray!50} \multicolumn{5}{c}{\emph{Reach}} \\
\midrule
\texttt{PRE} & $0.32 \scriptscriptstyle\pm 0.01$ & $0.31 \scriptscriptstyle\pm 0.00$ & $0.00 / 2$& $0.99 \scriptscriptstyle\pm 0.00$  \\
\midrule
\rowcolor{lightgray!50}\texttt{RES} & $1.00 \scriptscriptstyle\pm 0.00$ & $1.00 \scriptscriptstyle\pm 0.00$ & $2.00 / 2$& $0.98 \scriptscriptstyle\pm 0.01$  \\
\texttt{DSRL} & $0.98 \scriptscriptstyle\pm 0.00$ & $0.98 \scriptscriptstyle\pm 0.00$ & $2.00 / 2$& $0.97 \scriptscriptstyle\pm 0.00$ \\
\rowcolor{lightgray!50}\texttt{DPPO} & $0.93 \scriptscriptstyle\pm 0.01$ & $0.94 \scriptscriptstyle\pm 0.02$ & $2.00 / 2$& $0.66 \scriptscriptstyle\pm 0.33$  \\
\texttt{DPPO[10]} & $0.99 \scriptscriptstyle\pm 0.00$ & $0.99 \scriptscriptstyle\pm 0.00$ & $2.00 / 2$& $0.97 \scriptscriptstyle\pm 0.03$  \\
\midrule
\rowcolor{lightgray!50} \multicolumn{5}{c}{\emph{Lift}} \\
\midrule
\texttt{PRE} & $0.14 \scriptscriptstyle\pm 0.01$ & $0.15 \scriptscriptstyle\pm 0.01$ & $0.00 / 2$& $0.97 \scriptscriptstyle\pm 0.01$\\
\midrule
\rowcolor{lightgray!50}\texttt{RES} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.50 \scriptscriptstyle\pm 0.00$ & $1.00 / 2$& $0.00 \scriptscriptstyle\pm 0.00$  \\
\texttt{DSRL} & $0.78 \scriptscriptstyle\pm 0.03$ & $0.78 \scriptscriptstyle\pm 0.03$ & $0.67 / 2$& $0.98 \scriptscriptstyle\pm 0.01$  \\
\rowcolor{lightgray!50}\texttt{DPPO} & $0.99 \scriptscriptstyle\pm 0.01$ & $0.57 \scriptscriptstyle\pm 0.10$ & $1.00 / 2$& $0.05 \scriptscriptstyle\pm 0.03$ \\
\texttt{DPPO[10]} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.56 \scriptscriptstyle\pm 0.08$ & $1.00 / 2$& $0.02 \scriptscriptstyle\pm 0.01$  \\
\midrule
\rowcolor{lightgray!50} \multicolumn{5}{c}{\emph{Avoid}} \\
\midrule
\texttt{PRE} & $0.94 \scriptscriptstyle\pm 0.04$ & $0.86 \scriptscriptstyle\pm 0.04$ & $20.00 / 24$& $0.63 \scriptscriptstyle\pm 0.00$ \\
\midrule
\rowcolor{lightgray!50}\texttt{RES} & $0.98 \scriptscriptstyle\pm 0.03$ & $0.04 \scriptscriptstyle\pm 0.00$ & $1.00 / 24$& $0.00 \scriptscriptstyle\pm 0.00$  \\
\texttt{DSRL} & $1.00 \scriptscriptstyle\pm 0.01$ & $0.09 \scriptscriptstyle\pm 0.02$ & $2.00 / 24$& $0.01 \scriptscriptstyle\pm 0.00$ \\
\rowcolor{lightgray!50}\texttt{DPPO} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.26 \scriptscriptstyle\pm 0.11$ & $6.33 / 24$& $0.13 \scriptscriptstyle\pm 0.15$ \\
\texttt{DPPO[10]} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.04 \scriptscriptstyle\pm 0.00$ & $1.00 / 24$& $0.00 \scriptscriptstyle\pm 0.00$  \\
\bottomrule
\end{tabular}%
}
\end{minipage}\hfill
\begin{minipage}{0.5\linewidth}
\centering
\caption{Fine-tuning with regularization (\methodname{}).}
\label{tab:our_method}
\centering
\resizebox{0.95\linewidth}{!}{%
% \setlength{\tabcolsep}{3.5pt} % <-- reduce horizontal space between columns
\begin{tabular}{lcccc}
\toprule
 \textbf{Method} & $\mathrm{SR}$ $(\uparrow)$& $\mathrm{SR}_{\mathrm{M}}$ $(\uparrow)$ & $\mathrm{mc}@0.80$ $(\uparrow)$& $\mathcal{H}$ $(\uparrow)$ \\
\midrule
\rowcolor{lightgray!50} \multicolumn{5}{c}{\emph{Reach}} \\
\midrule
\texttt{PRE} & $0.32 \scriptscriptstyle\pm 0.01$ & $0.31 \scriptscriptstyle\pm 0.00$ & $0.00 / 2$& $0.99 \scriptscriptstyle\pm 0.00$  \\
\midrule
\rowcolor{lightgray!50}\texttt{RES[\methodname{}]} & $0.99 \scriptscriptstyle\pm 0.00$ & $0.99 \scriptscriptstyle\pm 0.00$ & $2.00 / 2$& $1.00 \scriptscriptstyle\pm 0.00$ \\
\texttt{DSRL[\methodname{}]} & $1.00 \scriptscriptstyle\pm 0.00$ & $1.00 \scriptscriptstyle\pm 0.00$ & $2.00 / 2$& $0.97 \scriptscriptstyle\pm 0.01$ \\
\rowcolor{lightgray!50}\texttt{DPPO[\methodname{}]} & $0.98 \scriptscriptstyle\pm 0.01$ & $0.98 \scriptscriptstyle\pm 0.01$ & $2.00 / 2$& $0.67 \scriptscriptstyle\pm 0.43$ \\
\texttt{DPPO[10]} & - & -  &  -  & -   \\
\midrule
\rowcolor{lightgray!50} \multicolumn{5}{c}{\emph{Lift}} \\
\midrule
\texttt{PRE} & $0.14 \scriptscriptstyle\pm 0.01$ & $0.15 \scriptscriptstyle\pm 0.01$ & $0.00 / 2$& $0.97 \scriptscriptstyle\pm 0.01$ \\
\midrule
\rowcolor{lightgray!50}\texttt{RES[\methodname{}]} & $0.99 \scriptscriptstyle\pm 0.00$ & $0.99 \scriptscriptstyle\pm 0.00$ & $2.00 / 2$& $1.00 \scriptscriptstyle\pm 0.00$ \\
\texttt{DSRL[\methodname{}]} & $0.88 \scriptscriptstyle\pm 0.07$ & $0.88 \scriptscriptstyle\pm 0.07$ & $1.67 / 2$& $0.99 \scriptscriptstyle\pm 0.01$ \\
\rowcolor{lightgray!50}\texttt{DPPO[\methodname{}]} & $0.99 \scriptscriptstyle\pm 0.00$ & $0.55 \scriptscriptstyle\pm 0.07$ & $1.00 / 2$& $0.06 \scriptscriptstyle\pm 0.04$ \\
\texttt{DPPO[10]} & - & -  &  -  & -   \\
\midrule
\rowcolor{lightgray!50} \multicolumn{5}{c}{\emph{Avoid}} \\
\midrule
\texttt{PRE} & $0.94 \scriptscriptstyle\pm 0.04$ & $0.86 \scriptscriptstyle\pm 0.04$ & $20.00 / 24$& $0.63 \scriptscriptstyle\pm 0.00$ \\
\midrule
\rowcolor{lightgray!50}\texttt{RES[\methodname{}]} & $0.99 \scriptscriptstyle\pm 0.01$ & $0.30 \scriptscriptstyle\pm 0.02$ & $7.33 / 24$& $0.53 \scriptscriptstyle\pm 0.01$  \\
\texttt{DSRL[\methodname{}]} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.42 \scriptscriptstyle\pm 0.00$ & $10.00 / 24$& $0.58 \scriptscriptstyle\pm 0.00$  \\
\rowcolor{lightgray!50}\texttt{DPPO[\methodname{}]} & $0.94 \scriptscriptstyle\pm 0.07$ & $0.43 \scriptscriptstyle\pm 0.05$ & $9.67 / 24$& $0.57 \scriptscriptstyle\pm 0.01$  \\
\texttt{DPPO[10]} & - & -  &  -  & -   \\
\bottomrule
\end{tabular}%
}
\end{minipage}
\end{table*}



\begin{table*}[t]
\centering
\begin{minipage}{0.465\linewidth}
\centering
\caption{ANYmal locomotion environment. }
\label{tab:anymal_results}
\resizebox{\linewidth}{!}{%
% \setlength{\tabcolsep}{3.5pt} % <-- reduce horizontal space between columns
\centering
\begin{tabular}{lcccc}
\toprule
 \textbf{Method} & $\mathrm{SR}$ $(\uparrow)$ & $\mathrm{SR}_{\mathrm{M}}$ $(\uparrow)$ & $\mathrm{mc}@0.80$ $(\uparrow)$ & $\mathcal{H}$ $(\uparrow)$ \\
\midrule
\rowcolor{lightgray!50}\texttt{PRE} & $0.41 \scriptscriptstyle\pm 0.01$ & $0.39 \scriptscriptstyle\pm 0.01$ & $0.00 / 4$ & $0.96 \scriptscriptstyle\pm 0.00$ \\
\midrule
\rowcolor{lightgray!50}\texttt{RES} & $0.98 \scriptscriptstyle\pm 0.01$ & $0.90 \scriptscriptstyle\pm 0.11$ & $3.67 / 4$ & $0.90 \scriptscriptstyle\pm 0.09$ \\
\texttt{DSRL} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.25 \scriptscriptstyle\pm 0.00$ & $1.00 / 4$ & $0.00 \scriptscriptstyle\pm 0.00$ \\
\rowcolor{lightgray!50}\texttt{DPPO} & $0.99 \scriptscriptstyle\pm 0.01$ & $0.32 \scriptscriptstyle\pm 0.10$ & $1.33 / 4$ & $0.09 \scriptscriptstyle\pm 0.13$ \\
\texttt{DPPO[10]} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.42 \scriptscriptstyle\pm 0.12$ & $1.67 / 4$ & $0.15 \scriptscriptstyle\pm 0.20$\\
\midrule
\rowcolor{lightgray!50}\texttt{DSRL[\methodname{}]}  & $0.97 \scriptscriptstyle\pm 0.01$ & $0.89 \scriptscriptstyle\pm 0.12$ & $3.67 / 4$ & $0.91 \scriptscriptstyle\pm 0.12$ \\
\bottomrule
\end{tabular}
}
\end{minipage}\hfill
\begin{minipage}{0.465\linewidth}
\centering
\caption{Franka Kitchen environment.}
\label{tab:franka_results}
\centering
\resizebox{\linewidth}{!}{%

\centering
\begin{tabular}{lcccc}
\toprule
  \textbf{Method} & $\mathrm{SR}$ $(\uparrow)$ & $\mathrm{SR}_{\mathrm{M}}$ $(\uparrow)$ & $\mathrm{mc}@0.80$ $(\uparrow)$ & $\mathcal{H}$ $(\uparrow)$ \\
\midrule
\rowcolor{lightgray!50}\texttt{PRE} & $0.00 \scriptscriptstyle\pm 0.00$ & $0.00 \scriptscriptstyle\pm 0.00$ & $0.00 / 24$ & $0.33 \scriptscriptstyle\pm 0.07$ \\
\midrule
% \rowcolor{lightgray!50}\texttt{RES} & 1.00 & 0.04 & $1.00 / 24$ & 0.00 \\
% \texttt{DSRL} & 1.00 & 0.04 & $1.00 / 24$ & 0.00 \\
% \rowcolor{lightgray!50}\texttt{DPPO[10]} & 1.00 & 0.04 & $1.00 / 24$ & 0.00 \\
% \texttt{DPPO} & 0.71 & 0.04 & $1.00 / 24$ & 0.19 \\
% \midrule
\rowcolor{lightgray!50}\texttt{RES} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.04 \scriptscriptstyle\pm 0.00$ & $1.0 / 24$ & $0.00 \scriptscriptstyle\pm 0.00$ \\
\texttt{DSRL} & $0.99 \scriptscriptstyle\pm 0.01$ & $0.04 \scriptscriptstyle\pm 0.00$ & $1.0 / 24$ & $0.01 \scriptscriptstyle\pm 0.02$ \\
\rowcolor{lightgray!50}\texttt{DPPO} & $0.56 \scriptscriptstyle\pm 0.41$ & $0.03 \scriptscriptstyle\pm 0.02$ & $0.67 / 24$ & $0.08 \scriptscriptstyle\pm 0.08$ \\
\texttt{DPPO[10]} & $1.00 \scriptscriptstyle\pm 0.00$ & $0.04 \scriptscriptstyle\pm 0.00$ & $1.0 / 24$ & $0.00 \scriptscriptstyle\pm 0.00$ \\
\rowcolor{lightgray!50}\texttt{DSRL[\methodname{}]} & 0.98 & 0.08 & $2.00 / 24$ & 0.23 \\
\bottomrule
\end{tabular}%
}
\end{minipage}
\end{table*}

  



% \al{To integrate: 
% \begin{itemize}
%     \item comparison with KL regularization, 
%     \item comparison without pre-training the steering policy.
%     \item (If time) Flow-based policies.
% \end{itemize}}





\begin{figure*}[t!]
  \centering
  % Constrain figure height to avoid oversized floats.
  \newcommand{\skillTopH}{0.18\textheight}
  \newcommand{\skillBotH}{0.22\textheight}
  % --- top row ---
  \begin{subfigure}[t]{0.485\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=\skillTopH,keepaspectratio]{figures/tasks/reach_skills.pdf}
    \caption{\emph{Reach}: (Left, green box) \texttt{DPPO}. (Right) \texttt{DPPO[\methodname{}]} modes.}
    \label{fig:top-left}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.485\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=\skillTopH,keepaspectratio]{figures/tasks/Lift_skills.pdf}
    \caption{\emph{Lift}: (Left, green box) \texttt{DPPO}. (Right) \texttt{DPPO[\methodname{}]} modes.}
    \label{fig:top-right}
  \end{subfigure}

  % --- bottom row (wide) ---
  \vspace{0.3em}
  \begin{subfigure}[t]{0.80\linewidth}
    \centering
    \includegraphics[width=\linewidth,height=\skillBotH,keepaspectratio]{figures/tasks/Avoid_Skills.pdf}
    \caption{\emph{Avoid}: (Left, purple box) \texttt{DPPO}. (Right) \texttt{DPPO[\methodname{}]} modes.}
    \label{fig:bottom-wide}
  \end{subfigure}

  \caption{Visualization of trajectories (blue) from standard fine-tuning and \methodname{} fine-tuning across different tasks. Highlighted boxes (green, purple) show trajectories sampled form  \texttt{DPPO}, which exhibits multimodal behavior only in the \emph{Reach} task. The remaining visualizations represent the modes learned by \texttt{DPPO[\methodname{}]}, where trajectories are sampled by varying $z \in \mathcal{Z}$}
  \label{fig:learned_skills_main}
\end{figure*}





\subsection{Robotic Manipulation}

% We evaluate fine-tuning strategies for pre-trained diffusion policies on simulated robotic manipulation tasks, with the aim of assessing their ability to preserve multimodal behaviors. In particular, we test whether our proposed method regularizes learning to retain diversity while improving task performance.

% \paragraph{Tasks} 
% Next, we evaluate our method on three simulated robotic tasks: \emph{Reach}, \emph{Lift}, and \emph{Avoid}, based on ManiSkill~\citep{tao2024maniskill3} and visualized in Figure~\ref{fig:tasks}. Multimodality arises either from goal diversity or trajectory diversity in achieving the same goal. In \emph{Reach}, the agent can approach the target sphere from either side, though the choice is committed early. In \emph{Lift}, the peg can be grasped from different sides or regions, leading to richer modality structure. \emph{Avoid} is the most complex, with many avoidance strategies emerging later in the episode, each with different trajectory lengths. Dense or intermediate rewards are provided to support fine-tuning, and a heuristic is used to assign trajectories to modes for evaluation. For each task, we collect $1000$ demos with a motion planner and pre-train a diffusion model for $1000$ epochs.  Further implementation details are given in Appendix~\ref{appendix:manip_tasks}.

Next, we evaluate our method on three simulated robotic tasks: \emph{Reach}, \emph{Lift}, and \emph{Avoid}, implemented on ManiSkill~\citep{tao2024maniskill3} and visualized in Figure~\ref{fig:tasks}. Multimodality arises either from goal diversity or trajectory diversity in achieving the same goal. For each task, we collect $1000$ demos with a motion planner and pre-train a diffusion model for $1000$ epochs. Subsequently, dense or intermediate rewards are provided to support fine-tuning, and a heuristic is used to assign trajectories to modes for evaluation. Further implementation details are given in Appendix~\ref{appendix:manip_tasks}.





% \paragraph{Standard Fine-tuning.} We first evaluate baseline fine-tuning methods that do not include explicit mechanisms to preserve multimodality. Results are summarized in Table~\ref{tab:baselines}. In the \emph{Reach} task, all methods successfully fine-tune the pre-trained policy without collapsing modes, obtaining high scores across all metrics, suggesting that the inherent exploration of the diffusion policy is sufficient to fine-tune both modes. For the \emph{Lift} task, although fine-tuning improves overall success rates, none of the methods consistently solves both modalities. Only the steering-based baseline (\texttt{DSRL}) maintains higher entropy in the distribution of explored behaviors, indicating that regularization via a KL divergence with a Gaussian prior can partially mitigate collapse at the cost of reduced task performance. Finally, in the \emph{Avoid} task, fine-tuning achieves high task success but largely eliminates multimodality. We attribute this collapse to two factors: (i) a mismatch between the reward landscape used for pre-training and that used during fine-tuning, and (ii) the varying trajectory lengths associated with different modalities, which bias the fine-tuning process toward solutions with a shorter horizon. Altogether, these results are consistent with the 2D Gaussian mixture experiments and show that standard RL fine-tuning progressively destroys multimodality as the reward landscape deviates from the pre-trained trajectory distribution and becomes unbalanced across modes. This highlights the necessity of explicit regularization and motivates our method as a general mechanism for preserving multimodal behaviors during fine-tuning.

\paragraph{Standard Fine-tuning.} Table~\ref{tab:baselines} summarizes results for baselines without explicit multimodality preservation. In \emph{Reach}, all methods fine-tune the pre-trained policy without collapse, indicating that the inherent exploration of diffusion policies suffices to adapt both modes. In \emph{Lift}, fine-tuning improves success rates but fails to consistently solve both modalities; only the steering-based baseline (\texttt{DSRL}) maintains higher entropy but fails to consistently solve both modalities. In \emph{Avoid}, fine-tuning achieves high task success but eliminates multimodality likely due to trajectory length asymmetries that bias toward shorter-horizon solutions. Taken together, the results align with the 2D Gaussian mixture experiments and indicate that standard RL fine-tuning progressively destroys multimodality as the reward landscape deviates from the pre-trained trajectory distribution and becomes unbalanced across modes. %This underscores the need for explicit regularization and motivates our method as a general mechanism to preserve multimodal behaviors during fine-tuning.



\paragraph{\methodname{} Fine-tuning.} 
% Here we evaluate the impact of incorporating our regularization during fine-tuning. As summarized in Table~\ref{tab:our_method}, the intrinsic reward enables adaptation of the pre-trained policy while preserving, in most cases, the multimodal structure of its behavior, with only a minimal trade-off between diversity and task performance. In the \emph{Reach} task, introducing our regularization has no adverse effect on the final success rate, confirming that diversity can be preserved without compromising performance. For the \emph{Lift} task, regularization enables the policy to retain both of the original solution modes from the pre-trained policy, improving upon standard fine-tuning. In the more challenging \emph{Avoid} task, it maintains high success rates while retaining a subset of the original modes, again surpassing standard fine-tuning. Although some collapse persists, the results indicate that the proposed regularization substantially mitigates mode loss, even under pronounced reward imbalance.
Table~\ref{tab:our_method} shows that incorporating our regularization enables adaptation of the pre-trained policy while largely preserving multimodality, with only minimal trade-offs between diversity and task performance. In \emph{Reach}, regularization leaves success rates unaffected, confirming that our regularization term does not sacrifice performance. In \emph{Lift}, \methodname{} enables the policy to retain both solution modes from the pre-trained policy, improving over standard fine-tuning. In the more challenging \emph{Avoid}, our method retains high success while preserving a subset of the modes, again outperforming baselines. Although some collapse remains, the results indicate that regularization substantially mitigates mode loss, even under pronounced reward imbalance. Qualitative visualizations of the skills learned by our method are shown in Figure~\ref{fig:learned_skills_main}. %For completeness, Appendix~\ref{appendix:manipulation_ablation} also presents ablations covering the curriculum, steering-policy pre-training for mode discovery, the choice of $\lambda$, and the effect of removing the steering policy after fine-tuning.
Additionally, ablations covering design choices such as curriculum learning and pre-training the steering policy for mode discovery, or the role of the regularization weight $\lambda$, are reported in Appendix~\ref{appendix:manipulation_ablation}.





\subsection{Robotic Locomotion and Sequential Manipulation}

We further evaluate \methodname{} in more challenging settings: ANYmal locomotion, a high-dimensional extension of the 2D Gaussian setup, and the sequential, combinatorial Franka Kitchen task, both detailed in Appendix~\ref{appendix:manip_tasks}. In both cases, we fine-tune the steering policy and compare against the previously introduced baselines, using latent dimensionalities of $4$ and $64$, respectively. %a high-dimensional locomotion task and a sequential manipulation task with combinatorial structure. We evaluate on ANYmal Locomotion, which extends the 2D Gaussian setup, and on the Franka Kitchen environment, both described in detail in the Appendix~\ref{appendix:manip_tasks}. In both cases, we fine-tune the steering policy and compare against the baselines introduced earlier. The latent dimensionality is set to $4$ and $16$, respectively.

Table~\ref{tab:anymal_results} summarizes the locomotion results. Despite the increased dimensionality, the results closely mirror those of the 2D Gaussian Mixture experiments: \texttt{RES} is a strong baseline in this setting, and \methodname{} preserves all four modes of the pre-trained policy. Additional rollout visualizations, mode-stability analyses, and robustness evaluations under observation noise and dynamics perturbations are provided in Appendix~\ref{appendix:mode stability} and Appendix~\ref{sec:noise_dyn_perturbations}.

Table~\ref{tab:franka_results} reports the results for Franka Kitchen, where success is defined as completing three subtasks in any order. We first observe that the original pre-trained policy exhibits limited mode coverage, as reflected by its relatively low entropy. All baselines collapse the multimodality present in the pre-trained policy. In contrast, \methodname{} maintains some of the original behavioral diversity while still fine-tuning the policy to high success. The slight entropy reduction reflects the loss of a single successful sequence, consistent with the sequences reported in Table~\ref{tab:kitchen_sequences}. Exploring different latent dimensionalities could further mitigate this loss and recover the missing mode. Overall, these results demonstrate that \methodname{} effectively preserves multimodal structure in both high-dimensional control tasks and sequential manipulation domains, where the baselines collapse to a single solution.

