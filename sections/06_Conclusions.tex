\section{Conclusions}
We studied \ac{rlft} of pre-trained generative policies while preserving multimodal behaviors. Focusing on diffusion policies trained from demonstrations, we showed that standard fine-tuning often collapses to a dominant mode when the reward landscape deviates from the demonstrations. To address this, we proposed \methodname{}, which discovers latent behavioral modes via a steering policy and uses a mutual-information–based intrinsic reward to regularize fine-tuning toward retaining diversity. Across a range of robotic domains, this approach mitigated mode collapse while matching or improving task success relative to standard fine-tuning.

% Our results also highlight important limitations and directions for future work. The intrinsic-reward regularization requires careful tuning, and maintaining an inference model during fine-tuning can introduce instability as the policy distribution shifts. Moreover, the current formulation focuses on within-task modes and may require hierarchical or richer latent parameterizations to scale to large, heterogeneous datasets. Exploring alternative trajectory-diversity metrics beyond mutual information, more expressive latent spaces, and mechanisms for semantic grounding of discovered modes—e.g., via language or preference learning—are promising avenues for extending the framework.

Our results also highlighted several limitations and directions for future work. The proposed regularization relies on a specific trajectory-diversity proxy based on mutual information, motivating the exploration of alternative diversity metrics that may better capture task-relevant behavioral variation. In addition, we assumed a discrete latent space for mode discovery; extending the framework to continuous or hierarchical latents is an important direction. Finally, while we focused on leveraging discovered modes for \ac{rlft}, uncovering latent structure in pre-trained generative policies may enable broader applications, such as language grounding or enhanced controllability and human alignment, opening new avenues beyond fine-tuning.

% \section{Conclusions, Limitations and Future Work}
% We studied the problem of \ac{rlft} of pre-trained generative policies while preserving multimodal action distributions. Focusing on diffusion policies trained from demonstrations, we showed that standard fine-tuning often collapsed multimodality to a dominant behavior when the fine-tuning reward landscape diverged from the demonstrations. To address this, we proposed using mutual information as a proxy for multimodality and introduced \methodname{}, an unsupervised mode-discovery method based on a latent reparameterization of a steering policy. We then used the steering policy together with the mutual-information estimate to provide an intrinsic reward that regularized \ac{rlft} toward retaining diverse behaviors.  We benchmarked the method across different robotics environments, and showcased that the proposed regularization mitigated mode collapse, supporting \methodname{} as a practical approach to fine-tuning generative policies without sacrificing behavioral diversity.


% \paragraph{Limitations and Future Work.} Our study revealed several trade-offs and open directions. The intrinsic-reward regularization required careful tuning, as excessive weight slowed learning and reduced task success. Maintaining an inference model during fine-tuning also introduced instabilities, as it needed to track the policy’s shifting state distribution. 
% Moreover, in its current form, \methodname{} is designed as a task-level mode extractor, as it focuses on discovering and preserving \emph{within-task} behavioral modes, rather than modeling the structure of multi-task datasets. 
% Therefore, scaling \methodname{} to large, heterogeneous datasets may require richer latent parametrizations, such as a hierarchical latent space, to capture multi-task-level multimodality. 

% Designing the appropriate parametrization and structure of the latent space is also challenging.  In all experiments, we employed a single categorical latent $\mathcal{Z}$ indexing discrete behavioral modes within each task. We deliberately used a mildly overparameterized space, and our results indicated that \methodname{} could reliably collapse redundant codes and recover the relevant modes, suggesting that overparameterization is not critical in practice. 
% Nonetheless, exploring more expressive continuous or hybrid latent spaces, together with regularization strategies that improve the controllability of $\mathcal{Z}$, such as entropy or KL terms to balance code usage and mitigate mode collapse, is an important direction for future work.


% We occasionally observed that distinct latent codes were mapped to the same environment-defined mode, indicating that our mutual information objective can be sensitive to small variations in the visited state distribution. This is a known weakness of mutual information-based unsupervised skill discovery illustrated in~\cite{CSD}, and systematically assessing which trajectory-diversity metrics most effectively retain multimodal behavior is a promising direction for future work.

% Finally, although the formulation was independent of language supervision, the learned latent space is amenable to post-hoc semantic grounding. Aligning modes with language via preference learning or VLA mappings and developing a joint inference model that preserves diversity while enabling reliable semantic labels are compelling directions for future work.


% \section*{Reproducibility Statement}

% We have made extensive efforts to ensure the reproducibility of our results. All algorithmic details, including model architectures, training procedures, and hyperparameters, are described in the main text and appendix. If any hyperparameter is not explicitly documented in the paper, it will be fully specified in the released code repository. Upon acceptance, we will release the complete codebase, together with configuration files, pretrained checkpoints, and evaluation scripts, to allow exact replication of our experiments. Additionally, proofs of theoretical claims and ablation studies supporting our design choices are included in the appendix.