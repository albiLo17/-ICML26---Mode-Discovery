\section{Mode Discovery for \ac{rl} Fine-tuning}
\label{sec:method}

% To fine-tune pre-trained diffusion policies while preserving multimodality, our method
% hinges on identifying and controlling latent behavioral modes of the pre-trained policy, which in turn enables us to regularize \ac{rlft} with a trajectory-diversity objective. To this end, our framework
% builds on three components:
% (i) we first introduce a tractable proxy to measure multimodality  $\mathcal{M}(\cdot)$ in pre-trained generative policies based on 
% mutual information;
% (ii) we then develop an unsupervised mode-discovery procedure by reparameterizing a steering policy $\pi_{\psi}^{\mathcal{W}}(w \mid s)$ through a latent variable $z \in \mathcal{Z}$, enabling us to uncover and control the behavioral modes of the pre-trained policy during training, while also providing an estimate of multimodality through mutual information.
% (iii) Finally, we use this estimate to construct a mutual information–based intrinsic reward and combine it with task rewards, regularizing \ac{rlft} to improve task performance while explicitly retaining diverse behaviors. In what follows, we describe each component in detail.

To fine-tune pre-trained diffusion policies while preserving multimodality, our method focuses on
identifying and controlling latent behavioral modes of the pre-trained policy. This enables us to
regularize \ac{rlft} using a trajectory-diversity objective. Our framework comprises three components:
(i) we introduce a tractable mutual information–based proxy $\mathcal{M}(\cdot)$ to quantify
multimodality in pre-trained generative policies;
(ii) we develop an unsupervised mode-discovery mechanism by reparameterizing a steering policy
$\pi_{\psi}^{\mathcal{W}}(w \mid s)$ through a latent variable $z \in \mathcal{Z}$, allowing us to uncover
and control behavioral modes while estimating multimodality;
(iii) we leverage this estimate to construct a mutual information–based intrinsic reward, which is
combined with task rewards to regularize \ac{rlft}, improving performance while explicitly retaining
diverse behaviors. We describe each component in detail below.


%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Multimodality in Generative Policies}

% To define multimodality in generative policies such as diffusion and flow-based models, we exploit the fact that these models generate actions by transforming an input noise vector $w \sim \mathcal{N}(0,I)$ through a denoising process conditioned on $s\in \mathcal{S}$. %, denoted $\pi_\theta(a \mid s,w)$. 
% Multimodality can therefore be understood in terms of the diversity of actions induced by different noise seeds $w$. This motivates the following definition:


% \begin{tcolorbox}[
%   enhanced,
%   breakable,
%   float,
%   floatplacement=h!,
%   title=\textbf{Definition: Multimodal Policy},
%   colframe=myblue,
%   colback=myblue!8,
%   coltitle=white,
%   parbox=false,
%   left=5pt,
%   right=5pt,
%   grow to left by=3pt,
%   grow to right by=3pt,
%   %
%   toprule=2pt,
%   titlerule=1pt,
%   leftrule=1pt,
%   rightrule=1pt,
%   bottomrule=1pt,
% ]
% A policy $\pi_\theta(a \mid s, w)$ is multimodal in state $s\in \mathcal{S}$ if there exist $w_1, w_2 \in \mathcal{W}$, $w_1   \neq w_2$, such that
% \begin{equation}
%     D\!\left(\pi_\theta(a \mid s, w_1), \pi_\theta(a \mid s, w_2)\right) \geq \delta,
%     \label{def:multimodal}
% \end{equation}
% for some distance measure $D$ (e.g.\, total variation, KL, Wasserstein) and threshold $\delta > 0$.
% \end{tcolorbox}

% This distance-based definition captures the intuition that different noise variables $w_1, w_2$ can induce distinct behaviors under the same state $s$. 
% However, because $D$ measures dissimilarity between action distributions, its evaluation is intractable for diffusion and flow-based policies. 


% \paragraph{Mutual Information as a Proxy.}
% To obtain a tractable surrogate, we observe that if a policy is multimodal according to Definition~\ref{def:multimodal}, then the latent $\mathcal{W}$ and the action $\mathcal{A}$ must be statistically dependent given the state $\mathcal{S}$. This implies that the conditional mutual information must be strictly positive (proof in Appendix~\ref{sec:proof_MI_positive})
% \[
%     I(W; A \mid S) = \mathbb{E}_{s \sim p(s)}\!\left[D_{\mathrm{KL}}\!\big(\pi_\theta(a \mid s, w)\,\|\, p(a \mid s)\big)\right] > 0,
% \]
% where $p(a \mid s) = \mathbb{E}_{w \sim p(w)}[\pi_\theta(a \mid s, w)]$ is the marginal action distribution. 
% Multimodality holds whenever $I(W;A\mid S)\geq\delta'$ for some $\delta'>0$ capturing the minimal dependence required. Thus, multimodality $\mathcal{M}(\pi_{\theta})$ can be quantified by the conditional mutual information $I(W;A\mid S)$. 
% Since computing this quantity exactly remains intractable, in the next section we will provide a method to estimate this measure in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/04_method/steering_policy_v2.pdf}
    \caption{ 
    \textbf{Unsipervised Behavioral Mode Discovery via Latent Reparameterization of a Steering Policy.}  
An inference model $q_\phi(z \mid s)$ and a steering policy $\pi^{\mathcal{W}}_\psi(w \mid s,z)$ are trained jointly to uncover latent modes $z \in \mathcal{Z}$ in the frozen diffusion actor $\pi_\theta(a \mid s,w)$.  
The steering policy structures the noise space $\mathcal{W}$ according to $z$, inducing diverse actions $a \in \mathcal{A}$, while the inference model recovers $z$ to provide a variational estimate of $I(Z; S)$ (Eq.~\ref{eq:variational_MI}), used as an intrinsic reward during mode discovery and fine-tuning.
    }
    \label{fig:method}
\end{figure*}




\subsection{Measuring Multimodality in Generative Policies}



% \reb{
%We assume access to a pre-trained generative policy that is inherently multimodal, reflecting the diverse behavioral strategies present in the demonstration dataset. 
% Classical definitions of multimodality characterize modes as local maxima of the explicit action distribution ${\pi_\theta(a \mid s)}$~\citep{stoepker2016testing}, but this view becomes impractical for diffusion or flow-based policies, whose action densities are not available in closed form.
% Under the assumption taht  the multimodality of the pre-trained policy $\pi_\theta(a \mid s)$ is realized through this latent noise, in 
% the sense that there exists a set of states of non-zero measure for which different values of $w$ 
% induce different action distributions, i.e.,  $\pi_\theta(\cdot \mid s, w_1) \neq \pi_\theta(\cdot \mid s, w_2)$ for some $w_1 \neq w_2$.  it is possible to show that the latent $W$ and the action $A$ are statistically dependent given $S$, and 
% therefore the conditional mutual information is strictly positive
% (proof in Appendix~\ref{sec:proof_MI_positive})
% \[
%     I(W; A \mid S) = \mathbb{E}_{s \sim p(s)}\!\left[D_{\mathrm{KL}}\!\big(\pi_\theta(a \mid s, w)\,\|\, p(a \mid s)\big)\right] > 0,
% \]
% where $p(a \mid s) = \mathbb{E}_{w \sim \mathcal{W}}[\pi_\theta(a \mid s, w)]$ is the marginal action distribution. 

Classical notions of multimodality define modes as local maxima of an explicit action density 
$\pi_\theta(a \mid s)$~\citep{stoepker2016testing}. This definition becomes impractical for 
modern generative policies such as diffusion or flow-based models, whose action densities are unavailable in closed form.
We instead characterize multimodality through the latent noise $\mathcal{W}$ of a pre-trained 
generative policy ${\pi_\theta(a \mid s, w)}$ with $w \sim \mathcal{W}$. 
We assume that multimodality is realized through this latent representation, in the sense that 
there exists a set of states of non-zero measure for which different latent values induce distinct 
action distributions, i.e.,
${\pi_\theta(\cdot \mid s, w_1) \neq \pi_\theta(\cdot \mid s, w_2)}$ for some $w_1 \neq w_2$. Under this assumption, it is possible to show that the latent variable $W$ and the action $A$ are statistically dependent conditioned on the state $S$. 
As a consequence, the conditional mutual information between $W$ and $A$ given $S$ is strictly positive (proof in Appendix~\ref{sec:proof_MI_positive})
\[
{\small
I(W; A \mid S)
= \mathbb{E}_{s \sim p(s)}\!\left[
    D_{\mathrm{KL}}\!\big(\pi_\theta(\cdot \mid s, w)\,\|\, p(\cdot \mid s)\big)
\right] > 0,}
\]
where $p(a \mid s) = \mathbb{E}_{w \sim \mathcal{W}}[\pi_\theta(a \mid s, w)]$ is the marginal action distribution. 


Although $I(W; A \mid S) > 0$ provides a valid proxy for multimodality in the pre-trained action
distribution, it does not guarantee multimodality in the induced trajectories during fine-tuning.
Action-level diversity does not necessarily translate into trajectory-level diversity, as distinct
actions may lead to similar state transitions under the environment dynamics. For instance, in a
kinematically redundant manipulator, multiple joint-space actions can yield nearly identical
end-effector motions, collapsing multimodality in action space into a single mode in trajectory
space. %Consequently, this quantity alone is insufficient to guarantee the preservation of behavioral diversity during fine-tuning.} 
% }

% Drawing inspiration from the unsupervised skill discovery literature, we address this problem by quantifying multimodality through a trajectory-diversity measure. In particular, rather than measuring multimodality at the action level, we consider the mutual information between the latent noise and the visited states, $I(W;S)$, which directly captures diversity in the induced trajectories~\citep{gregor2016variational,eysenbach2018diversity,sharma2019dynamics}.  Unlike skill-discovery methods, whose goal is to \emph{learn} a latent space of skills from scratch, our setting begins with a pre-trained generative policy whose latent noise space $\mathcal{W}$ already encodes multiple behavioral modes. 
% However, the corresponding modes are implicit in the structure of $\mathcal{W}$, which varies across time. 
% In the next section, we introduce a steering policy $\pi_\psi^{\mathcal{W}}$ to uncover and control the latent behavioral modes in pre-trained policies, effectively lifting the representation from step-wise noise to trajectory-level structure. %This is amenable as it would allow us to render
% this pre-existing diversity controllable as well as measure multimodality via $I(Z;S)$.
%define a framework to preserve it during fine-tuning by maximizing $I(Z;S)$.

To address this limitation, we quantify multimodality at the trajectory level by measuring the
dependence between the latent noise variable $W$ and the visited states. Specifically, we consider
the mutual information $I(W; S)$, which directly captures diversity in the induced trajectories.
Under the environment dynamics, actions mediate the influence of $W$ on future states $S$, inducing the
Markov chain $W \rightarrow A \rightarrow S'$. By the data processing inequality, this implies $I(W; S') \le I(W; A)$.
As a result, state-based mutual information provides a conservative estimate of multimodality,
retaining only behaviorally meaningful distinctions that persist through the dynamics. 
With a slight abuse of notation, we henceforth write $I(W; S)$ to denote the mutual information
between the latent variable $W$ and the state visitation distribution induced by trajectories under
the policy, omitting explicit time indices for clarity.

This perspective is inspired by the unsupervised skill discovery literature, which similarly
emphasizes trajectory diversity through mutual information between latent variables and visited
states~\citep{gregor2016variational,eysenbach2018diversity,sharma2019dynamics}. Unlike prior work,
however, which seeks to \emph{learn} a latent skill space from scratch, our setting starts from a
pre-trained generative policy whose latent noise space $\mathcal{W}$ already encodes multiple
behavioral modes. 
% These modes, however, are implicit in the structure of $\mathcal{W}$.
These modes, however, are not explicitly represented in $\mathcal{W}$ as the noise variable $w \in \mathcal{W}$ is
time-local and varies at each time step, whereas the behavioral modes we are interested in emerge at the trajectory level. As a result, the mode structure is implicit in
$\mathcal{W}$ and not directly accessible through $W$ alone.
In the next section, we introduce a method to uncover and control
these latent behavioral modes, effectively lifting step-wise noise into coherent trajectory-level
structure.

% However, while $I(W; S)$ captures the dependence between noise realizations and trajectories, the latent variable $w \in \mathcal{W}$
% does not directly represent behavioral modes as defined in Section~\ref{sec:problem_formulation}. In particular, $w$ is sampled
% locally and varies across time steps and denoising stages, whereas behavioral modes correspond to
% coherent patterns of behavior expressed over entire trajectories. As a result, the structure associated with behavioral modes is implicit in $\mathcal{W}$ and not directly accessible through $W$ alone.  To explicitly uncover and organize these modes, in the next sectionwe introduce a latent abstraction that operates at the trajectory level.




% \reb{
% Common approaches in the skill-discovery literature quantify diversity via the mutual information between a latent variable and the visited states $I(Z;S)$~\citep{gregor2016variational,eysenbach2018diversity,sharma2019dynamics}. Unlike these methods, whose goal is to \emph{learn} a latent space of skills from scratch, our setting assumes that a pre-trained diffusion policy already induces diverse trajectories through its latent noise $w \in \mathcal{W}$, so that $I(W;S) \geq 0$. While $I(W;S)$ could then be a proxy for maximizing diversity, these modes are implicit as their structure is unknown and not explicitly parameterized and not controllable (why do we want it controllable?). However, the corresponding modes in $\mathcal{W}$ are implicit and not directly controllable, unlike the explicit skills indexed by 
% $\mathcal{Z}$; in the next section, we therefore introduce a steering policy 
% $\mathcal{Z} \mapsto \mathcal{W}$ that exposes and preserves this pre-existing diversity during fine-tuning.

% Our objective is therefore not to discover new skills, but to \emph{preserve} the multimodal structure already embedded in the diffusion policy during fine-tuning. 
% } 

% , we proposed to estimate trajectory multimodality by measuring how the latent input noise $w \in \mathcal{W}$ affects the states reached by the generative policy. Because the transition dynamics induce the Markov structure $\mathcal{W} \to \mathcal{A}_t \to {S}_{t+1}$ given ${S}_t$, the data-processing inequality gives $I({W};{A}_t \mid {S}_t) \;\ge\; I({W};{S}_{t+1} \mid {S}_t)$. Thus, $
% I(W;S_{t+1})
% =
% \mathbb{E}_{p(s_t)}
% \!\left[
%   I(W;S_{t+1}\mid S_t = s_t)
% \right],
% $ serves as an information-theoretic lower bound on the action-level dependence and provides a principled surrogate for assessing trajectory-level multimodality.

% Unlike skill-discovery methods, whose goal is to \emph{learn} a latent space of skills from scratch, our setting begins with a pre-trained generative policy whose latent noise already encodes multiple behavioral modes. These modes are implicit as their structure is unknown and not explicitly parameterized, but they manifest through the way different noise inputs induce different action sequences and state transitions. Our objective is therefore not to discover new skills, but to \emph{preserve} the multimodal structure already embedded in the diffusion policy during fine-tuning.
% } Since computing $I(W;S)$ exactly remains intractable, in the next section we will provide a method to estimate it in practice.





\subsection{Behavioral Mode Discovery of Generative Policies}

% \al{This part could be rewritten by putting emphasis on discovering the latent structure at trajectory level modalities. So connecting different state-dependent modes.}

% \al {I think it might be worth restructuring the logic of this section by first introducing a variational lower bound for computing MI over the latent noise as in skill discovery. However, this is not that useful as it does not allow for controllability and would require the inference model to learn the structure of the noise space at each time step of the MDP, making it prone to overfitting. We thus introduce the latent reparameterization that abstracts temporal clusters in the noise over a trajectory level latent, which makes it easier learning the inference model as well as allows for controllability over the latent space by sampling specific z at the beginning of each episode and allowing for uncovering latent modes. }

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.98\linewidth]{figures/04_method/method.pdf}
%     \caption{ \al{Improve the figure.} \textbf{Unsupervised Mode Discovery via Latent Reparameterization of a Steering Policy.} 
%      A latent variable $z \in \mathcal{Z}$ steers the pre-trained diffusion policy by shaping the noise distribution $\mathcal{W}$ via the steering policy $\pi_\psi(w \mid s, z)$. The input noise $w$ is then mapped through the actor $\pi_\theta(a \mid s, w)$ to produce diverse actions $a \in \mathcal{A}$. This reparameterization of the steering policy via $\mathcal{Z}$ organizes the unstructured noise space $\mathcal{W}$ into controllable behavioral modes. To ensure these modes remain distinct, a discriminator $q_\phi(z \mid s,a)$ is trained to recover $z$ from state–action pairs, yielding a variational estimate of $I(Z;A \mid S)$. This mutual-information signal acts as an intrinsic reward during RL fine-tuning, aligning task adaptation with multimodality preservation.
% }
%     \label{fig:method}
% \end{figure}


% The analysis above motivates trajectory-level mutual information $I(W; S)$ as a principled measure
% of multimodality in pre-trained generative policies. Intuitively, high mutual information indicates
% that different realizations of the latent noise induce distinct state–trajectory distributions.
%However the latent noise $w \in \mathcal{W}$ is sampled locally and independently, making the 
% $I(W; S)$ is neither directly controllable nor tractable to estimate or optimize.
% However, while $I(W; S)$ captures the dependence between noise realizations and trajectories, the latent variable $w \in \mathcal{W}$
% does not directly represent behavioral modes as defined in Section~\ref{sec:problem_formulation}. In particular, $w$ is sampled
% locally and varies across time steps and denoising stages, whereas behavioral modes correspond to
% coherent patterns of behavior expressed over entire trajectories. As a result, the structure associated with behavioral modes is implicit in $\mathcal{W}$ and not directly accessible through $W$ alone.  To explicitly uncover and organize these modes, in the next sectionwe introduce a latent abstraction that operates at the trajectory level.

% In particular, the latent noise space $\mathcal{W}$ lacks a structured representation at the
% trajectory level, as its semantics vary across time steps and denoising stages, whereas the
% multimodal behaviors of interest emerge only over extended horizons. Consequently, realizing
% trajectory-level multimodality requires introducing a controllable abstraction that operates at the
% level of entire trajectories.

To explicitly uncover and organize the behavioral modes implicit in the latent noise space
$\mathcal{W}$, we introduce \methodname{} (\emph{\textbf{B}ehavioral \textbf{M}ode \textbf{D}iscovery}). Specifically, \methodname{} reparameterizes a steering policy with a latent variable $z \in \mathcal{Z}$ that organizes $\mathcal{W}$ into trajectory-level modes. 
% This simultaneously enables controllability over the latent space and the measurement of multimodality at the trajectory level.


% we introduce a latent variable $z \in \mathcal{Z}$ that steers the pre-trained diffusion policy by shaping the noise distribution $\mathcal{W}$ via the steering policy $\pi_\psi(w \mid s, z)$. The input noise $w$ is then mapped through the actor $\pi_\theta(a \mid s, w)$ to produce diverse actions $a \in \mathcal{A}$. This reparameterization of the steering policy via $\mathcal{Z}$ organizes the unstructured noise space $\mathcal{W}$ into controllable behavioral modes. To ensure these modes remain distinct, a discriminator $q_\phi(z \mid s,a)$ is trained to recover $z$ from state–action pairs, yielding a variational estimate of $I(Z;A \mid S)$. This mutual-information signal acts as an intrinsic reward during RL fine-tuning, aligning task adaptation with multimodality preservation.


%In the previous section, we defined a measure of multimodality $\mathcal{M}$ via the conditional mutual information ${I(W;A\mid S)}$ between the diffusion policy’s latent noise and the actions induced by the policy $\pi_\theta(a \mid s, w)$. 
% Directly optimizing $I(W;S)$
% is impractical as the implicit structure of $\mathcal{W}$ varies at every time-step and for each action-chunk, whereas the multimodal behaviors we are interested in emerge at the trajectory level. 
% To overcome this problem and simultaneously obtain a structured and controllable representation of the policy modes, we introduce \methodname{} (\emph{\textbf{M}ode \textbf{D}iscovery for \textbf{M}ultimodal \textbf{A}ction \textbf{D}istributions}), which reparameterizes a steering policy with a latent variable $z \in \mathcal{Z}$ that organizes $\mathcal{W}$ into trajectory-level modes.



\paragraph{Latent Reparameterization.}

Let $\pi_\psi^{\mathcal{W}}(w\mid s)$ denote a steering policy that selects the latent noise $w\in\mathcal{W}$ seeding the denoising process. We introduce a latent variable $z\in\mathcal{Z}$ and define a latent-conditioned steering policy $\pi_\psi^{\mathcal{W}}(w\mid s,z)$, which induces the family of action distributions
\begin{equation*}
    \pi_{\theta,\psi}(a\mid s,z) \;=\; \int \pi_\theta(a\mid s,w)\,\pi_\psi^{\mathcal{W}}(w\mid s,z)\,dw.
    \label{eq:steered_policy}
\end{equation*}
% Under this reparameterization, multimodality, \reb{or trajectory-level diversity,} can be measured by the mutual information \reb{$I(Z;S)$,  which places us back in the standard skill-discovery setting and allows us to leverage this class of methods to optimize the steering policy.
% By training the steering policy to maximize $I(Z;S)$, we encourage different values of $z$ to induce distinct state–trajectory distributions by steering the sampling of $\mathcal{W}$, thereby uncovering the modes implicitly encoded in the latent noise space.} Distinct values of $z$ can therefore select different behaviors (modes) encoded by the \emph{fixed} pretrained policy $\pi_\theta(a\mid s,w)$ through the steering policy $\pi_\psi^{\mathcal{W}}(w\mid s,z)$. 

Under this reparameterization, multimodality, or trajectory-level diversity, can be quantified by
the mutual information $I(Z; S)$ between the latent code and the visited states. Combined with the
assumption that $I(W; S) > 0$, this formulation allows us to uncover the behavioral modes implicitly
encoded in the latent noise space by training a steering policy to maximize $I(Z; S)$. Maximizing
this objective encourages different values of $z$ to steer the sampling of $\mathcal{W}$ toward
distinct state–trajectory distributions, thereby inducing coherent and identifiable behavioral
modes. However, $I(Z; S)$ is not directly tractable to optimize. 

% While $I(Z; S)$ defines the desired objective, it is not directly tractable to optimize. In the
% following, we therefore derive a variational lower bound that yields a practical learning signal
% for training the steering policy. Distinct values of $z$ can then be used to selectively invoke
% different behaviors (modes) of the \emph{fixed} pre-trained policy
% $\pi_\theta(a \mid s, w)$ through the latent-conditioned steering policy
% $\pi_\psi^{\mathcal{W}}(w \mid s, z)$.


\paragraph{Variational Lower Bound.}
To optimize $\pi_\psi^{\mathcal{W}}$ via $I(Z;  S)$
we follow standard practice in skill discovery~\citep{eysenbach2018diversity}, which derives a variational lower bound on the mutual information by introducing an inference model $q_\phi(z \mid s)$ that approximates the posterior over latent codes. This yields

\begin{align}
    I(Z; S) 
    &= \mathbb{E}_{p(s,z)} \left[ \log \frac{p(z \mid s)}{p(z)} \right] \nonumber\\
    &\geq \mathbb{E}_{p(s,z)} \left[ \log q_\phi(z \mid s) - \log p(z) \right],
    \label{eq:variational_MI}
\end{align}


where $p(s,z)$ denotes the joint distribution induced by sampling 
$z \sim p(z)$ and rolling out a policy $\pi(a \mid s, z)$ in the environment.
We refer to ~\cite{eysenbach2018diversity} for the derivation of Equation~\ref{eq:variational_MI}.




\paragraph{Steering Policy Training.} The log-posterior likelihood is used as an intrinsic reward for training the steering policy $\pi_\psi^{\mathcal{W}}$, aligning the RL objective with the identifiability of $z$. This establishes a feedback loop in which $q_\phi$ improves at classifying latent codes while the $\pi_\psi^{\mathcal{W}}$ is incentivized to select $w\in \mathcal{W}$ so as to induce trajectories that are consistent and discriminable. 

As a result, the proposed method explicitly uncovers and organizes the behavioral modes implicit in
the pre-trained policy, yielding a trajectory-level representation that both quantifies
multimodality, via Equation~\ref{eq:variational_MI}, and allows different modes to be controlled via $z$. An overview of the mode
discovery process is shown in Figure~\ref{fig:method}.

% The proposed method simultaneously enables controllability over the latent space and the measurement of multimodality at the trajectory level. An overview of the method for mode discovery is illustrated in Figure~\ref{fig:method}.




\subsection{Policy Fine-tuning with Intrinsic Reward}
Recall from Section~\ref{sec:problem_formulation} that we formulated the fine-tuning objective as maximizing task return regularized by a multimodality measure $\mathcal{M}$.  Building on this, the variational lower bound introduced above provides a tractable instantiation of $\mathcal{M}$, which is leveraged as an intrinsic signal to preserve multimodality during \ac{rlft}.  Concretely, we define the augmented reward
\begin{equation}
    r_{\text{total}}(s,z) = r_{\mathrm{env}}(s,a) 
    + \lambda \Big( \log q_\phi(z \mid s) - \log p(z) \Big),
\end{equation}
where $r_{\mathrm{env}}$ is the environment reward and $\lambda \geq 0$ balances task performance with multimodality preservation. %The extrinsic term promotes task success, while the intrinsic term encourages different latent codes $z$ to induce distinguishable behaviors, thereby preventing mode collapse. 
Directly combining task and intrinsic rewards may lead to premature collapse if the task signal dominates before the multimodal structure is discovered. We therefore adopt a two-stage scheme: first, the steering policy is trained with the intrinsic objective alone to uncover the modes of the pretrained policy; then the environment reward is introduced to guide fine-tuning toward high-return behaviors without destroying diversity.  During mode discovery, we also apply a short-to-long horizon curriculum to stabilize learning.
Algorithm~\ref{alg:mode_finetuning}  summarizes the overall procedure, which is thoroughly described in Appendix~\ref{appendix:algorithm}. 
%The steering policy $\pi^{\mathcal{W}}(w\mid s,z)$ is updated using the augmented reward $r_{\text{total}}$, while the discriminator $q_\phi(z \mid s,a)$ is trained to maximize the variational lower bound, providing the intrinsic reward signal. 
While we adopt PPO~\citep{schulman2017proximal} in our experiments, our framework is agnostic to the specific RL algorithm used. % to train the steering policy and is compatible with both on-policy and off-policy methods.


\paragraph{Broader Use of the Framework.}
While the formulation above fine-tunes the generative model indirectly via the steering policy, the framework is not limited to this case. Because the steering head actively explores diverse input-noise regions while pursuing reward, it can be combined with direct fine-tuning of the diffusion weights, acting as a structured exploration agent. At test time, the steering policy can either be retained—allowing explicit control over the behavioral mode—or removed, reverting to random sampling from the noise prior. Furthermore, while outside the present study, the learned latent space $\mathcal{Z}$ provides a natural basis for grounding semantic labels (e.g.\, language instructions) when limited annotations are available. 


\input{sections/04Algo_method}

% \al{Introduce here the fact that we will focus on diffusion policies but it is straightforward to extend to flow based methods, which are also by definition deterministic.}

