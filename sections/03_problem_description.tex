\section{Problem Formulation}
\label{sec:problem_formulation}

We study \ac{rlft} of a pre-trained generative policy (e.g., diffusion or flow-based) to maximize expected return while preserving the multimodality acquired from demonstrations. Here, multimodality refers to the presence of multiple distinct high-probability behaviors, which may arise from heterogeneous task goals and/or multiple feasible trajectories that solve the same goal. We model the environment as a Markov Decision Process (MDP) $(\mathcal{S}, \mathcal{A}, r, p, \gamma)$ with state space $\mathcal{S}$, action space $\mathcal{A}$, reward function $r$, transition dynamics $p$, and discount factor $\gamma \in [0,1)$. The objective of \ac{rl} is to learn a policy $\pi_\theta(a \mid s)$ maximizing the expected discounted return
\[
    J(\pi) = \mathbb{E}_{\pi}\!\left[ \sum_{t=0}^\infty \gamma^t r(s_t,a_t)\right],\]

where $s_t$ and $a_t$ are distributed according to the transition dynamics $p$ and the policy $\pi$.

% In our setting, multimodality arises within a single task: the policyâ€™s trajectory distribution may place mass on multiple distinct solutions, where each mode corresponds to a self-consistent behavioral strategy that successfully accomplishes the objective. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{Pre-trained Multimodal Action Distirbutions.} %\al{Here we discuss mode in the trajectories but not in the action distribution.}
% We assume access to an offline \new{}dataset of state-action pairs 
% ${\mathcal{D} = \{(s_t, a_t)\}_{i=1}^N}$  collected by diverse behavioral policies (e.g.\, human demonstrations), which is used to pre-train a generative policy $\pi_\theta(a \mid s)$ via imitation learning.  We define a mode of the policy $\pi_\theta$ as a latent variable $z \in \mathcal{Z}$, implicitly encoded in the pre-trained multimodal policy, which induces the trajectory distribution ${ p^\pi(\tau \mid z) = p(s_0)\prod_{t=0}^{T-1} \pi(a_t \mid s_t, z)\, p(s_{t+1}\mid s_t,a_t)}$, so that different values of $z$ correspond to distinct self-consistent strategies that solve the task, i.e., different modes. We assume the original modes $z\in\mathcal{Z}$ contained in the datasets are unknown. When relevant, we make explicit the dependence of the generative policy on its input noise variable  $w \in \mathcal{W}$ by denoting it as $\pi_\theta(a \mid s, w)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Pre-Trained Multimodal Generative Policies.}%Action Distirbutions.} %\al{Here we discuss mode in the trajectories but not in the action distribution.}
We assume access to an offline demonstration dataset ${\mathcal{D}
= \{\tau^{(i)}\}_{i=1}^N, \tau^{(i)} = (s^{(i)}_0,a^{(i)}_0,\dots,s^{(i)}_{T_i},a^{(i)}_{T_i})}$, collected with diverse behavioral policies (e.g.\, human demonstrations), used to pre-train a generative policy $\pi_\theta$ via imitation learning. We further assume that the resulting policy is \emph{multimodal}, in the sense that it assigns non-negligible probability mass to multiple distinct behaviors reflected in the dataset~\cite{chi2023diffusion}. When relevant, we make explicit the dependence of the generative policy on its input noise variable $w \in \mathcal{W}$ by denoting it as $\pi_\theta(a \mid s, w)$.


\paragraph{Behavioral Modes.}
We represent behavioral modes with a discrete latent variable $z \in \mathcal{Z}$, where each value indexes a policy instance inducing the trajectory distribution
$
p^\pi(\tau \mid z) = p(s_0)\prod_{t=0}^{T-1} \pi(a_t \mid s_t, z)\, p(s_{t+1}\mid s_t,a_t).
$
Different $z$ correspond to distinct self-consistent strategies (behavioral modes) in the data. Although we focus on discrete $z$, continuous latents (e.g., $z \in \mathbb{R}^d$) are also possible. We assume the true modes are unknown but implicitly encoded in the pre-trained multimodal policy.



%Following~\citep{wagenmaker2025steering}, we assume the pre-trained policy is \emph{steerable} through a latent input noise variable $w \in \mathcal{W}$. %$w \in \mathcal{W} := \mathbb{R}^d$. 
% Specifically, given $w$, a diffusion or flow-based policy can be rewritten by making explicit the dependence of the action on the input noise as $\pi_{\theta}(a \mid s, w)$. 

% \paragraph{Steerability Assumption.} We assume that the pre-trained generative policy $\pi_\theta(a \mid s, w)$ is \emph{steerable}, in the sense that its behavior can be systematically influenced through the choice of the latent noise input $w \in \mathcal{W}$. A \emph{steering policy} $\pi_\psi^{\mathcal{W}}(w \mid s)$, parameterized by $\psi$, selects which point $w$ in the latent-noise space to denoise, %acts in the latent space $\mathcal{W}$ and  
% biasing the generative model toward different behavioral modes~\citep{wagenmaker2025steering}. %By choosing which point in the latent-noise space to denoise, the steering policy can bias the generative model toward different behavioral modes.

\paragraph{Steerability Assumption.} We assume that the pre-trained generative policy $\pi_\theta(a \mid s, w)$ can be \emph{steered} by controlling its latent noise input $w \in \mathcal{W}$. We introduce a \emph{steering policy} $\pi_\psi^{\mathcal{W}}(w \mid s)$ that maps the current state to a distribution over noise inputs, thereby biasing the actions produced by $\pi_\theta$ toward different behaviors~\citep{wagenmaker2025steering}. 

%selects noise variables conditioned on the current state, thereby indirectly shaping the action distribution of $\pi_\theta$. selecting which point in the latent-noise space to denoise, we can steer the action produced by $\pi_\theta$ to a desired mode.  %Steerability also requires that the generative process preserves dependencies between the input noise and the generated actions~\citep{domingo2024adjoint}. %; in particular, the noise schedule must not be memoryless.
%We assume that the pre-trained policy is \emph{steerable} through its latent input noise variable $w \in \mathcal{W}$. A \emph{steering policy} $\pi_\psi^{\mathcal{W}}(w \mid s)$  parameterized by $\psi$ is a policy that selects latent noise variables, thereby indirectly controlling the behavior of $\pi_\theta$. While we do not assume that the pretrained policy has to be deterministic to be steerable, the noise schedule must not be memoryless~\citep{domingo2024adjoint}, meaning that the dependency between noise variables and the generated samples is preserved throughout the generation process.

\paragraph{Fine-tuning Objective.}
Our goal is to fine-tune the policy $\pi_{\theta}$ in order to (i) maximize expected return and (ii) preserve the multimodality present in the pre-trained policy $\pi_\theta$. 
We formalize this as the regularized optimization problem
\[
    \max_\theta \; J(\pi_{\theta}) + \lambda \, \mathcal{M}(\pi_{\theta}),
\]
where $\mathcal{M}$ denotes a multimodality measure of the generative policy, %induced action distribution, 
and $\lambda \ge 0$ balances task performance with diversity preservation. Importantly, we do not assume prior knowledge of the number of modes in $\pi_\theta$. %, nor do we rely on differentiability of the policy to locate local maxima.
%Designing a practical measure for multimodality under these constraints is a central contribution of this work.

% \paragraph{Fine-tuning Objective.}
% Let $\pi_{\theta_0}$ denote a pre-trained generative policy. Our goal is to fine-tune its parameters $\theta$ to (i) maximize expected return and (ii) preserve the multimodality present in $\pi_{\theta_0}$. We formalize this as
% \[
%     \max_{\theta}\; J(\pi_{\theta}) + \lambda \, \mathcal{M}(\pi_{\theta}),
% \]
% where $\mathcal{M}$ measures multimodality of the generative policy and $\lambda \ge 0$ trades off task performance and diversity preservation. In the following, we introduce auxiliary models (e.g., a steering policy) to obtain a tractable estimate of $\mathcal{M}(\pi_{\theta})$ and an intrinsic reward for optimization.